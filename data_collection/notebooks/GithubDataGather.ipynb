{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount the Harddrive and Change to Git Data Gather Folder\n"
      ],
      "metadata": {
        "id": "GKJA4nJb8S2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/ML_TRAINING/Git_Data_Gather\n",
        "!set GITHUB_TOKEN=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"
      ],
      "metadata": {
        "id": "hgzYZsce8SG8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ2JFVp49oUt",
        "outputId": "1e19c390-6a73-4303-ba63-2cbba44ebf78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Github Repo Links from user profile"
      ],
      "metadata": {
        "id": "q10H6C6v8Uu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import requests\n",
        "import getpass\n",
        "\n",
        "GITHUB_API = \"https://api.github.com/users/{username}/repos\"\n",
        "\n",
        "def get_github_repos(username, token):\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "    page = 1\n",
        "    repos = []\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(GITHUB_API.format(username=username) + f'?page={page}&per_page=100', headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch repos from GitHub for user {username}. HTTP status code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        repo_page = response.json()\n",
        "        if repo_page:\n",
        "            repos.extend(repo_page)\n",
        "            page += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return repos\n",
        "\n",
        "def generate_git_clone_links(repos):\n",
        "    return [repo['clone_url'] for repo in repos]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    token = getpass.getpass(prompt='Please enter your GitHub token: ')\n",
        "    username = input('Please enter your GitHub username: ')\n",
        "\n",
        "    repos = get_github_repos(username, token)\n",
        "    if repos is not None:\n",
        "        clone_links = generate_git_clone_links(repos)\n",
        "\n",
        "        with open(f'git_links_{username}.txt', 'w') as f:\n",
        "            for link in clone_links:\n",
        "                f.write(link + '\\n')\n"
      ],
      "metadata": {
        "id": "YMOtjDuu8SCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53ae43a-7a06-4ef2-f726-59e9deffe433"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your GitHub token: ··········\n",
            "Please enter your GitHub username: huggingface\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ML_TRAINING/Git_Data_Gather/git_repo_link_gen.py sindresorhus"
      ],
      "metadata": {
        "id": "Tvf8Uq9D_K6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e438cede-e742-4459-90a8-38a63192e2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/sindresorhus/-.git\n",
            "https://github.com/sindresorhus/.github.git\n",
            "https://github.com/sindresorhus/acosh.git\n",
            "https://github.com/sindresorhus/Actions.git\n",
            "https://github.com/sindresorhus/active-win.git\n",
            "https://github.com/sindresorhus/active-win-cli.git\n",
            "https://github.com/sindresorhus/add-asset-webpack-plugin.git\n",
            "https://github.com/sindresorhus/add-module-exports-webpack-plugin.git\n",
            "https://github.com/sindresorhus/aggregate-error.git\n",
            "https://github.com/sindresorhus/alfred-dark-mode.git\n",
            "https://github.com/sindresorhus/alfred-emoj.git\n",
            "https://github.com/sindresorhus/alfred-lock.git\n",
            "https://github.com/sindresorhus/alfred-npms.git\n",
            "https://github.com/sindresorhus/alfred-plash.git\n",
            "https://github.com/sindresorhus/alfred-simple.git\n",
            "https://github.com/sindresorhus/alfred-xcode.git\n",
            "https://github.com/sindresorhus/alfy.git\n",
            "https://github.com/sindresorhus/aliases.git\n",
            "https://github.com/sindresorhus/all-keys.git\n",
            "https://github.com/sindresorhus/alpha-sort.git\n",
            "https://github.com/sindresorhus/ama.git\n",
            "https://github.com/sindresorhus/amas.git\n",
            "https://github.com/sindresorhus/anatine.git\n",
            "https://github.com/sindresorhus/ansi-escapes.git\n",
            "https://github.com/sindresorhus/any-observable.git\n",
            "https://github.com/sindresorhus/anybar.git\n",
            "https://github.com/sindresorhus/anybar-cli.git\n",
            "https://github.com/sindresorhus/app-exists.git\n",
            "https://github.com/sindresorhus/app-path.git\n",
            "https://github.com/sindresorhus/app-path-cli.git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Git Clone all repo links\n"
      ],
      "metadata": {
        "id": "is8RSpiRMrD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/ML_TRAINING/Git_Data_Gather/git_clones\n"
      ],
      "metadata": {
        "id": "X3eNdQ_rg-VD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/ML_TRAINING/Git_Data_Gather/git_clones\n",
        "\n",
        "\n",
        "!git clone https://github.com/huggingface/100-times-faster-nlp.git\n",
        "!git clone https://github.com/huggingface/accelerate.git\n",
        "!git clone https://github.com/huggingface/accelerate-wip.git\n",
        "!git clone https://github.com/huggingface/adversarialnlp.git\n",
        "!git clone https://github.com/huggingface/allennlp.git\n",
        "!git clone https://github.com/huggingface/amazon-eks-ami.git\n",
        "!git clone https://github.com/huggingface/api-inference-community.git\n",
        "!git clone https://github.com/huggingface/audio-transformers-course.git\n",
        "!git clone https://github.com/huggingface/autotrain-advanced.git\n",
        "!git clone https://github.com/huggingface/awd-lstm-lm.git\n",
        "!git clone https://github.com/huggingface/awesome-huggingface.git\n",
        "!git clone https://github.com/huggingface/awesome-papers.git\n",
        "!git clone https://github.com/huggingface/bert-syntax.git\n",
        "!git clone https://github.com/huggingface/block_movement_pruning.git\n",
        "!git clone https://github.com/huggingface/blog.git\n",
        "!git clone https://github.com/huggingface/bloom-jax-inference.git\n",
        "!git clone https://github.com/huggingface/chat-ui.git\n",
        "!git clone https://github.com/huggingface/collaborative-training-auth.git\n",
        "!git clone https://github.com/huggingface/community-events.git\n",
        "!git clone https://github.com/huggingface/course.git\n",
        "!git clone https://github.com/huggingface/data-measurements.git\n",
        "!git clone https://github.com/huggingface/data-measurements-tool.git\n",
        "!git clone https://github.com/huggingface/datablations.git\n",
        "!git clone https://github.com/huggingface/datasets.git\n",
        "!git clone https://github.com/huggingface/datasets-server.git\n",
        "!git clone https://github.com/huggingface/datasets-tagging.git\n",
        "!git clone https://github.com/huggingface/datasets-viewer.git\n",
        "!git clone https://github.com/huggingface/deep-rl-class.git\n",
        "!git clone https://github.com/huggingface/diff2html.git\n",
        "!git clone https://github.com/huggingface/diffusers.git\n",
        "!git clone https://github.com/huggingface/diffusers-ft.git\n",
        "!git clone https://github.com/huggingface/diffusers_all.git\n",
        "!git clone https://github.com/huggingface/diffusion-models-class.git\n",
        "!git clone https://github.com/huggingface/disaggregators.git\n",
        "!git clone https://github.com/huggingface/distill-bloom-deepspeed.git\n",
        "!git clone https://github.com/huggingface/doc-build.git\n",
        "!git clone https://github.com/huggingface/doc-build-dev.git\n",
        "!git clone https://github.com/huggingface/doc-builder.git\n",
        "!git clone https://github.com/huggingface/education-toolkit.git\n",
        "!git clone https://github.com/huggingface/efficient_scripts.git\n",
        "!git clone https://github.com/huggingface/ethics-education.git\n",
        "!git clone https://github.com/huggingface/ethics-scripts.git\n",
        "!git clone https://github.com/huggingface/evaluate.git\n",
        "!git clone https://github.com/huggingface/exporters.git\n",
        "!git clone https://github.com/huggingface/flappy-bird-gym.git\n",
        "!git clone https://github.com/huggingface/flax_bert.git\n",
        "!git clone https://github.com/huggingface/fuego.git\n",
        "!git clone https://github.com/huggingface/gaia.git\n",
        "!git clone https://github.com/huggingface/gym-games.git\n",
        "!git clone https://github.com/huggingface/helm-common.git\n",
        "!git clone https://github.com/huggingface/helm-publish-action.git\n",
        "!git clone https://github.com/huggingface/hf-endpoints-documentation.git\n",
        "!git clone https://github.com/huggingface/hf-endpoints-emulator.git\n",
        "!git clone https://github.com/huggingface/hfapi.git\n",
        "!git clone https://github.com/huggingface/hfcc.nvim.git\n",
        "!git clone https://github.com/huggingface/hffs.git\n",
        "!git clone https://github.com/huggingface/hf_benchmarks.git\n",
        "!git clone https://github.com/huggingface/hf_transfer.git\n",
        "!git clone https://github.com/huggingface/hmtl.git\n",
        "!git clone https://github.com/huggingface/hub-docs.git\n",
        "!git clone https://github.com/huggingface/huggingface-sagemaker-snowflake-example.git\n",
        "!git clone https://github.com/huggingface/huggingface-vscode.git\n",
        "!git clone https://github.com/huggingface/huggingface.js.git\n",
        "!git clone https://github.com/huggingface/huggingface_hub.git\n",
        "!git clone https://github.com/huggingface/huggingface_sb3.git\n",
        "!git clone https://github.com/huggingface/huggingface_tianshou.git\n",
        "!git clone https://github.com/huggingface/instruction-tuned-sd.git\n",
        "!git clone https://github.com/huggingface/ionic.git\n",
        "!git clone https://github.com/huggingface/khipu_workshop.git\n",
        "!git clone https://github.com/huggingface/knockknock.git\n",
        "!git clone https://github.com/huggingface/large_language_model_training_playbook.git\n",
        "!git clone https://github.com/huggingface/llm_training_handbook.git\n",
        "!git clone https://github.com/huggingface/lm-evaluation-harness.git\n",
        "!git clone https://github.com/huggingface/m4-logbook.git\n",
        "!git clone https://github.com/huggingface/Megatron-LM.git\n",
        "!git clone https://github.com/huggingface/ml-agents.git\n",
        "!git clone https://github.com/huggingface/ml-agents-patch.git\n",
        "!git clone https://github.com/huggingface/ML-Agents-Training-Executables.git\n",
        "!git clone https://github.com/huggingface/model-evaluator.git\n",
        "!git clone https://github.com/huggingface/model_card.git\n",
        "!git clone https://github.com/huggingface/Mongoku.git\n",
        "!git clone https://github.com/huggingface/naacl_transfer_learning_tutorial.git\n",
        "!git clone https://github.com/huggingface/neural-compressor.git\n",
        "!git clone https://github.com/huggingface/neuralcoref.git\n",
        "!git clone https://github.com/huggingface/neuralcoref-models.git\n",
        "!git clone https://github.com/huggingface/neuralcoref-viz.git\n",
        "!git clone https://github.com/huggingface/nn_pruning.git\n",
        "!git clone https://github.com/huggingface/node-question-answering.git\n",
        "!git clone https://github.com/huggingface/notebooks.git\n",
        "!git clone https://github.com/huggingface/OBELISC.git\n",
        "!git clone https://github.com/huggingface/olm-datasets.git\n",
        "!git clone https://github.com/huggingface/olm-training.git\n",
        "!git clone https://github.com/huggingface/open-muse.git\n",
        "!git clone https://github.com/huggingface/optimum.git\n",
        "!git clone https://github.com/huggingface/optimum-benchmark.git\n",
        "!git clone https://github.com/huggingface/optimum-furiosa.git\n",
        "!git clone https://github.com/huggingface/optimum-graphcore.git\n",
        "!git clone https://github.com/huggingface/optimum-habana.git\n",
        "!git clone https://github.com/huggingface/optimum-intel.git\n",
        "!git clone https://github.com/huggingface/optimum-neuron.git\n",
        "!git clone https://github.com/huggingface/paper-style-guide.git\n",
        "!git clone https://github.com/huggingface/peft.git\n",
        "!git clone https://github.com/huggingface/personas.git\n",
        "!git clone https://github.com/huggingface/python-readability.git\n",
        "!git clone https://github.com/huggingface/pytorch-image-models.git\n",
        "!git clone https://github.com/huggingface/pytorch-openai-transformer-lm.git\n",
        "!git clone https://github.com/huggingface/pytorch-pretrained-BigGAN.git\n",
        "!git clone https://github.com/huggingface/pytorch_block_sparse.git\n",
        "!git clone https://github.com/huggingface/rasa_hmtl.git\n",
        "!git clone https://github.com/huggingface/rl-baselines3-zoo.git\n",
        "!git clone https://github.com/huggingface/rl-baselines3-zoo-update.git\n",
        "!git clone https://github.com/huggingface/RL-model-card-template.git\n",
        "!git clone https://github.com/huggingface/rlhf-interface.git\n",
        "!git clone https://github.com/huggingface/roots-search-tool.git\n",
        "!git clone https://github.com/huggingface/s3prl.git\n",
        "!git clone https://github.com/huggingface/safetensors.git\n",
        "!git clone https://github.com/huggingface/semver-release-action.git\n",
        "!git clone https://github.com/huggingface/setfit.git\n",
        "!git clone https://github.com/huggingface/simulate.git\n",
        "!git clone https://github.com/huggingface/snapchat-lens-api.git\n",
        "!git clone https://github.com/huggingface/speechbox.git\n",
        "!git clone https://github.com/huggingface/spm_precompiled.git\n",
        "!git clone https://github.com/huggingface/stable-baselines3.git\n",
        "!git clone https://github.com/huggingface/swift-coreml-diffusers.git\n",
        "!git clone https://github.com/huggingface/swift-coreml-transformers.git\n",
        "!git clone https://github.com/huggingface/tensorboard.git\n",
        "!git clone https://github.com/huggingface/test_gh_secret.git\n",
        "!git clone https://github.com/huggingface/text-generation-inference.git\n",
        "!git clone https://github.com/huggingface/tflite-android-transformers.git\n",
        "!git clone https://github.com/huggingface/tokenizers.git\n",
        "!git clone https://github.com/huggingface/torchMoji.git\n",
        "!git clone https://github.com/huggingface/transfer-learning-conv-ai.git\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!git clone https://github.com/huggingface/transformers-bloom-inference.git\n",
        "!git clone https://github.com/huggingface/transformers_bloom_parallel.git\n",
        "!git clone https://github.com/huggingface/tune.git\n",
        "!git clone https://github.com/huggingface/unity-api.git\n",
        "!git clone https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces.git\n",
        "!git clone https://github.com/huggingface/ViZDoom.git\n",
        "!git clone https://github.com/huggingface/widgets-server.git\n",
        "!git clone https://github.com/huggingface/workshops.git\n",
        "!git clone https://github.com/huggingface/xlnet.git\n",
        "!git clone https://github.com/huggingface/yaml-ast-parser.git\n",
        "!git clone https://github.com/huggingface/zapier.git\n"
      ],
      "metadata": {
        "id": "cc98Jm6qLLiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# move all cloned repositories to git_clones"
      ],
      "metadata": {
        "id": "JHjIwSiGnqj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Specify the source and destination directories\n",
        "source_dir = '/content/'\n",
        "destination_dir = '/content/drive/MyDrive/ML_TRAINING/Git_Data_Gather/git_clones'\n",
        "\n",
        "# Specify the directories to exclude\n",
        "exclude_dirs = ['/content/sample_data']\n",
        "\n",
        "# Get all directories in the source directory\n",
        "all_dirs = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
        "\n",
        "# Filter out the directories to exclude\n",
        "dirs_to_move = [d for d in all_dirs if d not in exclude_dirs]\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Move the directories\n",
        "for dir_to_move in dirs_to_move:\n",
        "    shutil.move(os.path.join(source_dir, dir_to_move), destination_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "XDlA2A7Dnns-",
        "outputId": "82e7e090-af4c-4d25-8d05-f580ba1f08c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 18] Invalid cross-device link: '/content/drive' -> '/content/drive/MyDrive/ML_TRAINING/Git_Data_Gather/git_clones/drive'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8fec49fe7b14>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Move the directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdir_to_move\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs_to_move\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_to_move\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_destinsrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 raise Error(\"Cannot move a directory '%s' into itself\"\n\u001b[0m\u001b[1;32m    825\u001b[0m                             \" '%s'.\" % (src, dst))\n\u001b[1;32m    826\u001b[0m             if (_is_immutable(src)\n",
            "\u001b[0;31mError\u001b[0m: Cannot move a directory '/content/drive' into itself '/content/drive/MyDrive/ML_TRAINING/Git_Data_Gather/git_clones'."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloned Repository to Text generator"
      ],
      "metadata": {
        "id": "MA_mNyCjpiqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import fnmatch\n",
        "\n",
        "def get_ignore_list(ignore_file_path):\n",
        "    ignore_list = []\n",
        "    with open(ignore_file_path, 'r') as ignore_file:\n",
        "        for line in ignore_file:\n",
        "            if sys.platform == \"win32\":\n",
        "                line = line.replace(\"/\", \"\\\\\")\n",
        "            ignore_list.append(line.strip())\n",
        "    return ignore_list\n",
        "\n",
        "def should_ignore(file_path, ignore_list):\n",
        "    for pattern in ignore_list:\n",
        "        if fnmatch.fnmatch(file_path, pattern):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def process_repository(repo_path, ignore_list, output_file):\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            relative_file_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "            if not should_ignore(relative_file_path, ignore_list):\n",
        "                with open(file_path, 'r', errors='ignore') as file:\n",
        "                    contents = file.read()\n",
        "                output_file.write(f'File: {relative_file_path}\\nContents:\\n{contents}\\n')\n",
        "\n",
        "    output_file.write('---\\n')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_dir = input(\"Enter the directory containing your repositories: \")\n",
        "\n",
        "    output_file_path = os.path.join(base_dir, 'output.txt')\n",
        "    ignore_file_path = os.path.join(base_dir, \".gptignore\")\n",
        "    if sys.platform == \"win32\":\n",
        "        ignore_file_path = ignore_file_path.replace(\"/\", \"\\\\\")\n",
        "\n",
        "    if not os.path.exists(ignore_file_path):\n",
        "        # try and use the .gptignore file in the current directory as a fallback.\n",
        "        HERE = os.path.dirname(os.path.abspath(sys.argv[0]))\n",
        "        ignore_file_path = os.path.join(HERE, \".gptignore\")\n",
        "\n",
        "    if os.path.exists(ignore_file_path):\n",
        "        ignore_list = get_ignore_list(ignore_file_path)\n",
        "    else:\n",
        "        ignore_list = []\n",
        "\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        for repo in os.listdir(base_dir):\n",
        "            repo_path = os.path.join(base_dir, repo)\n",
        "            if os.path.isdir(repo_path):\n",
        "                process_repository(repo_path, ignore_list, output_file)\n",
        "\n",
        "    print(f\"Repository contents written to {output_file_path}.\")\n"
      ],
      "metadata": {
        "id": "Dy1M4ZACpiY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edd7810-0be6-4b1d-8126-6195cc06cb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the directory containing your repositories: /content/pytorch_clones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloned Repository to Json generator- single output file"
      ],
      "metadata": {
        "id": "P_FEox0K8VTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import fnmatch\n",
        "import json\n",
        "\n",
        "def get_ignore_list(ignore_file_path):\n",
        "    ignore_list = []\n",
        "    with open(ignore_file_path, 'r') as ignore_file:\n",
        "        for line in ignore_file:\n",
        "            if sys.platform == \"win32\":\n",
        "                line = line.replace(\"/\", \"\\\\\")\n",
        "            ignore_list.append(line.strip())\n",
        "    return ignore_list\n",
        "\n",
        "def should_ignore(file_path, ignore_list):\n",
        "    for pattern in ignore_list:\n",
        "        if fnmatch.fnmatch(file_path, pattern):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def process_repository(repo_path, ignore_list, output_file):\n",
        "    data = []\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            relative_file_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "            if not should_ignore(relative_file_path, ignore_list):\n",
        "                with open(file_path, 'r', errors='ignore') as file:\n",
        "                    contents = file.read()\n",
        "                data.append({\n",
        "                    \"file_path\": relative_file_path,\n",
        "                    \"contents\": contents\n",
        "                })\n",
        "\n",
        "    json_data = json.dumps(data)\n",
        "\n",
        "    output_file.write(json_data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_dir = input(\"Enter the directory containing your repositories: \")\n",
        "\n",
        "    output_file_path = os.path.join(base_dir, 'output.json')\n",
        "    ignore_file_path = os.path.join(base_dir, \".gptignore\")\n",
        "    if sys.platform == \"win32\":\n",
        "        ignore_file_path = ignore_file_path.replace(\"/\", \"\\\\\")\n",
        "\n",
        "    if not os.path.exists(ignore_file_path):\n",
        "        # try and use the .gptignore file in the current directory as a fallback.\n",
        "        HERE = os.path.dirname(os.path.abspath(sys.argv[0]))\n",
        "        ignore_file_path = os.path.join(HERE, \".gptignore\")\n",
        "\n",
        "    if os.path.exists(ignore_file_path):\n",
        "        ignore_list = get_ignore_list(ignore_file_path)\n",
        "    else:\n",
        "        ignore_list = []\n",
        "\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        for repo in os.listdir(base_dir):\n",
        "            repo_path = os.path.join(base_dir, repo)\n",
        "            if os.path.isdir(repo_path):\n",
        "                process_repository(repo_path, ignore_list, output_file)\n",
        "\n",
        "    print(f\"Repository contents written to {output_file_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzoF9R4A8R8B",
        "outputId": "620f520c-f8e6-489f-a8e0-571746a12aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the directory containing your repositories: /content/pytorch_clones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K7ce7xDQMzxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def get_repo_info(repo_dir):\n",
        "    \"\"\"Gets information about a GitHub repository.\n",
        "\n",
        "    Args:\n",
        "        repo_dir: The directory containing the cloned GitHub repository.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing information about the repository.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the repository name.\n",
        "    repo_name = os.path.basename(repo_dir)\n",
        "\n",
        "    # Get the list of files in the repository.\n",
        "    files = os.listdir(repo_dir)\n",
        "\n",
        "    # Get the list of README files in the repository.\n",
        "    readme_files = [\n",
        "        file\n",
        "        for file in files\n",
        "        if file.endswith('README.md') or file.endswith('README.rst')\n",
        "    ]\n",
        "\n",
        "    # Get the contents of the README files.\n",
        "    readme_contents = []\n",
        "    for readme_file in readme_files:\n",
        "        with open(os.path.join(repo_dir, readme_file)) as f:\n",
        "            readme_contents.append(f.read())\n",
        "\n",
        "    # Get the repository description.\n",
        "    description = ''\n",
        "    for readme_content in readme_contents:\n",
        "        description += readme_content\n",
        "\n",
        "    # Get the repository license.\n",
        "    license = None\n",
        "    for readme_content in readme_contents:\n",
        "        if 'license' in readme_content:\n",
        "            license = readme_content.split('name=')[1].strip()\n",
        "\n",
        "    # Get the repository homepage.\n",
        "    homepage = None\n",
        "    for readme_content in readme_contents:\n",
        "        if 'homepage=' in readme_content:\n",
        "            homepage = readme_content.split('url=')[1].strip()\n",
        "\n",
        "    # Get the repository keywords.\n",
        "    keywords = []\n",
        "    for readme_content in readme_contents:\n",
        "        if 'keywords=' in readme_content:\n",
        "            keywords = readme_content.split('keywords=')[1].strip().split(',')\n",
        "\n",
        "    # Get the repository tags.\n",
        "    tags = []\n",
        "    for readme_content in readme_contents:\n",
        "        if 'tags=' in readme_content:\n",
        "            tags = readme_content.split('tags=')[1].strip().split(',')\n",
        "\n",
        "    repo_info = {\n",
        "        'name': repo_name,\n",
        "        'description': description,\n",
        "        'license': license,\n",
        "        'homepage': homepage,\n",
        "        'keywords': keywords,\n",
        "        'tags': tags,\n",
        "    }\n",
        "\n",
        "    return repo_info\n",
        "\n",
        "def main():\n",
        "    repo_dirs = ['path/to/repo1', 'path/to/repo2', 'path/to/repo3']\n",
        "\n",
        "    # Create a .json file for each repository.\n",
        "    for repo_dir in repo_dirs:\n",
        "        repo_info = get_repo_info(repo_dir)\n",
        "        with open(os.path.join(repo_dir, 'repo.json'), 'w') as f:\n",
        "            json.dump(repo_info, f)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "moTH_klmMzS3",
        "outputId": "1772e135-def6-4445-d7fa-2a6bf8d0f6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e04b0b9a4e4c>\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-e04b0b9a4e4c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Create a .json file for each repository.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrepo_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrepo_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mrepo_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_repo_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repo.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-e04b0b9a4e4c>\u001b[0m in \u001b[0;36mget_repo_info\u001b[0;34m(repo_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Get the list of files in the repository.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Get the list of README files in the repository.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/repo1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "def get_github_file_contents(repo_url, token):\n",
        "    repo_name = '/'.join(repo_url.split('/')[-2:]) # Get the username/repo part of the URL\n",
        "    api_url = f\"https://api.github.com/repos/{repo_name}/git/trees/master?recursive=1\"\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "\n",
        "    repo_data = []\n",
        "\n",
        "    try:\n",
        "        response = requests.get(api_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        repo_json = response.json()\n",
        "\n",
        "        for file in repo_json['tree']:\n",
        "            if file['type'] == 'blob':\n",
        "                file_api_url = file['url']\n",
        "                response = requests.get(file_api_url, headers=headers)\n",
        "                response.raise_for_status()\n",
        "                file_json = response.json()\n",
        "\n",
        "                file_data = {\n",
        "                    \"filename\": file['path'],\n",
        "                    \"content\": file_json['content']\n",
        "                }\n",
        "                repo_data.append(file_data)\n",
        "\n",
        "    except requests.exceptions.HTTPError as err:\n",
        "        print(f\"HTTP error occurred: {err}\")\n",
        "\n",
        "    except Exception as err:\n",
        "        print(f\"An error ocurred: {err}\")\n",
        "\n",
        "    return repo_data\n",
        "\n",
        "token = getpass.getpass(prompt='Please enter your GitHub token: ')\n",
        "repo_urls_input = input(\"Please enter a list of GitHub repository links, separated by a comma: \")\n",
        "repo_urls = [url.strip() for url in repo_urls_input.split(\",\")]\n",
        "\n",
        "output_file_path = 'output.json'\n",
        "\n",
        "for repo_url in repo_urls:\n",
        "    repo_data = get_github_file_contents(repo_url, token)\n",
        "\n",
        "    with open(output_file_path, 'a') as output_file:\n",
        "        json.dump(repo_data, output_file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "4KoHVWtFGYKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Cloned Repository to Seperate JSON files"
      ],
      "metadata": {
        "id": "5R4oSBUjz3Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import mimetypes\n",
        "\n",
        "def process_repository(repo_path, ignore_list):\n",
        "    data = []\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            relative_file_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "            #if not should_ignore(relative_file_path, ignore_list):\n",
        "         #       file_type, _ = mimetypes.guess_type(file_path)\n",
        "         #       if file_type is None or not file_type.startswith('text'):\n",
        "         #           continue  # skip non-text files\n",
        "\n",
        "            try:\n",
        "                with open(file_path, 'r', errors='ignore') as file:\n",
        "                    contents = file.read()\n",
        "                data.append({\n",
        "                    'file_path': relative_file_path,\n",
        "                    'contents': contents,\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Get the repository name from the repo_path\n",
        "    repo_name = os.path.basename(repo_path)\n",
        "\n",
        "    # Use the repository name in the output file name\n",
        "    output_file_path = os.path.join(repo_path, f'{repo_name}_output.json')\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        json.dump(data, output_file)\n",
        "\n",
        "# Define your ignore list and repository path\n",
        "ignore_list = [\"ignore_me.txt\", \"ignore_me_too.txt\"]\n",
        "repo_path = \"/content/pytorch_clones\"\n",
        "\n",
        "process_repository(repo_path, ignore_list)\n"
      ],
      "metadata": {
        "id": "c4qS1TkUz_19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Git Data Gatherer and compiler"
      ],
      "metadata": {
        "id": "4vQgbiI-8V3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPFAHXE78Jur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "96f71835-3667-4597-8580-80503704cf0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch repos from GitHub for user -f. HTTP status code: 404\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import requests\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "\n",
        "os.environ['GITHUB_TOKEN'] = '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'\n",
        "GITHUB_API = \"https://api.github.com/users/{username}/repos\"\n",
        "CLONE_DIR = \"./repos/{username}/{repo_name}\"\n",
        "GIT_CLONE_CMD = \"git clone {repo_url} {clone_dir}\"\n",
        "PROCESS_REPO_CMD = \"python /content/drive/MyDrive/ML_TRAINING/Git_Data_Gather/git_to_text.py {repo_path}\"\n",
        "\n",
        "def get_github_repos(username, token):\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "    response = requests.get(GITHUB_API.format(username=username), headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch repos from GitHub for user {username}. HTTP status code: {response.status_code}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    return response.json()\n",
        "\n",
        "def clone_and_process_repos(repos, username):\n",
        "    for repo in repos:\n",
        "        clone_dir = CLONE_DIR.format(username=username, repo_name=repo['name'])\n",
        "        clone_cmd = GIT_CLONE_CMD.format(repo_url=repo['clone_url'], clone_dir=clone_dir)\n",
        "        process_repo_cmd = PROCESS_REPO_CMD.format(repo_path=clone_dir)\n",
        "\n",
        "        subprocess.run(clone_cmd, shell=True, check=True)\n",
        "        subprocess.run(process_repo_cmd, shell=True, check=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python git_to_text.py <github_username>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    username = sys.argv[1]\n",
        "    token = os.getenv('GITHUB_TOKEN')\n",
        "\n",
        "    if not token:\n",
        "        print(\"Please set the GITHUB_TOKEN environment variable.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    repos = get_github_repos(username, token)\n",
        "    clone_and_process_repos(repos, username)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ML_TRAINING/Git_Data_Gather/git_data_gather.py https://github.com/deepmind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfemKDvD9i2D",
        "outputId": "04816a78-92e9-48e7-84e2-55745e0cbf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch repos from GitHub for user https://github.com/deepmind. HTTP status code: 404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyl6lOn5ZoiE",
        "outputId": "ce3a26a3-35eb-4dae-c9c2-2b6e32f2cec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, safetensors, xxhash, dill, multiprocess, huggingface-hub, transformers, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 huggingface-hub-0.15.1 multiprocess-0.70.14 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ijson"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Eg6cb-lfxDh",
        "outputId": "6607017e-5aca-4ff5-f0b1-0730039a87a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ijson\n",
            "  Downloading ijson-3.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/112.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ijson\n",
            "Successfully installed ijson-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the data\n",
        "with open('/content/drive/MyDrive/Repository_Clones/output.json', 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Separate all JSON objects or lists of JSON objects onto different lines\n",
        "data = data.replace('}][{', '}]\\n[{')\n",
        "\n",
        "# Save the separated data to a new JSON file\n",
        "with open('separated.json', 'w') as f:\n",
        "    f.write(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "S0c9KOfaguPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define a function to load a large JSON file in chunks\n",
        "def load_large_json(file_path, chunk_size=1000):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        chunk = []\n",
        "        for line in f:\n",
        "            try:\n",
        "                # Try to load each line as a separate JSON object\n",
        "                chunk.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                # If an error occurs, skip this line\n",
        "                continue\n",
        "            if len(chunk) >= chunk_size:\n",
        "                data.append(chunk)\n",
        "                chunk = []\n",
        "        if chunk:\n",
        "            data.append(chunk)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Load the JSON file in chunks\n",
        "chunks = load_large_json('/content/separated.json')\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a function to preprocess the data\n",
        "def preprocess_data(examples):\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(examples['contents'], truncation=True, padding='max_length', max_length=512)\n",
        "    # Prepare the labels for a causal language modeling task\n",
        "    inputs['labels'] = inputs['input_ids'].copy()\n",
        "    return inputs\n",
        "\n",
        "# Convert each chunk to a Dataset and process it separately\n",
        "datasets = []\n",
        "for chunk in chunks:\n",
        "    dataset = Dataset.from_dict({k: [dic[k] for dic in chunk] for k in chunk[0].keys()})\n",
        "    # Preprocess the dataset\n",
        "    dataset = dataset.map(preprocess_data, batched=True)\n",
        "    datasets.append(dataset)\n",
        "\n",
        "# Now, `datasets` is a list of Dataset objects, each representing a chunk of the data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "XXUpBHt8Zosf",
        "outputId": "8de9b2f2-c7e8-4d6b-e626-ac7836ce18f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-07de12cce48b>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Preprocess the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Path to your JSON file\n",
        "file_path = '/content/separated.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        try:\n",
        "            # Try to decode the JSON object\n",
        "            json.loads(line)\n",
        "        except json.JSONDecodeError as e:\n",
        "            # If an error occurs, print the line number and the error\n",
        "            print(f'Error on line {i}: {e}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSydbuLoycK",
        "outputId": "6621f305-b1d5-495b-9592-9457db031b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error on line 2: Invalid control character at: line 1 column 10001903 (char 10001902)\n",
            "Error on line 3: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
            "Error on line 4: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
            "Error on line 17: Invalid control character at: line 1 column 11799746 (char 11799745)\n",
            "Error on line 18: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
            "Error on line 21: Invalid control character at: line 1 column 243190284 (char 243190283)\n",
            "Error on line 22: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
            "Error on line 25: Invalid control character at: line 1 column 30468533 (char 30468532)\n",
            "Error on line 26: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
            "Error on line 66: Invalid control character at: line 1 column 48483773 (char 48483772)\n",
            "Error on line 67: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n"
          ]
        }
      ]
    }
  ]
}