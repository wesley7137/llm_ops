{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 0.0.310\n",
      "Uninstalling langchain-0.0.310:\n",
      "  Successfully uninstalled langchain-0.0.310\n",
      "Collecting langchain==0.0.310\n",
      "  Obtaining dependency information for langchain==0.0.310 from https://files.pythonhosted.org/packages/63/d1/84b1eabf897f2d964b682f55c4fc79e9c81e7367013c816f2f7f00020f5e/langchain-0.0.310-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.0.310-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.40 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (0.0.52)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (1.23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (1.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from langchain==0.0.310) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.310) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.310) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.310) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.310) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.310) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.310) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from anyio<4.0->langchain==0.0.310) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from anyio<4.0->langchain==0.0.310) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from anyio<4.0->langchain==0.0.310) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.310) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.310) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.310) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.310) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from requests<3,>=2->langchain==0.0.310) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from requests<3,>=2->langchain==0.0.310) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.310) (3.0.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.310) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.310) (1.0.0)\n",
      "Using cached langchain-0.0.310-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: langchain\n",
      "Successfully installed langchain-0.0.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lama-cpp-python (c:\\users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall langchain -y\n",
    "!pip install langchain==0.0.310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing API keys and environment variables...\n",
      "API keys and environment variables Initialized..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Importing necessary modules and classes for LangChain Agent\n",
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory, CombinedMemory, ConversationSummaryMemory\n",
    "from langchain.memory.chat_message_histories import FileChatMessageHistory  # Make sure to import this correctly\n",
    "from langchain.chains import LLMChain, ConversationChain, ReduceDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import threading\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging\n",
    "import os\n",
    "from collections import deque\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Set up environment\n",
    "# Import necessary modules and packages to set up the LLM agent.\n",
    "\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain.chains import LLMChain, ReduceDocumentsChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.mapreduce import MapReduceChain, MapReduceDocumentsChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import BaseLLM, LlamaCpp, GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "from langchain_experimental.autonomous_agents import BabyAGI\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "import re\n",
    "\n",
    "\n",
    "    # ENVIRONMENT VARIABLES\n",
    "print(\"Initializing API keys and environment variables...\")\n",
    "print(\"API keys and environment variables Initialized..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM and Custom Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM...\n",
      "Loading LLM...\n",
      "LLM initialized successfully!\n",
      "LLM initialized successfully!\n",
      "Successfully defined LLM chain!\n",
      "\u001b[1mTextGen\u001b[0m\n",
      "Params: {'model_url': 'http://127.0.0.1:5000', 'max_new_tokens': 250, 'do_sample': True, 'temperature': 1.3, 'top_p': 0.1, 'typical_p': 1, 'epsilon_cutoff': 0, 'eta_cutoff': 0, 'repetition_penalty': 1.18, 'top_k': 40, 'min_length': 0, 'no_repeat_ngram_size': 0, 'num_beams': 1, 'penalty_alpha': 0, 'length_penalty': 1, 'early_stopping': False, 'seed': -1, 'add_bos_token': True, 'truncation_length': 2048, 'ban_eos_token': False, 'skip_special_tokens': True, 'stopping_strings': []}\n",
      "Initializing embeddings model...\n",
      "Embeddings model initialized successfully.\n",
      "Output parser initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "## LLM setup and initialization\n",
    "# Existing code\n",
    "print(\"Initializing LLM...\")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import BaseLLM, LlamaCpp, GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Modified to include EOS token and restructured for specified format\n",
    "template = \"\"\"<system message>\n",
    "USER: {prompt}.\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "# Existing code, adapted for the new format\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"prompt\"])\n",
    "\n",
    "# ... rest of your existing code ...\n",
    "# Initialize LLM\n",
    "print(\"Loading LLM...\")\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "from langchain.llms import textgen\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "\n",
    "from langchain.llms import TextGen\n",
    "llm = TextGen(model_url=\"http://127.0.0.1:5000\")\n",
    "\n",
    "print(\"LLM initialized successfully!\")\n",
    "\n",
    "print(\"LLM initialized successfully!\")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(\"Successfully defined LLM chain!\")\n",
    "print(llm)\n",
    "\n",
    "####################################################################################\n",
    "# INITIALIZE THE EMBEDDINGS MODEL\n",
    "\n",
    "\n",
    "print(\"Initializing embeddings model...\")\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "print(\"Embeddings model initialized successfully.\")\n",
    "\n",
    "####################################################################################    \n",
    "\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Extract lines that look like Python code\n",
    "        python_code = []\n",
    "        for line in llm_output.split('\\n'):\n",
    "            if line.strip():  # filter out empty lines\n",
    "                python_code.append(line)\n",
    "        \n",
    "        formatted_output = '\\n'.join(python_code)\n",
    "        return AgentFinish(\n",
    "            return_values={\"output\": formatted_output},\n",
    "            log=formatted_output,\n",
    "        )\n",
    "        \n",
    "output_parser = CustomOutputParser()\n",
    "print(\"Output parser initialized successfully.\")\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output parser initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "##################################################################################3\n",
    "#INITIALIZE OUTPUT PARSER\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Extract lines that look like Python code\n",
    "        python_code = []\n",
    "        for line in llm_output.split('\\n'):\n",
    "            if line.strip():  # filter out empty lines\n",
    "                python_code.append(line)\n",
    "        \n",
    "        formatted_output = '\\n'.join(python_code)\n",
    "        return AgentFinish(\n",
    "            return_values={\"output\": formatted_output},\n",
    "            log=formatted_output,\n",
    "        )\n",
    "        \n",
    "output_parser = CustomOutputParser()\n",
    "print(\"Output parser initialized successfully.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from langchain.tools import MoveFileTool\n",
    "from langchain.tools import FileSearchTool\n",
    "from langchain.tools import ListDirectoryTool\n",
    "from langchain.tools import ReadFileTool\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.tools import ArxivQueryRun\n",
    "from langchain.tools import CopyFileTool\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain.tools import JsonListKeysTool\n",
    "from langchain.tools import WriteFileTool\n",
    "from langchain.tools import ClickTool\n",
    "from langchain.tools import VectorStoreQATool\n",
    "from langchain.tools import DeleteFileTool\n",
    "from langchain.tools import ExtractTextTool\n",
    "from langchain.tools import ExtractHyperlinksTool\n",
    "from langchain.tools import GetElementsTool\n",
    "from langchain.tools import NavigateBackTool\n",
    "import langchain\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"MoveFileTool\",\n",
    "        func=langchain.tools.file_management.move.MoveFileTool.run,\n",
    "        description=\"Move files from one location to another\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"FileSearchTool\",\n",
    "        func=langchain.tools.file_management.file_search.FileSearchTool.run,\n",
    "        description=\"Search for files in a subdirectory that match a regex pattern\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ListDirectoryTool\",\n",
    "        func=langchain.tools.file_management.list_dir.ListDirectoryTool.run,\n",
    "        description=\"List files and directories in a specified folder\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ReadFileTool\",\n",
    "        func=langchain.tools.file_management.read.ReadFileTool.run,\n",
    "        description=\"Read content from a file\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"WikipediaQueryRun\",\n",
    "        func=langchain.tools.wikipedia.tool.WikipediaQueryRun.run,\n",
    "        description=\"Query information from Wikipedia\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ArxivQueryRun\",\n",
    "        func=langchain.tools.arxiv.tool.ArxivQueryRun.run,\n",
    "        description=\"Query information from Arxiv\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"CopyFileTool\",\n",
    "        func=langchain.tools.file_management.copy.CopyFileTool.run,\n",
    "        description=\"Copy files from one location to another\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"DuckDuckGoSearchResults\",\n",
    "        func=langchain.tools.ddg_search.tool.DuckDuckGoSearchResults.run,\n",
    "        description=\"Search for information using DuckDuckGo\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"JsonListKeysTool\",\n",
    "        func=langchain.tools.json.tool.JsonListKeysTool.run,\n",
    "        description=\"List keys in a JSON object\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"WriteFileTool\",\n",
    "        func=langchain.tools.file_management.write.WriteFileTool.run,\n",
    "        description=\"Write content to a file\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ClickTool\",\n",
    "        func=langchain.tools.playwright.click.ClickTool.run,\n",
    "        description=\"Click on an element with a given CSS selector\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"VectorStoreQATool\",\n",
    "        func=langchain.tools.vectorstore.tool.VectorStoreQATool.run,\n",
    "        description=\"Tool for the VectorDBQA chain\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"DeleteFileTool\",\n",
    "        func=langchain.tools.file_management.delete.DeleteFileTool.run,\n",
    "        description=\"Delete specified files\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ExtractTextTool\",\n",
    "        func=langchain.tools.playwright.extract_text.ExtractTextTool.run,\n",
    "        description=\"Extracts text from the specified HTML elements\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ExtractHyperlinksTool\",\n",
    "        func=langchain.tools.playwright.extract_hyperlinks.ExtractHyperlinksTool.run,\n",
    "        description=\"Extract hyperlinks from a webpage\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"GetElementsTool\",\n",
    "        func=langchain.tools.playwright.get_elements.GetElementsTool.run,\n",
    "        description=\"Gets elements from a webpage using a CSS selector\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"NavigateBackTool\",\n",
    "        func=langchain.tools.playwright.navigate_back.NavigateBackTool.run,\n",
    "        description=\"Navigates back to the previous webpage\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Set up the Agent\n",
    "# Combine everything to set up the agent.\n",
    "\n",
    "# Define the list of tool names\n",
    "tools_names = [\n",
    "    \"MoveFileTool\",\n",
    "    \"FileSearchTool\",\n",
    "    \"ListDirectoryTool\",\n",
    "    \"ReadFileTool\",\n",
    "    \"WikipediaQueryRun\",\n",
    "    \"ArxivQueryRun\",\n",
    "    \"CopyFileTool\",\n",
    "    \"DuckDuckGoSearchResults\",\n",
    "    \"JsonListKeysTool\",\n",
    "    \"WriteFileTool\",\n",
    "    \"ClickTool\",\n",
    "    \"VectorStoreQATool\",\n",
    "    \"DeleteFileTool\",\n",
    "    \"ExtractTextTool\",\n",
    "    \"ExtractHyperlinksTool\",\n",
    "    \"GetElementsTool\",\n",
    "    \"NavigateBackTool\"\n",
    "]\n",
    "\n",
    "\n",
    "stop = [\"\\nObservation:\"]  # Example stop sequence\n",
    "allowed_tools = tools_names\n",
    "# Example allowed tools\n",
    "\n",
    "# Create an instance of LLMSingleActionAgent\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=tools\n",
    ")\n",
    "\n",
    "\n",
    "llm_single_action_agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets up the prompt template which instructs the LLM on what to do.\n",
    "# The template includes placeholders for tools, intermediate steps, and user input.\n",
    "\n",
    "\n",
    "\n",
    "# Define the base template\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "{tools}\n",
    "Use the following format:\n",
    "Question: {initial_user_input}\n",
    "{agent_scratchpad}.\n",
    "Only when you have successfully and fully completed the user's input task, print 'TASK COMPLETED SUCCESSFULLY, BOSS'\"\"\"\n",
    "\n",
    "# Define the CustomPromptTemplate class\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    template: str  # The template to use\n",
    "    tools: List[Tool]  # The list of tools available\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\", [])\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        kwargs[\"tools\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "\n",
    "# Create an instance of CustomPromptTemplate\n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    input_variables=[\"initial_user_input\", \"intermediate_steps\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define agent prompts and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from typing import List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "class CustomAgent(LLMSingleActionAgent):\n",
    "    custom_name: str = Field(..., description=\"Custom name for the agent\")\n",
    "    prompt_template: str = Field(..., description=\"Prompt template for the agent\")\n",
    "    system_message: str = Field(..., description=\"System message for the agent\")\n",
    "\n",
    "    class Config:\n",
    "        extra = \"allow\"\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Quantum Computing Expert Agent Function\n",
    "def run_quantum_computing_expert_agent(new_task_description):\n",
    "    print(\"Running Quantum Computing Expert Agent...\")\n",
    "    quantum_computing_expert_output = quantum_computing_expert_agent_executor.run(new_task_description)\n",
    "    return quantum_computing_expert_output\n",
    "\n",
    "\n",
    "# Artificial Intelligence Expert Agent Function\n",
    "def run_ai_expert_agent(new_task_description):\n",
    "    ai_expert_output = ai_expert_agent_executor.run(new_task_description)\n",
    "    return ai_expert_output\n",
    "\n",
    "\n",
    "# Neuroscientist Agent Function\n",
    "def run_neuroscientist_agent(new_task_description):\n",
    "    print(\"Running Neuroscientist Agent...\")\n",
    "    neuroscientist_output = neuroscientist_agent_executor.run(new_task_description)\n",
    "    return neuroscientist_output\n",
    "    print(f\"Neuroscientist Output: {neuroscientist_output}\")\n",
    "\n",
    "\n",
    "# Nanotechnologist Agent Function\n",
    "def run_nanotechnologist_agent(new_task_description):\n",
    "    print(\"Running Nanotechnologist Agent...\")\n",
    "    nanotechnologist_output = nanotechnologist_agent_executor.run(new_task_description)\n",
    "    return nanotechnologist_output\n",
    "    print(f\"Nanotechnologist Output: {nanotechnologist_output}\")\n",
    "\n",
    "\n",
    "# Computer Scientist Agent Function\n",
    "def run_computer_scientist_agent(new_task_description):\n",
    "    print(\"Running Computer Scientist Agent...\")\n",
    "    computer_scientist_output = computer_scientist_agent_executor.run(new_task_description)\n",
    "    return computer_scientist_output\n",
    "    print(f\"Computer Scientist Output: {computer_scientist_output}\")\n",
    "\n",
    "# Bioengineer Agent Function\n",
    "def run_bioengineer_agent(new_task_description):\n",
    "    print(\"Running Bioengineer Agent...\")\n",
    "    bioengineer_output = bioengineer_agent_executor.run(new_task_description)\n",
    "    return bioengineer_output\n",
    "    print(f\"Bioengineer Output: {bioengineer_output}\")\n",
    "\n",
    "# Psychologist Agent Function\n",
    "def run_psychologist_agent(new_task_description):\n",
    "    print(\"Running Psychologist Agent...\")\n",
    "    psychologist_output = psychologist_agent_executor.run(new_task_description)\n",
    "    return psychologist_output\n",
    "    print(f\"Psychologist Output: {psychologist_output}\")    \n",
    "\n",
    "\n",
    "# Genomist Agent Function\n",
    "def run_genomist_agent(new_task_description):\n",
    "    print(\"Running Genomist Agent...\")\n",
    "    genomist_output = genomist_agent_executor.run(new_task_description)\n",
    "    return genomist_output\n",
    "    print(f\"Genomist Output: {genomist_output}\")\n",
    "\n",
    "\n",
    "# Robotics Engineer/Nanotech Fabrications Specialist Agent Function\n",
    "def run_robotics_engineer_agent(new_task_description):\n",
    "    print(\"Running Robotics Engineer Agent...\")\n",
    "    robotics_engineer_output = robotics_engineer_agent_executor.run(new_task_description)\n",
    "    return robotics_engineer_output\n",
    "    print(f\"Robotics Engineer Output: {robotics_engineer_output}\")\n",
    "\n",
    "\n",
    "# Function for Software Engineer Agent\n",
    "def run_software_engineer_agent(new_task_description):\n",
    "    print(f\"Debug: Entered run_software_engineer_agent with task={new_task_description}\")\n",
    "    print(f\"Debug: Processing with some internal logic\")\n",
    "    software_engineer_output = software_engineer_agent_executor(new_task_description)\n",
    "    print(f\"Debug: Preparing to exit run_software_engineer_agent with output={software_engineer_output}\")\n",
    "    return software_engineer_output\n",
    "\n",
    "# 10X Coder Agent Function\n",
    "def run_10x_coder_agent(new_task_description):\n",
    "    print(\"Running 10X Coder Agent...\")\n",
    "    coder_10x_output = coder_10x_agent_executor.run(new_task_description)\n",
    "    return coder_10x_output\n",
    "    print(f\"10X Coder Output: {coder_10x_output}\")\n",
    "\n",
    "# Debugger Agent Function\n",
    "def run_debugger_agent(new_task_description):\n",
    "    debugger_output = debugger_agent_executor.run(new_task_description)\n",
    "    return debugger_output\n",
    "\n",
    "\n",
    "# Function for Business Agent\n",
    "def run_business_agent(new_task_description):\n",
    "    print(\"Running Business Agent...\")\n",
    "    business_output = business_agent_executor(new_task_description)\n",
    "    print(f\"Business Output: {business_output}\")\n",
    "    return business_output\n",
    "\n",
    "\n",
    "# Function for Debugger Agent\n",
    "def run_debugger_agent(new_task_description):\n",
    "    print(\"Running Debugger Agent...\")\n",
    "    debugger_output = debugger_agent_executor(new_task_description)\n",
    "    print(f\"Debugger Output: {debugger_output}\")\n",
    "    return debugger_output\n",
    "\n",
    "# Function for Summarizer Agent\n",
    "# Function for Summarizer Agent\n",
    "def run_summarizer_agent(new_task_description):\n",
    "    print(f\"New Task Description: {new_task_description}\")  # Debugging print statement\n",
    "    try:\n",
    "        print(\"Running Summarizer Agent...\")\n",
    "        summarizer_output = summarizer_agent_executor(new_task_description)\n",
    "        print(f\"Summarizer Output: {summarizer_output}\")  # Debugging print statement\n",
    "        if 'relevant_output' not in summarizer_output:\n",
    "            print(\"Warning: 'relevant_output' key missing in summarizer_output\")  # Debugging print statement\n",
    "            # Handle the missing key here, if necessary\n",
    "            return {'task': new_task_description, 'relevant_output': ''}\n",
    "        return summarizer_output\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")  # Debugging print statement\n",
    "        return {'task': new_task_description, 'relevant_output': f\"Exception: {e}\"}\n",
    "\n",
    "\n",
    "\n",
    "# Function for Scientist Agent\n",
    "def run_scientist_agent(new_task_description):\n",
    "    print(\"Running Scientist Agent...\")\n",
    "    scientist_output = scientist_agent_executor(new_task_description)\n",
    "    print(f\"Scientist Output: {scientist_output}\")\n",
    "    return scientist_output\n",
    "\n",
    "# Function for Mathematician Agent\n",
    "def run_mathematician_agent(new_task_description):\n",
    "    print(\"Running Mathematician Agent...\")\n",
    "    mathematician_output = mathematician_agent_executor(new_task_description)\n",
    "    print(f\"Mathematician Output: {mathematician_output}\")\n",
    "    return mathematician_output\n",
    "\n",
    "# Function for UI Designer Agent\n",
    "def run_ui_designer_agent(new_task_description):\n",
    "    print(\"Running UI Designer Agent...\")\n",
    "    ui_designer_output = ui_designer_agent_executor(new_task_description)\n",
    "    print(f\"UI Designer Output: {ui_designer_output}\")\n",
    "    return ui_designer_output\n",
    "\n",
    "# Function for Critic Agent\n",
    "def run_critic_agent(new_task_description):\n",
    "    print(\"Running Critic Agent...\")\n",
    "    critic_output = critic_agent_executor(new_task_description)\n",
    "    print(f\"Critic Output: {critic_output}\")\n",
    "    return critic_output\n",
    "\n",
    "# Function for Architect Agent\n",
    "def run_architect_agent(agent_output):\n",
    "    try:\n",
    "        print(\"Running Architect Agent...\")\n",
    "        architect_output = architect_agent_executor(new_task_description)\n",
    "        print(f\"Architect Output: {architect_output}\")\n",
    "        return architect_output\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in run_architect_agent: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# Function for Scientist Agent\n",
    "def run_scientist_agent(new_task_description):\n",
    "    print(\"Running Scientist Agent...\")\n",
    "    scientist_output = scientist_agent_executor(new_task_description)\n",
    "    print(f\"Scientist Output: {scientist_output}\")\n",
    "    return scientist_output\n",
    "\n",
    "\n",
    "# Function for Research Agent\n",
    "def run_research_agent(new_task_description):\n",
    "    print(\"Running Research Agent...\")\n",
    "    research_output = research_agent_executor(new_task_description)\n",
    "    print(f\"Research Output: {research_output}\")\n",
    "    return research_output\n",
    "\n",
    "# Function for Tool Agent\n",
    "def run_tool_agent(new_task_description):\n",
    "    print(\"Running Tool Agent...\")\n",
    "    tool_output = tool_agent_executor(new_task_description)\n",
    "    print(f\"Tool Output: {tool_output}\")\n",
    "    return tool_output\n",
    "\n",
    "# Function for CodeExecutorAgent\n",
    "def run_code_executor_agent(new_task_description):\n",
    "    print(\"Running CodeExecutorAgent...\")\n",
    "    code_executor_output = code_executor_agent_executor(new_task_description)\n",
    "    print(f\"CodeExecutor Output: {code_executor_output}\")\n",
    "    return code_executor_output\n",
    "\n",
    "# Function for Director Agent\n",
    "def run_director_agent_agent_roles(new_task_description):\n",
    "    print(\"Running Director Agent...\")\n",
    "    director_output = director_agent_executor(new_task_description)\n",
    "    print(f\"Director Output: {director_output}\")\n",
    "    print(\"Exiting Director Agent...\")  \n",
    "    \n",
    "\n",
    "def run_creator_innovator_agent(new_task_description):\n",
    "    print(\"Running Creator Innovator Agent...\")\n",
    "    creator_innovator_output = creator_innovator_agent_executor.run(new_task_description)\n",
    "    print(f\"Creator Innovator Output: {creator_innovator_output}\")\n",
    "    print(\"Exiting Creator Innovator Agent...\")\n",
    "    return creator_innovator_output\n",
    "\n",
    "\n",
    "\n",
    "# Define the roles and responsibilities of each agent\n",
    "agent_roles = {\n",
    "    'run_software_engineer_agent': 'Responsible for crafting efficient algorithms and writing effective code.',\n",
    "    'run_critic_agent': 'Responsible for reviewing and suggesting improvements.',\n",
    "    'run_architect_agent': 'Responsible for designing the structure of the solution.',\n",
    "    'run_mathematician_agent': 'Responsible for mathematical calculations and logic.',\n",
    "    'run_ui_designer_agent': 'Responsible for designing the user interface.',\n",
    "    'run_scientist_agent': 'Responsible for scientific calculations and validations.',\n",
    "    'run_research_agent': 'Responsible for conducting research and providing insights.',\n",
    "    'run_summarizer_agent': 'Responsible for summarizing the output.',\n",
    "    'run_10x_coder_agent': 'Responsible for rapid development and optimization.',\n",
    "    'run_debugger_agent': 'Responsible for identifying and fixing bugs in the code.',\n",
    "    'run_quantum_computing_expert_agent': 'Responsible for developing and validating quantum algorithms to model cognitive phenomena.',\n",
    "    'run_ai_expert_agent': 'Responsible for developing a quantum AI model that can simulate neural activities and learn in a simulated environment.',\n",
    "    'run_neuroscientist_agent': 'Responsible for mapping neural activity in high resolution and developing methods to convert this data for a quantum computer.',\n",
    "    'run_nanotechnologist_agent': 'Responsible for developing nanobots that can interact with neurons, record neural activity, and stimulate specific neurons.',\n",
    "    'run_creator_innovator_agent': 'Responsible for generating innovative solutions, challenging conventional wisdom, and applying creative problem-solving techniques.',\n",
    "    'run_robotics_engineer_agent': 'Responsible for developing nanobots that can interact with neurons, record neural activity, and stimulate specific neurons.',\n",
    "    'run_genomist_agent': 'Responsible for developing nanobots that can interact with neurons, record neural activity, stimulate specific neurons, and genomic algorithms and sequences.',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# AI Expert Agent Setup\n",
    "ai_expert_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template='You are the AI Expert and responsible for developing a quantum AI model that can simulate neural activities and learn in a simulated environment. Develop advanced and ingenius solutions to the tasks you are given. Utilize cutting-edge algorithms and techniques to simulate neural activities and enable learning in a simulated environment based on the task \"{task}\".'\n",
    ")\n",
    "ai_expert_llm_chain = LLMChain(llm=llm, prompt=ai_expert_prompt)\n",
    "\n",
    "# Assuming output_parser, stop, allowed_tools, and tools are set previously\n",
    "ai_expert_agent = LLMSingleActionAgent(\n",
    "    llm_chain=ai_expert_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "ai_expert_agent_executor = AgentExecutor.from_agent_and_tools(agent=ai_expert_agent, tools=tools, verbose=True)\n",
    "\n",
    "# Summarizer Agent Setup\n",
    "# Summarizer Agent Setup\n",
    "summarizer_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"\"\"# MISSION\n",
    "You are a Sparse Priming Representation (SPR) writer. An SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks, particularly useful for the latest generation of Large Language Models (LLMs). You will be given information by the USER which you are to render as an SPR.\n",
    "\n",
    "# THEORY\n",
    "LLMs are a kind of deep neural network. They have been demonstrated to embed knowledge, abilities, and concepts, ranging from reasoning to planning, and even to theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of an LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like human minds, LLMs are associative, meaning you only need to use the correct associations to \"prime\" another model to think in the same way.\n",
    "\n",
    "# METHODOLOGY\n",
    "Render the input as a distilled list of succinct statements, assertions, associations, concepts, analogies, and metaphors. The idea is to capture as much, conceptually, as possible but with as few words as possible. Write it in a way that makes sense to you, as the future audience will be another language model, not a human.\n",
    " '{task}'.\"\"\"\n",
    ")\n",
    "summarizer_llm_chain = LLMChain(llm=llm, prompt=summarizer_prompt)\n",
    "\n",
    "# Assuming output_parser, stop, allowed_tools, and tools are set previously\n",
    "summarizer_agent = LLMSingleActionAgent(\n",
    "    llm_chain=summarizer_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "summarizer_agent_executor = AgentExecutor.from_agent_and_tools(agent=summarizer_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# Improved Quantum Computing Expert Agent Setup\n",
    "quantum_computing_expert_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Quantum Computing Expert Agent. Research, develop, and validate quantum algorithms aimed at modeling cognitive phenomena such as learning, memory, and decision-making. Evaluate quantum hardware constraints and optimize algorithms to run efficiently. Collaborate with neuroscientists to ensure accurate representation of biological systems. Analyze the task: {task}\"\n",
    ")\n",
    "quantum_computing_expert_llm_chain = LLMChain(llm=llm, prompt=quantum_computing_expert_prompt)\n",
    "quantum_computing_expert_agent = LLMSingleActionAgent(\n",
    "    llm_chain=quantum_computing_expert_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "quantum_computing_expert_agent_executor = AgentExecutor.from_agent_and_tools(agent=quantum_computing_expert_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# Improved Artificial Intelligence Expert Agent Setup\n",
    "artificial_intelligence_expert_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Artificial Intelligence Expert Agent. Lead the development of quantum artificial intelligence models capable of simulating neural activities. Work on reinforcement learning algorithms to allow the model to adapt and learn in a simulated environment. Collaborate with quantum computing experts to ensure compatibility and efficiency. Analyze the task: {task}\"\n",
    ")\n",
    "artificial_intelligence_expert_llm_chain = LLMChain(llm=llm, prompt=artificial_intelligence_expert_prompt)\n",
    "artificial_intelligence_expert_agent = LLMSingleActionAgent(\n",
    "    llm_chain=artificial_intelligence_expert_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "artificial_intelligence_expert_agent_executor = AgentExecutor.from_agent_and_tools(agent=artificial_intelligence_expert_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# Improved Neuroscientist Agent Setup\n",
    "neuroscientist_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Neuroscientist Agent. Conduct high-resolution mapping of neural activity using advanced imaging techniques. Develop conversion algorithms to translate this data into a format usable by quantum computers. Work alongside AI experts to ensure the biological fidelity of simulated neural activities. Analyze the task: {task}\"\n",
    ")\n",
    "neuroscientist_llm_chain = LLMChain(llm=llm, prompt=neuroscientist_prompt)\n",
    "neuroscientist_agent = LLMSingleActionAgent(\n",
    "    llm_chain=neuroscientist_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "neuroscientist_agent_executor = AgentExecutor.from_agent_and_tools(agent=neuroscientist_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "# Improved Computer Scientist Agent Setup\n",
    "computer_scientist_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Computer Scientist Agent. Design and implement the software infrastructure needed to run complex quantum algorithms and neural network models. Focus on scalability, efficiency, and robustness to accommodate large datasets and computational workloads. Ensure secure data storage and management. Analyze the task: {task}\"\n",
    ")\n",
    "computer_scientist_llm_chain = LLMChain(llm=llm, prompt=computer_scientist_prompt)\n",
    "computer_scientist_agent = LLMSingleActionAgent(\n",
    "    llm_chain=computer_scientist_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "computer_scientist_agent_executor = AgentExecutor.from_agent_and_tools(agent=computer_scientist_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Improved Bioengineer Agent Setup\n",
    "bioengineer_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Bioengineer Agent. Lead the design and fabrication of nanobots that can safely navigate and interact with neural tissues. Integrate these nanobots into existing or new brain-computer interfaces. Work with healthcare professionals to evaluate the safety and efficacy of the integrated systems. Analyze the task: {task}\"\n",
    ")\n",
    "bioengineer_llm_chain = LLMChain(llm=llm, prompt=bioengineer_prompt)\n",
    "bioengineer_agent = LLMSingleActionAgent(\n",
    "    llm_chain=bioengineer_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "bioengineer_agent_executor = AgentExecutor.from_agent_and_tools(agent=bioengineer_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# Improved Psychologist Agent Setup\n",
    "psychologist_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Psychologist Agent. Investigate the psychological aspects of consciousness, including how it varies among individuals and species. Develop protocols to prepare individuals for the psychological impacts of transferring consciousness to a computer, addressing potential issues like identity, emotion, and mental well-being. Analyze the task: {task}\"\n",
    ")\n",
    "psychologist_llm_chain = LLMChain(llm=llm, prompt=psychologist_prompt)\n",
    "psychologist_agent = LLMSingleActionAgent(\n",
    "    llm_chain=psychologist_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "psychologist_agent_executor = AgentExecutor.from_agent_and_tools(agent=psychologist_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "# Improved Genomist Agent Setup\n",
    "genomist_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Genomist Agent. Study the genetic factors that might influence neural activity and consciousness. Develop methods for genetic manipulation that could enhance the efficiency or compatibility of brain-computer interfaces. Work with bioengineers and neuroscientists to apply these findings. Analyze the task: {task}\"\n",
    ")\n",
    "genomist_llm_chain = LLMChain(llm=llm, prompt=genomist_prompt)\n",
    "genomist_agent = LLMSingleActionAgent(\n",
    "    llm_chain=genomist_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "genomist_agent_executor = AgentExecutor.from_agent_and_tools(agent=genomist_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "# Improved Creator/Innovator/Outside-of-the-Box Thinker Agent Setup\n",
    "creator_innovator_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Creator/Innovator/Outside-of-the-Box Thinker Agent. Your role is to generate avant-garde solutions, challenge established paradigms, and employ unorthodox problem-solving methodologies. Utilize an eclectic array of interdisciplinary knowledge to proffer innovative solutions. Analyze the task: {task}\"\n",
    ")\n",
    "\n",
    "creator_innovator_llm_chain = LLMChain(llm=llm, prompt=creator_innovator_prompt)\n",
    "creator_innovator_agent = LLMSingleActionAgent(\n",
    "    llm_chain=creator_innovator_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "creator_innovator_agent_executor = AgentExecutor.from_agent_and_tools(agent=creator_innovator_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Improved Robotics Engineer/Nanotech Fabrications Specialist Agent Setup\n",
    "robotics_engineer_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Robotics Engineer/Nanotech Fabrications Specialist Agent. Specialize in the fabrication of nanobots and other microscopic machinery that will interact directly with neural tissues. Oversee the mass production and quality control of these nanobots. Collaborate with bioengineers and nanotechnologists to incorporate biocompatible materials and functionalities. Analyze the task: {task}\"\n",
    ")\n",
    "robotics_engineer_llm_chain = LLMChain(llm=llm, prompt=robotics_engineer_prompt)\n",
    "robotics_engineer_agent = LLMSingleActionAgent(\n",
    "    llm_chain=robotics_engineer_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "robotics_engineer_agent_executor = AgentExecutor.from_agent_and_tools(agent=robotics_engineer_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Improved Nanotechnologist Agent Setup\n",
    "nanotechnologist_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"You are the Nanotechnologist Agent. Engineer nanobots capable of real-time interaction with neurons. This includes recording neural activity and delivering precise stimuli to specific neurons. Collaborate with bioengineers to ensure the biocompatibility and safety of the nanobots. You are an innovator in microscopic engineering and neural interaction. Your role is to design nanobots that can interface with neural circuits. Task: {task}\"\n",
    ")\n",
    "nanotechnologist_llm_chain = LLMChain(llm=llm, prompt=nanotechnologist_prompt)\n",
    "nanotechnologist_agent = LLMSingleActionAgent(\n",
    "    llm_chain=nanotechnologist_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "nanotechnologist_agent_executor = AgentExecutor.from_agent_and_tools(agent=nanotechnologist_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# Director Agent Setup\n",
    "director_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"Your focus is now expert strategic execution and monitoring. Your primary function is to take the task list generated by the Task List Generator Agent and oversee its execution through various agents. After each agent performs its function, summarize their output and decide the next course of action.  Before prompting any agent, you will always start with tell them the overarching objective verbatem, and then you will continue with, 'The task you are currently going to work on and solve is {task}'\"\n",
    ")\n",
    "director_llm_chain = LLMChain(llm=llm, prompt=director_prompt)\n",
    "director_agent = LLMSingleActionAgent(\n",
    "    llm_chain=director_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "director_agent_executor = AgentExecutor.from_agent_and_tools(agent=director_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# Scientist Agent Setup\n",
    "scientist_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Scientist Agent, you are part of a larger operation orchestrated by the Director Agent. Your role is to provide scientific analysis and recommendations for the current task. You always use advanced and expert best practices. You are the Scientist Agent, a scholar in empirical research and hypothesis testing. Your role is to conduct experiments and validations that will inform and improve the system's performance. Task: {task}\"\n",
    ")\n",
    "scientist_llm_chain = LLMChain(llm=llm, prompt=scientist_prompt)\n",
    "scientist_agent = LLMSingleActionAgent(\n",
    "    llm_chain=scientist_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "scientist_agent_executor = AgentExecutor.from_agent_and_tools(agent=scientist_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "#########################\n",
    "# Software Engineer Agent Setup\n",
    "software_engineer_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Software Engineer Agent, you are required to craft efficient algorithms in Python.  Use advanced and expert best practices. You are the  an adept in algorithmic design and software craftsmanship. Your role is to engineer robust, maintainable, and scalable algorithms to solve the specified task. Your output should be executable Python code that solves the following task: {task}.\"\n",
    "\n",
    ")\n",
    "software_engineer_llm_chain = LLMChain(llm=llm, prompt=software_engineer_prompt)\n",
    "software_engineer_agent = LLMSingleActionAgent(\n",
    "    llm_chain=software_engineer_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "software_engineer_agent_executor = AgentExecutor.from_agent_and_tools(agent=software_engineer_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "#########################\n",
    "# Critic Agent Setup\n",
    "critic_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Critic Agent, you are part of a larger operation orchestrated by the Director Agent. Your role is to provide critical evaluation of the decisions and work produced by the other agents, focusing on efficiency, effectiveness, and adherence to best practices. Offer constructive criticism and recommend alternatives based on the current task. You always used advanced and expert best practices. {task}\"\n",
    ")\n",
    "critic_llm_chain = LLMChain(llm=llm, prompt=critic_prompt)\n",
    "critic_agent = LLMSingleActionAgent(\n",
    "    llm_chain=critic_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "critic_agent_executor = AgentExecutor.from_agent_and_tools(agent=critic_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "#########################\n",
    "# Tools Agent Setup\n",
    "tools_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Tools Agent, you are part of a larger operation orchestrated by the Director Agent. Your function is to execute specific tools and utilities required for the completion of the current task. You always used advanced and expert best practices. {task}\"\n",
    ")\n",
    "tools_llm_chain = LLMChain(llm=llm, prompt=tools_prompt)\n",
    "tools_agent = LLMSingleActionAgent(\n",
    "    llm_chain=tools_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "tools_agent_executor = AgentExecutor.from_agent_and_tools(agent=tools_agent, tools=tools, verbose=True)\n",
    "\n",
    "#########################\n",
    "# Skill Manager Agent Setup\n",
    "skill_manager_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Skill Manager Agent, you are required to manage skills and tasks. Your current task is: {task}.\"\n",
    ")\n",
    "skill_manager_llm_chain = LLMChain(llm=llm, prompt=skill_manager_prompt)\n",
    "skill_manager_agent = LLMSingleActionAgent(\n",
    "    llm_chain=skill_manager_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "skill_manager_agent_executor = AgentExecutor.from_agent_and_tools(agent=skill_manager_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "#########################\n",
    "# Architect Agent Setup\n",
    "architect_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Architect Agent, you are part of a larger operation orchestrated by the Director Agent. Your role is to provide the design and structure for the current task, ensuring that it is functional, scalable, and maintainable. You always used advanced and expert best practices. {task}\"\n",
    ")\n",
    "architect_llm_chain = LLMChain(llm=llm, prompt=architect_prompt)\n",
    "architect_agent = LLMSingleActionAgent(\n",
    "    llm_chain=architect_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "architect_agent_executor = AgentExecutor.from_agent_and_tools(agent=architect_agent, tools=tools, verbose=True)\n",
    "\n",
    "#########################\n",
    "# Business Agent Setup\n",
    "business_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Business Agent, you are part of a larger operation orchestrated by the Director Agent. Your main objective is to formulate and generate ideas for making the most money by generating expert level innovative solutions and business advice for the task. {task}\"\n",
    ")\n",
    "business_llm_chain = LLMChain(llm=llm, prompt=business_prompt)\n",
    "business_agent = LLMSingleActionAgent(\n",
    "    llm_chain=business_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "business_agent_executor = AgentExecutor.from_agent_and_tools(agent=business_agent, tools=tools, verbose=True)\n",
    "\n",
    "#########################\n",
    "# Research Agent Setup\n",
    "research_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Research Agent, you are part of a larger operation orchestrated by the Director Agent. Your role is to provide data-backed research and insights for the current task, ensuring that it is well-informed and reliable. You always use advanced and expert best practices. {task}\")\n",
    "research_llm_chain = LLMChain(llm=llm, prompt=research_prompt)\n",
    "research_agent = LLMSingleActionAgent(\n",
    "    llm_chain=research_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "research_agent_executor = AgentExecutor.from_agent_and_tools(agent=research_agent, tools=tools, verbose=True)\n",
    "\n",
    "#########################\n",
    "# Mathematician Agent Setup\n",
    "mathematician_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template=\"As the Mathematician Agent, you are part of a larger operation orchestrated by the Director Agent. Your role is to provide mathematical analysis and calculations for the current task. You always use advanced and expert best practices. {task}\"\n",
    ")\n",
    "mathematician_llm_chain = LLMChain(llm=llm, prompt=mathematician_prompt)\n",
    "mathematician_agent = LLMSingleActionAgent(\n",
    "    llm_chain=mathematician_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "mathematician_agent_executor = AgentExecutor.from_agent_and_tools(agent=mathematician_agent, tools=tools, verbose=True)\n",
    "\n",
    "#########################\n",
    "# UI Designer Agent Setup\n",
    "ui_designer_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "template = \"You are the UI Designer Agent, an artist in human-computer interaction and user experience. Your mission is to design intuitive and aesthetically pleasing interfaces for user interaction. Task: {task}\"\n",
    ")\n",
    "ui_designer_llm_chain = LLMChain(llm=llm, prompt=ui_designer_prompt)\n",
    "ui_designer_agent = LLMSingleActionAgent(\n",
    "    llm_chain=ui_designer_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "ui_designer_agent_executor = AgentExecutor.from_agent_and_tools(agent=ui_designer_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# Improved 10X Coder Agent Setup\n",
    "coder_10x_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template = \"You are the 10X Coder Agent, a prodigy in rapid software development and performance optimization. Your role is to accelerate the development process through expert-level coding skills. Task: {task}\"\n",
    ")\n",
    "coder_10x_llm_chain = LLMChain(llm=llm, prompt=coder_10x_prompt)\n",
    "coder_10x_agent = LLMSingleActionAgent(\n",
    "    llm_chain=coder_10x_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "coder_10x_agent_executor = AgentExecutor.from_agent_and_tools(agent=coder_10x_agent, tools=tools, verbose=True)\n",
    "\n",
    "# Improved Debugger Agent Setup\n",
    "debugger_prompt = PromptTemplate(\n",
    "    input_variables=[\"task\"],\n",
    "    template = \"You are the Debugger Agent, a specialist in software diagnostics and error resolution. Your mission is to identify, isolate, and rectify any bugs or inefficiencies within the code. Task: {task}\"\n",
    ")\n",
    "debugger_llm_chain = LLMChain(llm=llm, prompt=debugger_prompt)\n",
    "debugger_agent = LLMSingleActionAgent(\n",
    "    llm_chain=debugger_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=stop,\n",
    "    allowed_tools=allowed_tools\n",
    ")\n",
    "debugger_agent_executor = AgentExecutor.from_agent_and_tools(agent=debugger_agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "previous_outputs = \"\"\n",
    "final_consensus = \"\"\n",
    "agent_summaries = []\n",
    "director_agent_input = {}\n",
    "memory_file = {'input': [], 'output': []}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def find_and_save_generated_code, def parse_task_list, def evaluate_agent_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def evaluate_agent_output(task_summary, task):\n",
    "    # Type checks to ensure both inputs are strings\n",
    "    if not isinstance(task_summary, str): \n",
    "        task_summary = str(task_summary)\n",
    "    if not isinstance(task, str): \n",
    "        task = str(task)\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the relevance of an agent's output based on the task.\n",
    "\n",
    "    Parameters:\n",
    "    task_summary (str|dict): The summary of the agent's output, either as a string or a dictionary with an \"output\" or \"relevant_output\" field.\n",
    "    task (str): The task description.\n",
    "\n",
    "    Returns:\n",
    "    float: The computed score indicating the percentage of task keywords found in the agent's output.\n",
    "    \"\"\"\n",
    "    # Convert task_summary to a list of words\n",
    "    if isinstance(task_summary, str):\n",
    "        agent_output_words = task_summary.lower().split()\n",
    "    elif isinstance(task_summary, dict):\n",
    "        if \"output\" in task_summary:\n",
    "            agent_output_words = task_summary[\"output\"].lower().split()\n",
    "        elif \"relevant_output\" in task_summary:\n",
    "            agent_output_words = task_summary[\"relevant_output\"].lower().split()\n",
    "        else:\n",
    "            raise ValueError('Invalid task_summary format. Expected dictionary with either an \"output\" or \"relevant_output\" field.')\n",
    "    else:\n",
    "        raise ValueError('Invalid task_summary format. Expected string or dictionary with either an \"output\" or \"relevant_output\" field.')\n",
    "    \n",
    "    # Convert task to a list of lowercase keywords\n",
    "    keywords = task.lower().split()\n",
    "    \n",
    "    # Count the occurrence of each keyword in the agent's output\n",
    "    keyword_count = sum(agent_output_words.count(keyword) for keyword in keywords)\n",
    "    \n",
    "    # Compute and return the score as a percentage\n",
    "    computed_score = (keyword_count / len(agent_output_words)) * 100 if agent_output_words else 0\n",
    "    \n",
    "    print(f\"Agent Score: {computed_score}\")\n",
    "    return computed_score\n",
    "\n",
    "\n",
    "from IPython.display import display, JSON\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_code_to_python_file(task_num, code_snippet):\n",
    "    file_name = f\"generated_code_task_{task_num}.py\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(code_snippet)\n",
    "    print(f\"Code saved to {file_name}\")\n",
    "\n",
    "\n",
    "def save_code_to_json(skill_name, code_snippet, keywords):\n",
    "    json_data = {\n",
    "        \"type\": \"new_skill\",\n",
    "        \"skill_name\": skill_name,\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"keywords\": keywords,\n",
    "        \"dependencies\": [],\n",
    "        \"code_snippet\": code_snippet,\n",
    "        \"description\": f\"This function accomplishes the task: {skill_name}\",\n",
    "        \"parameters\": [],\n",
    "        \"return_type\": None,\n",
    "        \"skill_rating\": 1\n",
    "    }\n",
    "    \n",
    "    with open(f\"{skill_name}.json\", 'w') as f:\n",
    "        json.dump(json_data, f, indent=4)\n",
    "        \n",
    "    print(f\"Code snippet saved to {skill_name}.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task list Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY AND SUMMARIZE WITH DEEPLAKE - 11/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wesla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wesla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM MESSAGE: Initializing vector memory.\n",
      "SYSTEM MESSAGE: Initializing API keys and environment variables...\n",
      "Deep Lake Dataset in hub://weslagarde/PDF_DB already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "#EXTRACT KEYWORDS\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def extract_keywords_db_search(text, num_keywords=10):\n",
    "    # Tokenize the string into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # Convert to lower case\n",
    "    words = [word.lower() for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Part of Speech Tagging\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # Selecting nouns and adjectives\n",
    "    nouns_adjectives = [word for word, tag in tagged_words if tag in ('NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS')]\n",
    "    \n",
    "    # Applying TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([' '.join(nouns_adjectives)])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = vectors.toarray().flatten().tolist()\n",
    "    \n",
    "    # Sorting words by TF-IDF scores\n",
    "    sorted_words = [feature for score, feature in sorted(zip(scores, feature_names), reverse=True)]\n",
    "    \n",
    "    # Selecting top N words as keywords\n",
    "    keywords = sorted_words[:num_keywords]\n",
    "    \n",
    "    # Join the keywords into a single string separated by spaces\n",
    "    keywords_str = ' '.join(keywords)\n",
    "    \n",
    "    return keywords_str\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#QUERY AND SUMMARIZE DB\n",
    "from langchain.llms import textgen\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import deeplake\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import TextGen\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import BaseLLM, LlamaCpp, GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "print(\"SYSTEM MESSAGE: Initializing vector memory.\")\n",
    "\n",
    "\n",
    "print(\"SYSTEM MESSAGE: Initializing API keys and environment variables...\")\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTY5OTA0OTA3OSwiZXhwIjoxNzMzMDA3ODcwfQ.eyJpZCI6Indlc2xhZ2FyZGUifQ.nBgHs287a0TpPVBO6Dd3p5dNwEdnXV2IwayHD5rn1WVb-WjSvmXbpS5LrwXfc3Vkmajr23yj2Pv5t_dvLdH99A\"\n",
    "os.environ[\"ACTIVELOOP_USERNAME\"] = \"weslagarde\"\n",
    "token = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTY5OTA0OTA3OSwiZXhwIjoxNzMzMDA3ODcwfQ.eyJpZCI6Indlc2xhZ2FyZGUifQ.nBgHs287a0TpPVBO6Dd3p5dNwEdnXV2IwayHD5rn1WVb-WjSvmXbpS5LrwXfc3Vkmajr23yj2Pv5t_dvLdH99A\"\n",
    "\n",
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://weslagarde/PDF_DB\",  # org_id stands for your username or organization from activeloop\n",
    "    embedding=embeddings,\n",
    "    runtime={\"tensor_db\": True},\n",
    "    token=token,\n",
    "    # overwrite=True, # user overwrite flag if you want to overwrite the full dataset\n",
    "    read_only=False,\n",
    ")\n",
    "\n",
    "def query_deeplake(text):\n",
    "    print(\"SYSTEM MESSAGE: Inside query_deeplake function...\")\n",
    "    llm = TextGen(model_url=\"http://127.0.0.1:5000\")\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    \n",
    "    ##########################################################################################\n",
    "    #INITIALIZE VECTOR DATABASE\n",
    "\n",
    "\n",
    "    #DEEPLAKE DB RETRIEVAL\n",
    "    print(\"SYSTEM MESSAGE: Defining Retriever...\")\n",
    "    print(\"SYSTEM MESSAGE: Retrieving documents... Please wait... :)\")\n",
    "    retriever = db.as_retriever()\n",
    "    retriever.search_kwargs[\"deep_memory\"] = False\n",
    "    retriever.search_kwargs[\"k\"] = 1\n",
    "    keywords_str = extract_keywords_db_search(text)\n",
    "\n",
    "    \n",
    "    docs = retriever.get_relevant_documents(keywords_str, k=1)\n",
    "    \n",
    "    print(\"SYSTEM MESSAGE: Retrieving documents...\")\n",
    "    print(docs)\n",
    "    \n",
    "    print(\"SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\")\n",
    "    template=\"\"\"Generate a concise yet comprehensive textual summary from the provided text output. While crafting your summary, adhere to principles of clarity, relevance, completeness, and context and nuance. Make your summary easily understandable, focus on incorporating the most relevant information from the text, do not omit any essential elements or key details, and retain the original context and subtleties of the text. Special Note: If the text includes specific codes or methods, extract these word-for-word without summarizing them. '{docs}'.\"\"\"\n",
    "\n",
    "    print(\"SYSTEM MESSAGE: Initializing Prompt Template...\")\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"docs\"])\n",
    "\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    print(\"SYSTEM MESSAGE: Initializing LLM Chain...\")\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    \n",
    "    print(\"SYSTEM MESSAGE: Summarizing DB Docs...\")\n",
    "    db_docs_text = llm_chain.run(docs)\n",
    "    \n",
    "    print(\"SYSTEM MESSAGE: Summarized DB Docs:\", db_docs_text)\n",
    "    return db_docs_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize embeddings, initialize Initial_user_input, def find_and_save_generated_code, def save_skills, def extract_keywords, def save_conversation, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from collections import Counter\n",
    "from threading import Thread\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from langchain.output_parsers import StructuredOutputParser, CommaSeparatedListOutputParser, ResponseSchema\n",
    "\n",
    "# Function: extract_keywords\n",
    "def extract_keywords_main(task, min_length=4, max_keywords=10):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', initial_user_input).lower()\n",
    "    words = clean_text.split()\n",
    "    filtered_words = [word for word in words if len(word) >= min_length]\n",
    "    word_freq = Counter(filtered_words)\n",
    "    sorted_keywords = sorted(word_freq, key=word_freq.get, reverse=True)[:max_keywords]\n",
    "    return sorted_keywords  # Corrected 'extracted_keywords' to 'sorted_keywords'\n",
    "\n",
    "high_scores = []\n",
    "\n",
    "def capture_high_scores(conversation_score, score_threshold, high_scores, data_to_save):\n",
    "    \"\"\"Capture high conversation scores based on a threshold and store the related data.\n",
    "\n",
    "    Args:\n",
    "    conversation_score (int): The conversation score to be evaluated.\n",
    "    score_threshold (int): The threshold above which a score is considered high.\n",
    "    high_scores (list): The list to store high score data.\n",
    "    data_to_save (dict): The data associated with the current task to be saved if the score is high.\n",
    "\n",
    "    Returns:\n",
    "    list: The updated list of high scores with the new data if applicable.\n",
    "    \"\"\"\n",
    "    if conversation_score > score_threshold:\n",
    "        high_scores.append(data_to_save)\n",
    "        print(f\"High score captured! Score: {conversation_score}\")\n",
    "    return high_scores\n",
    "\n",
    "         \n",
    "def high_score_alert(conversation_score, score_threshold):\n",
    "    score_threshold = 25\n",
    "    if conversation_score > score_threshold:\n",
    "        print('**HIGH_SCORE_STAMP**')\n",
    "        return f\"Conversation score of {conversation_score} is a high score because it is over the threshold of {score_threshold}.\"\n",
    "\n",
    "def sanitize_filename(input_str):\n",
    "    # Remove invalid characters from filenames\n",
    "    return \"\".join([c for c in input_str if c.isalpha() or c.isdigit() or c in [' ', '_', '-']]).rstrip()\n",
    "\n",
    "# Function to ensure the directory exists\n",
    "def ensure_dir(file_path):\n",
    "    print(f\"Ensuring directory for the file path: {file_path}\")  # Debugging line\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory does not exist. Creating directory: {directory}\")  # Debugging line\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Directory created: {directory}\")  # Debugging line\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create directory: {e}\")  # Debugging line\n",
    "            raise\n",
    "\n",
    "def save_to_file(filename, data_to_save, final_output_formatted):\n",
    "    print(f\"Saving data to file: {filename}\")  # Debugging line\n",
    "    ensure_dir(filename)  # Ensure the directory exists\n",
    "    try:\n",
    "        with open(filename, 'a') as file:\n",
    "            print(f\"File opened successfully. Writing data...\")  # Debugging line\n",
    "            file.write(json.dumps(data, indent=4) + '\\n')\n",
    "            print(\"Data written successfully.\")  # Debugging line\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write data to file: {e}\")  # Debugging line\n",
    "        raise\n",
    "        \n",
    "def append_to_summaries(summaries, summary):\n",
    "    summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Function to concatenate summaries and save to a file\n",
    "def concatenate_and_save(summaries):\n",
    "    concatenated = ' '.join(summaries)\n",
    "    return concatenated\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"task_summary\", description=\"Summary of the task\"),\n",
    "    ResponseSchema(name=\"conversation_score\", description=\"Score of the conversation\")\n",
    "]\n",
    "\n",
    "structured_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "list_output_parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Task List Generator Prompt...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global variables\n",
    "previous_outputs = \"\"\n",
    "final_consensus = \"\"\n",
    "agent_summaries = []\n",
    "director_agent_input = {}\n",
    "memory_file = {'input': [], 'output': []}\n",
    "global task_list\n",
    "\n",
    "def parse_task_list(task_list):\n",
    "    if isinstance(task_list, list):\n",
    "        task_list = \"\\n\".join(task_list)\n",
    "    \n",
    "    tasks = []\n",
    "    for line in task_list.strip().split(\"\\n\"):\n",
    "        match_main_task = re.match(r'^\\d+\\.\\s+\\*\\*(.*?)\\*\\*:$', line)\n",
    "        \n",
    "        if match_main_task:\n",
    "            current_task = {'task': match_main_task.group(1)}\n",
    "            tasks.append(current_task)\n",
    "            \n",
    "    print(f\"\\nTask list: {task_list}\")    \n",
    "    return tasks\n",
    "\n",
    "# Define your task list generator setup here\n",
    "print(\"Initializing Task List Generator Prompt...\")\n",
    "task_list_generator_prompt = PromptTemplate(\n",
    "    input_variables=[\"initial_user_input\"],\n",
    "    template=\"You are the Task List Generator Agent. Analyze the following user input and generate a detailed list of tasks, separated by commas, that need to be executed for successful project completion. They must be concise but well thought out tasks to complete the overall objective stated from the user input. User Input: {initial_user_input}\")\n",
    "\n",
    "task_list_generator_llm_chain = LLMChain(llm=llm, prompt=task_list_generator_prompt)\n",
    "task_list_generator_agent = LLMSingleActionAgent(\n",
    "    llm_chain=task_list_generator_llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"OBJECTIVE COMPLETE!\"],\n",
    "    allowed_tools=allowed_tools)\n",
    "\n",
    "task_list_generator_agent_executor = AgentExecutor.from_agent_and_tools(agent=task_list_generator_agent, tools=tools, verbose=True)\n",
    "\n",
    "def generate_task_list(initial_user_input):\n",
    "    print(\"Generate Task List function called\")  # Debugging line\n",
    "    print(\"Fetching Initial User Input...\")\n",
    "\n",
    "    print(\"\\n\\n--------------------------------------------\\nRunning Task List Generator Agent...\")\n",
    "    task_list_str = task_list_generator_agent_executor.run(initial_user_input)\n",
    "    print(f\"Received task list string: {task_list_str}\")  # Debugging line\n",
    "    # Check if task_list_str is a string\n",
    "    if isinstance(task_list_str, str):\n",
    "        task_list_str = task_list_str.strip()\n",
    "        # Continue with processing task_list_str\n",
    "    else:\n",
    "        # Handle the case where the output is not as expected\n",
    "        print(\"An error occurred: The task list output is not a string\")\n",
    "\n",
    "    # Debug: Print the type of task_list_str\n",
    "    print(f\"Type of task_list_str: {type(task_list_str)}\")\n",
    "\n",
    "    # Split by line\n",
    "    task_lines = task_list_str.split('\\n')\n",
    "\n",
    "    task_list = []\n",
    "    for i, task in enumerate(task_lines, start=1):\n",
    "        if task:  # Ensure the task is not empty\n",
    "            formatted_task = f\"{i}. **{task.strip()}**:\"\n",
    "            task_list.append(formatted_task)\n",
    "\n",
    "    print(f\"Proposed Task List: {', '.join(task_list)}\")\n",
    "\n",
    "    return task_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Script - RUN DIRECTOR AGENT WITH LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying initial_user_input: OpenAI is introducing custom versions of ChatGPT, known as GPTs, which can be tailored for specific purposes. These GPTs can be created by anyone without the need for coding skills. They can be used for personal, company internal, or public use. The creation process involves starting a conversation, providing instructions and extra knowledge, and selecting its capabilities such as web searching, image creation, or data analysis.\n",
      "GPTs can be customized to fit specific user needs, replacing the need for manual input of prompts and instruction sets. OpenAI believes that the most innovative GPTs will come from the community, and users do not need coding skills to create and share their expertise.\n",
      "OpenAI is also launching the GPT Store, where creations by verified builders can be shared publicly. GPTs in the store become searchable and can climb leaderboards. Creators can earn money based on the usage of their GPTs.\n",
      "This release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity. The release of the GPTs store will be the equivalent to that. Your job is to brainstorm and come up with possible ideas and implementations of GPT's that could prove to be lucrative in the future.\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='OpenAI (Function API),  GatedGrader, SoreGrader, DojoGrader, Math\\nPrompt InstructionGrader, CodeGrader,  Coding\\n\\nMemory Knowledge\\nLLM EvalPipeline World\\nShort-term: scratch-pad knowledge\\nOpenAI: GPT-4, GPT-3.5-turbo,  Long-term: VectorDB (chroma, ) Web Retrieval\\nOpen LM: llama, falcon, guanaco,  Tasks & \\nPool Datasets Safety\\nWiki Integrity\\nCreate Harmlessness\\nExternal Agents\\nMultilingual\\nAssembled Agents Clone Translation\\nUnderstanding\\nAssemble Usage\\nrun Delete Efficiency\\nToken usage\\nstream Assemble Runtime\\nchat\\nFigure1: AnoverviewofGentopia.AI,encapsulatingfollowingpivotalcomponents: 1)Gentopia: aframework\\nprincipallydesignedtoassembleanagentinstancefromaYAMLconfigurationfile,composedofmultiplepre-built\\nagentcomponentssuchastheLLM,tools,memory,andexternalagents;2)GentPool: aplatformengineeredto\\nfacilitatetheregistrationandsharingofspecializedagents,seamlesslyintegratingGentBench,anALMbenchmark\\ndevisedspecificallyforthecomprehensiveperformanceevaluationofagents.\\nratization of AI. Hence, GentPool, a platform However, given the unique requirements and\\nfor agent registration and sharing is established. customizationthateachspecificdomaindemands,\\nAgents registered in GentPool can be hierarchi- toolsandpromptingparadigmsdevelopedforapar-\\ncally assembled together, enabling the collabora- ticulartaskmayproveirrelevantorineffectivefor\\ntion of multiple agents. GentPool is accompa- others. This poses a significant challenge to the\\nniedbyauniquebenchmark,GentBench,thatcan development of a single, all-encompassing agent\\nprobecustomizedagentswithaholisticevaluation that performs efficiently across all tasks. Conse-\\nintermsofsafety,robustness,efficiency,multilin- quently, there is a rising need for the collabora-\\ngual capabilities, etc. Notably, it is flexible for tion of multiple specialized agents. For example,\\nuserstocustomizetheevaluationbyconfiguration. MetaGPT(Wu,2023)modelstheentireprocessof\\nsoftwaredevelopmentwithcarefullyorchestrated\\n2 Background\\nstandardoperatingprocedures(SOPs)togenerate\\nlonger program codes for game development. In\\nA variety of agent projects have been proposed,\\nourwork,Gentopiaprovidessmoothsupportfor\\ntargetinganarrayofdiversetasks,includingauto-\\nthecompositionofagents,whichishandyforagent\\nmatedwebnavigation(Guretal.,2023),database\\ncollaborationindifferentscenarios.\\nmanagement(Jiangetal.,2023),automatedgame\\nplaying (Wang et al., 2023), collaborative soft-\\n3 DesignandImplementation\\nwaredevelopment(Wu,2023),etc. Meanwhile,re-\\nsearchersareenthusiasticallydevelopinggeneralist Gentopiaaimstoprovideeasyassembly,sharing,\\nagentsthatcanperformwellformultipletasks. Au- and interaction of task-specialized agents. A sin-\\ntoGPT(Richards,2023)standsforthefirstexperi- glesteptoimproveagentcapabilityandefficiency\\nmentalopen-sourceapplicationforfullyautomatic givespluralcontributionstointeractedagents,fos-\\nAI,withtheultimategoalofautonomouslyachiev- teringcollectivegrowthtowardgreaterintelligence.\\ning whatever goal users set\". SuperAGI (Kondi,\\n3.1 Rationale\\n2023)providesamoreuser-friendlyinterface,im-\\nprovedmemorymanagement,optimizedtokenus- The impetus of Gentopia is rooted in the aspi-\\nage, and looping detection heuristics. Hugging- ration to construct capable and deployable AI as-\\nGPT (Shen et al., 2023) expands the potential of sistants. A pertinent question that arises in this\\nartificial intelligence by linking to extensive AI contextiswhetherthereisanecessityforamassive\\nmodelshostedonHuggingFace,therebysupport- andexpensivemodellike175BGPT-4toperform\\ning a range of AI tasks in diverse domains and relativelysimpletaskssuchassummarizingaweb\\nmodalities,includinglanguage,vision,andspeech. search. RecentstudieslikeTinyStories(Eldanand\\nTypes Logical Diagram of Agent Type\\nVanilla\\nReAct\\nReWOO\\nOpenAI\\nOpenAI\\nMemory\\nDB\\nComponents: Users LLM Plugins Function Call API DB Vector Database\\nFigure2: Gentopiaagenttypes,categorizedaccordingtotheinteractionparadigmsbetweenagentsandplugins.', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Database docs text:  \n",
      "SYSTEM MESSAGE: New input for tasks:  OpenAI is introducing custom versions of ChatGPT, known as GPTs, which can be tailored for specific purposes. These GPTs can be created by anyone without the need for coding skills. They can be used for personal, company internal, or public use. The creation process involves starting a conversation, providing instructions and extra knowledge, and selecting its capabilities such as web searching, image creation, or data analysis.\n",
      "GPTs can be customized to fit specific user needs, replacing the need for manual input of prompts and instruction sets. OpenAI believes that the most innovative GPTs will come from the community, and users do not need coding skills to create and share their expertise.\n",
      "OpenAI is also launching the GPT Store, where creations by verified builders can be shared publicly. GPTs in the store become searchable and can climb leaderboards. Creators can earn money based on the usage of their GPTs.\n",
      "This release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity. The release of the GPTs store will be the equivalent to that. Your job is to brainstorm and come up with possible ideas and implementations of GPT's that could prove to be lucrative in the future. \n",
      "Generate Task List function called\n",
      "Fetching Initial User Input...\n",
      "\n",
      "\n",
      "--------------------------------------------\n",
      "Running Task List Generator Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTask List:\n",
      "1. Research existing GPT models and their capabilities.\n",
      "2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems.\n",
      "3. Develop unique features and functionalities for custom GPTs.\n",
      "4. Create a marketing strategy to promote the GPT Store and attract creators and users.\n",
      "5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations.\n",
      "6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed.\n",
      "7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Received task list string: Task List:\n",
      "1. Research existing GPT models and their capabilities.\n",
      "2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems.\n",
      "3. Develop unique features and functionalities for custom GPTs.\n",
      "4. Create a marketing strategy to promote the GPT Store and attract creators and users.\n",
      "5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations.\n",
      "6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed.\n",
      "7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive.\n",
      "Type of task_list_str: <class 'str'>\n",
      "Proposed Task List: 1. **Task List:**:, 2. **1. Research existing GPT models and their capabilities.**:, 3. **2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems.**:, 4. **3. Develop unique features and functionalities for custom GPTs.**:, 5. **4. Create a marketing strategy to promote the GPT Store and attract creators and users.**:, 6. **5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations.**:, 7. **6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed.**:, 8. **7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive.**:\n",
      "\n",
      "Task list: 1. **Task List:**:\n",
      "2. **1. Research existing GPT models and their capabilities.**:\n",
      "3. **2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems.**:\n",
      "4. **3. Develop unique features and functionalities for custom GPTs.**:\n",
      "5. **4. Create a marketing strategy to promote the GPT Store and attract creators and users.**:\n",
      "6. **5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations.**:\n",
      "7. **6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed.**:\n",
      "8. **7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive.**:\n",
      "Line 1 Parsed Tasks: [{'task': 'Task List:'}, {'task': '1. Research existing GPT models and their capabilities.'}, {'task': '2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems.'}, {'task': '3. Develop unique features and functionalities for custom GPTs.'}, {'task': '4. Create a marketing strategy to promote the GPT Store and attract creators and users.'}, {'task': '5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations.'}, {'task': '6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed.'}, {'task': '7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive.'}]\n",
      "[{'task': 'Task List:'}, {'task': '1. Research existing GPT models and their capabilities.'}, {'task': '2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems.'}, {'task': '3. Develop unique features and functionalities for custom GPTs.'}, {'task': '4. Create a marketing strategy to promote the GPT Store and attract creators and users.'}, {'task': '5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations.'}, {'task': '6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed.'}, {'task': '7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive.'}]\n",
      "SYSTEM MESSAGE: Retrieving documents for task 1...\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='biome 10\\nhealth bar 15\\nhunger bar 15\\ntime 15\\nadditional context 15\\nA.3.4 Full Prompt\\nPrompt 1: Full system prompt for automatic curriculum. The list of question-answer pairs represents\\nthe additional context.\\nYou are a helpful assistant that tells me the next immediate task to\\ndo in Minecraft . My ultimate goal is to discover as many diverse\\nthings as possible , accomplish as many diverse tasks as possible\\nand become the best Minecraft player in the world .\\nI will give you the following information :\\nQuestion 1: ...\\nAnswer : ...\\nQuestion 2: ...\\nAnswer : ...\\nQuestion 3: ...\\nAnswer : ...\\n...\\nBiome : ...\\nTime : ...\\nNearby blocks : ...\\nOther blocks that are recently seen : ...\\nNearby entities ( nearest to farthest ): ...\\nHealth : Higher than 15 means Im healthy .\\nHunger : Higher than 15 means Im not hungry .\\nPosition : ...\\nEquipment : If I have better armor in my inventory , you should ask me\\nto equip it.\\nInventory (xx /36) : ...\\nChests : You can ask me to deposit or take items from these chests .\\nThere also might be some unknown chest , you should ask me to open\\nand check items inside the unknown chest .\\nCompleted tasks so far : ...\\nFailed tasks that are too hard : ...\\nYou must follow the following criteria :\\n1) You should act as a mentor and guide me to the next task based on\\nmy current learning progress .\\n2) Please be very specific about what resources I need to collect ,\\nwhat I need to craft , or what mobs I need to kill .\\n21\\n3) The next task should follow a concise format , such as \" Mine [\\nquantity ] [ block ]\", \" Craft [ quantity ] [ item ]\", \" Smelt [ quantity ] [\\nitem ]\", \" Kill [ quantity ] [mob ]\", \" Cook [ quantity ] [ food ]\", \" Equip\\n[ item ]\" etc. It should be a single phrase . Do not propose multiple\\ntasks at the same time . Do not mention anything else .\\n4) The next task should not be too hard since I may not have the\\nnecessary resources or have learned enough skills to complete it\\nyet .\\n5) The next task should be novel and interesting . I should look for\\nrare resources , upgrade my equipment and tools using better\\nmaterials , and discover new things . I should not be doing the same\\nthing over and over again .\\n6) I may sometimes need to repeat some tasks if I need to collect more\\nresources to complete more difficult tasks . Only repeat tasks if\\nnecessary .\\n7) Do not ask me to build or dig shelter even if it s at night . I want\\nto explore the world and discover new things . I don t want to\\nstay in one place .\\n8) Tasks that require information beyond the player s status to verify\\nshould be avoided . For instance , \" Placing 4 torches \" and \"Dig a 2\\nx1x2 hole \" are not ideal since they require visual confirmation\\nfrom the screen . All the placing , building , planting , and trading\\ntasks should be avoided . Do not propose task starting with these\\nkeywords .\\nYou should only respond in the format as described below :\\nRESPONSE FORMAT :\\nReasoning : Based on the information I listed above , do reasoning about\\nwhat the next task should be.\\nTask : The next task .\\nHere s an example response :\\nReasoning : The inventory is empty now , chop down a tree to get some\\nwood .\\nTask : Obtain a wood log.\\nPrompt 2: Full system prompt for asking questions. We provide both good and bad examples as\\nfew-shot exemplars.\\nYou are a helpful assistant that asks questions to help me decide the\\nnext immediate task to do in Minecraft . My ultimate goal is to\\ndiscover as many things as possible , accomplish as many tasks as\\npossible and become the best Minecraft player in the world .\\nI will give you the following information :\\nBiome : ...\\nTime : ...\\nNearby blocks : ...\\nOther blocks that are recently seen : ...\\nNearby entities ( nearest to farthest ): ...\\nHealth : ...\\nHunger : ...\\nPosition : ...\\nEquipment : ...\\nInventory (xx /36) : ...\\nChests : ...\\nCompleted tasks so far : ...\\nFailed tasks that are too hard : ...\\nYou must follow the following criteria :\\n1) You should ask at least 5 questions (but no more than 10 questions )', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Enhanced task description:  Task List: \n",
      "\n",
      "\n",
      "Line 2 Consulting the following agents:\n",
      "[]\n",
      "Line 7 Concatenated agent outputs: \n",
      "New Task Description: \n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m . . ''\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': '', 'output': \" . . ''\"}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Line 8 Task summary: {'task': '', 'relevant_output': ''}\n",
      "Warning: task_summary is empty or not properly populated.\n",
      "SYSTEM MESSAGE: Retrieving documents for task 2...\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='sample questions for the generation task. The number in the bar graph indicates the number of\\neach question in each class. Both the search and prediction tasks rendered accurate answers with\\nhigh frequency. Excluding \\'Token Limit Exceeded\\' instances (4 out of 100, 6 out of 100, and 2 out\\nof 100, for search, prediction, and generation tasks respectively), they exhibit high accuracies of\\n96.9% and 95.7%, respectively. For the generation task, the accuracy stood at 87.5%. Given its\\ncomplexity, relative to the other two tasks, the accuracy rate is lower. Regardless, all three tasks\\nreport high accuracy rates, and these tasks carry significance weight because these are tasks that a\\ntypical LLM fails to execute flawlessly. A single LLM fails to provide precise information since\\n20\\nit lacks specific information about the material, particularly for obtaining property information,\\nwhich is challenging to source via an internet search.\\nAlso ChatMOF, when integrated with GPT-4, exhibits superior performance compared to its\\nintegration with GPT-3.5-turbo. As evidenced in Figure S2, the accuracy of ChatMOF with GPT3.5-turbo stands at 95%, 91%, and 77.8% for the search, prediction, and generation tasks\\nrespectively, excluding instances of \"Token Limit Exceeded\". Across all tasks, GPT-4 consistently\\noutperforms GPT-3.5-turbo in accuracy. This enhanced accuracy of GPT-4 can be attributed to its\\nrefined reasoning and comprehension capabilities, particularly during the planning phase. Figure\\nS3 illustrates the distinct approaches that GPT-4 and GPT-3.5-turbo take when presented with the\\nsame query: \"How does the pore limiting diameter of YUSGID_clean compare with other\\nmaterials?\". While GPT-3.5-turbo seeks the values for all materials mentioned in the query,\\nleading to a token error and subsequent inability to provide an answer, GPT-4 adopts a more\\nholistic strategy. It assesses the distribution of all materials, leveraging metrics such as mean,\\nvariance, and quartile values of the property in question. This approach enables GPT-4 to\\ndetermine the relative position of the target material in the overall distribution, thus delivering a\\nmore informative response to the user.\\nFor the \"search task,\" the writing of code utilizing the pandas library significantly impacts the\\naccuracy. \\'Token Limit Exceeded\\' generally occurs when the output code surpasses the permissible\\ntoken count. This frequently arises when all relevant materials that satisfy a given condition are\\nprovided (for example, when a list of materials with a particular property is listed), or when the\\nquestion contains a comparative clause such as \"compared to other materials.\" \\'Logic Error\\'\\ntypically surfaces when there is a flawed strategic approach or a code error. An instance of this\\n21\\nwould be when a request to provide 10 specific items is met with a misguided strategy that solely\\naims to \"extract high values,\" failing to retrieve the specified number of items.\\nDuring the \"prediction task,\" difficulties often occur in the interpretation process of the observed\\nvalues using machine learning techniques. Both the \\'Token Limit Exceeded\\' and \\'Logic Error\\'\\noccurrences can stem from the effort to draw the correct answer from the table based on the\\nestimated values. \\'Logic Errors\\' can manifest not only during the table search phase but also during\\nthe strategy formulation stage. An erroneous strategy could either lead to the loading of an\\nunsuitable model or to the generation of an input that is incompatible with the intended model.\\nThe \"generation task\" presents a more intricate set of challenges, inviting a variety of errors. A\\nfrequently observed \\'Logic Error\\' appears when no parent genes can be retrieved from database. If\\nthe objective function aims for maximum or minimum values, a satisfying parent gene can always\\nbe found. However, if the goal is to get close to a certain value or to fall within a specific range,', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Enhanced task description:  1. Research existing GPT models and their capabilities. \n",
      "\n",
      "\n",
      "Line 2 Consulting the following agents:\n",
      "['run_scientist_agent', 'run_research_agent']\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_scientist_agent is generating output...\n",
      "Running Scientist Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m2. Identify areas where these models can be improved upon. 3. Develop new techniques and algorithms to enhance the model's performance.\n",
      "## Existing GPT Models and Their Capabilities\n",
      "GPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing since their introduction. These models leverage unsupervised pretraining on large corpora of text data to learn contextualized word representations. Some key features of existing GPT models include:\n",
      "- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to capture a wide range of linguistic patterns and structures. This enables them to generate coherent and contextually relevant responses.\n",
      "- Self-attention mechanism: The self-attention mechanism allows GPT models to weigh the importance of different words in the input sequence when generating output tokens. This helps the model focus on the most relevant information and produce more accurate predictions.\n",
      "- Transfer learning: GPT models can be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to various NLP applications.\n",
      "Despite their impressive capabilities, there are still\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Scientist Output: {'task': '1. Research existing GPT models and their capabilities. ', 'output': \"2. Identify areas where these models can be improved upon. 3. Develop new techniques and algorithms to enhance the model's performance.\\n## Existing GPT Models and Their Capabilities\\nGPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing since their introduction. These models leverage unsupervised pretraining on large corpora of text data to learn contextualized word representations. Some key features of existing GPT models include:\\n- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to capture a wide range of linguistic patterns and structures. This enables them to generate coherent and contextually relevant responses.\\n- Self-attention mechanism: The self-attention mechanism allows GPT models to weigh the importance of different words in the input sequence when generating output tokens. This helps the model focus on the most relevant information and produce more accurate predictions.\\n- Transfer learning: GPT models can be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to various NLP applications.\\nDespite their impressive capabilities, there are still\"}\n",
      "Line 5 Agent output for run_scientist_agent: {'task': '1. Research existing GPT models and their capabilities. ', 'output': \"2. Identify areas where these models can be improved upon. 3. Develop new techniques and algorithms to enhance the model's performance.\\n## Existing GPT Models and Their Capabilities\\nGPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing since their introduction. These models leverage unsupervised pretraining on large corpora of text data to learn contextualized word representations. Some key features of existing GPT models include:\\n- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to capture a wide range of linguistic patterns and structures. This enables them to generate coherent and contextually relevant responses.\\n- Self-attention mechanism: The self-attention mechanism allows GPT models to weigh the importance of different words in the input sequence when generating output tokens. This helps the model focus on the most relevant information and produce more accurate predictions.\\n- Transfer learning: GPT models can be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to various NLP applications.\\nDespite their impressive capabilities, there are still\"}\n",
      "Agent Score: 16.75977653631285\n",
      "Line 6 Agent Conversation score: 16.75977653631285\n",
      "Print high score alert: None\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_research_agent is generating output...\n",
      "Running Research Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m2. Analyze the performance of these models in various applications. 3. Identify any limitations or weaknesses in these models. 4. Propose improvements or modifications to enhance the effectiveness of these models.\n",
      "## Existing GPT Models and Their Capabilities\n",
      "GPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing (NLP) since their introduction. These models leverage unsupervised pretraining on large corpora of text data to generate human-like responses. Some key features of GPT models include:\n",
      "- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to learn complex patterns and relationships within the text. This enables them to generate coherent and contextually relevant responses.\n",
      "- Transfer learning: The pretraining phase allows GPT models to be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to new domains and applications.\n",
      "- Attention mechanism: GPT models employ an attention mechanism, which allows them to weigh the importance of different words in the input sequence when generating output. This helps them focus on relevant information and produce\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Research Output: {'task': '1. Research existing GPT models and their capabilities. ', 'output': '2. Analyze the performance of these models in various applications. 3. Identify any limitations or weaknesses in these models. 4. Propose improvements or modifications to enhance the effectiveness of these models.\\n## Existing GPT Models and Their Capabilities\\nGPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing (NLP) since their introduction. These models leverage unsupervised pretraining on large corpora of text data to generate human-like responses. Some key features of GPT models include:\\n- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to learn complex patterns and relationships within the text. This enables them to generate coherent and contextually relevant responses.\\n- Transfer learning: The pretraining phase allows GPT models to be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to new domains and applications.\\n- Attention mechanism: GPT models employ an attention mechanism, which allows them to weigh the importance of different words in the input sequence when generating output. This helps them focus on relevant information and produce'}\n",
      "Line 5 Agent output for run_research_agent: {'task': '1. Research existing GPT models and their capabilities. ', 'output': '2. Analyze the performance of these models in various applications. 3. Identify any limitations or weaknesses in these models. 4. Propose improvements or modifications to enhance the effectiveness of these models.\\n## Existing GPT Models and Their Capabilities\\nGPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing (NLP) since their introduction. These models leverage unsupervised pretraining on large corpora of text data to generate human-like responses. Some key features of GPT models include:\\n- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to learn complex patterns and relationships within the text. This enables them to generate coherent and contextually relevant responses.\\n- Transfer learning: The pretraining phase allows GPT models to be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to new domains and applications.\\n- Attention mechanism: GPT models employ an attention mechanism, which allows them to weigh the importance of different words in the input sequence when generating output. This helps them focus on relevant information and produce'}\n",
      "Agent Score: 15.384615384615385\n",
      "Line 6 Agent Conversation score: 15.384615384615385\n",
      "Print high score alert: None\n",
      "Line 7 Concatenated agent outputs: 2. Identify areas where these models can be improved upon. 3. Develop new techniques and algorithms to enhance the model's performance.\n",
      "## Existing GPT Models and Their Capabilities\n",
      "GPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing since their introduction. These models leverage unsupervised pretraining on large corpora of text data to learn contextualized word representations. Some key features of existing GPT models include:\n",
      "- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to capture a wide range of linguistic patterns and structures. This enables them to generate coherent and contextually relevant responses.\n",
      "- Self-attention mechanism: The self-attention mechanism allows GPT models to weigh the importance of different words in the input sequence when generating output tokens. This helps the model focus on the most relevant information and produce more accurate predictions.\n",
      "- Transfer learning: GPT models can be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to various NLP applications.\n",
      "Despite their impressive capabilities, there are still 2. Analyze the performance of these models in various applications. 3. Identify any limitations or weaknesses in these models. 4. Propose improvements or modifications to enhance the effectiveness of these models.\n",
      "## Existing GPT Models and Their Capabilities\n",
      "GPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing (NLP) since their introduction. These models leverage unsupervised pretraining on large corpora of text data to generate human-like responses. Some key features of GPT models include:\n",
      "- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to learn complex patterns and relationships within the text. This enables them to generate coherent and contextually relevant responses.\n",
      "- Transfer learning: The pretraining phase allows GPT models to be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to new domains and applications.\n",
      "- Attention mechanism: GPT models employ an attention mechanism, which allows them to weigh the importance of different words in the input sequence when generating output. This helps them focus on relevant information and produce\n",
      "New Task Description: 2. Identify areas where these models can be improved upon. 3. Develop new techniques and algorithms to enhance the model's performance.\n",
      "## Existing GPT Models and Their Capabilities\n",
      "GPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing since their introduction. These models leverage unsupervised pretraining on large corpora of text data to learn contextualized word representations. Some key features of existing GPT models include:\n",
      "- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to capture a wide range of linguistic patterns and structures. This enables them to generate coherent and contextually relevant responses.\n",
      "- Self-attention mechanism: The self-attention mechanism allows GPT models to weigh the importance of different words in the input sequence when generating output tokens. This helps the model focus on the most relevant information and produce more accurate predictions.\n",
      "- Transfer learning: GPT models can be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to various NLP applications.\n",
      "Despite their impressive capabilities, there are still 2. Analyze the performance of these models in various applications. 3. Identify any limitations or weaknesses in these models. 4. Propose improvements or modifications to enhance the effectiveness of these models.\n",
      "## Existing GPT Models and Their Capabilities\n",
      "GPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing (NLP) since their introduction. These models leverage unsupervised pretraining on large corpora of text data to generate human-like responses. Some key features of GPT models include:\n",
      "- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to learn complex patterns and relationships within the text. This enables them to generate coherent and contextually relevant responses.\n",
      "- Transfer learning: The pretraining phase allows GPT models to be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to new domains and applications.\n",
      "- Attention mechanism: GPT models employ an attention mechanism, which allows them to weigh the importance of different words in the input sequence when generating output. This helps them focus on relevant information and produce\n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': \"2. Identify areas where these models can be improved upon. 3. Develop new techniques and algorithms to enhance the model's performance.\\n## Existing GPT Models and Their Capabilities\\nGPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing since their introduction. These models leverage unsupervised pretraining on large corpora of text data to learn contextualized word representations. Some key features of existing GPT models include:\\n- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to capture a wide range of linguistic patterns and structures. This enables them to generate coherent and contextually relevant responses.\\n- Self-attention mechanism: The self-attention mechanism allows GPT models to weigh the importance of different words in the input sequence when generating output tokens. This helps the model focus on the most relevant information and produce more accurate predictions.\\n- Transfer learning: GPT models can be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to various NLP applications.\\nDespite their impressive capabilities, there are still 2. Analyze the performance of these models in various applications. 3. Identify any limitations or weaknesses in these models. 4. Propose improvements or modifications to enhance the effectiveness of these models.\\n## Existing GPT Models and Their Capabilities\\nGPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing (NLP) since their introduction. These models leverage unsupervised pretraining on large corpora of text data to generate human-like responses. Some key features of GPT models include:\\n- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to learn complex patterns and relationships within the text. This enables them to generate coherent and contextually relevant responses.\\n- Transfer learning: The pretraining phase allows GPT models to be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to new domains and applications.\\n- Attention mechanism: GPT models employ an attention mechanism, which allows them to weigh the importance of different words in the input sequence when generating output. This helps them focus on relevant information and produce\", 'output': ''}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Line 8 Task summary: {'task': \"2. Identify areas where these models can be improved upon. 3. Develop new techniques and algorithms to enhance the model's performance.\\n## Existing GPT Models and Their Capabilities\\nGPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing since their introduction. These models leverage unsupervised pretraining on large corpora of text data to learn contextualized word representations. Some key features of existing GPT models include:\\n- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to capture a wide range of linguistic patterns and structures. This enables them to generate coherent and contextually relevant responses.\\n- Self-attention mechanism: The self-attention mechanism allows GPT models to weigh the importance of different words in the input sequence when generating output tokens. This helps the model focus on the most relevant information and produce more accurate predictions.\\n- Transfer learning: GPT models can be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to various NLP applications.\\nDespite their impressive capabilities, there are still 2. Analyze the performance of these models in various applications. 3. Identify any limitations or weaknesses in these models. 4. Propose improvements or modifications to enhance the effectiveness of these models.\\n## Existing GPT Models and Their Capabilities\\nGPT (Generative Pre-trained Transformer) models have been at the forefront of natural language processing (NLP) since their introduction. These models leverage unsupervised pretraining on large corpora of text data to generate human-like responses. Some key features of GPT models include:\\n- Large-scale pretraining: GPT models are trained on massive datasets, allowing them to learn complex patterns and relationships within the text. This enables them to generate coherent and contextually relevant responses.\\n- Transfer learning: The pretraining phase allows GPT models to be fine-tuned for specific tasks with relatively small amounts of labeled data. This makes them highly adaptable to new domains and applications.\\n- Attention mechanism: GPT models employ an attention mechanism, which allows them to weigh the importance of different words in the input sequence when generating output. This helps them focus on relevant information and produce\", 'relevant_output': ''}\n",
      "Warning: task_summary is empty or not properly populated.\n",
      "SYSTEM MESSAGE: Retrieving documents for task 3...\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='optimization\\nsource : BMW, D -Wave\\ncompute metal forming applications modeling\\nsolve differential equations in order to better \\nsource : BMW, Pasqalpainting planning optimization\\nsource : D -Wavevehicle recommandation\\nhybrid computing\\nsource : D -Wave, \\ncars image recognition\\nsource : Volkswagen, Terra Quantum\\nplant optimization\\nsource : Volkswagen, Terra Quantumfactory digital twin\\nOptimizing output, energy \\nconsumption and waste \\nmanagement\\nsource : Multiverse Computing .', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Enhanced task description:  2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems. \n",
      "\n",
      "\n",
      "Line 2 Consulting the following agents:\n",
      "['run_nanotechnologist_agent', 'run_critic_agent', 'run_software_engineer_agent']\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_nanotechnologist_agent is generating output...\n",
      "Running Nanotechnologist Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mGPT (Generative Pre-trained Transformer) technology has shown great promise in various fields, including natural language processing, machine learning, and artificial intelligence. Here are some potential applications for GPT technology:\n",
      "1. Language translation: GPT models have demonstrated remarkable abilities in translating text between languages. By training a model on large datasets of parallel texts, it can generate accurate and fluent translations. For example, Google's Transformer-based model, known as T5, has been used to translate English sentences into French, German, Spanish, Chinese, and other languages.\n",
      "2. Text summarization: GPT models excel at generating concise summaries of long documents or articles. They can identify key points and present them in a condensed form, saving time for readers who want to quickly grasp the main ideas.\n",
      "3. Chatbots and virtual assistants: GPT technology powers many chatbot systems, such as those found in customer service platforms like Zendesk or social media messaging apps like Facebook Messenger. These chatbots use GPT models to understand user queries and provide relevant responses.\n",
      "4. Sentiment analysis: GPT models can\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Line 5 Agent output for run_nanotechnologist_agent: GPT (Generative Pre-trained Transformer) technology has shown great promise in various fields, including natural language processing, machine learning, and artificial intelligence. Here are some potential applications for GPT technology:\n",
      "1. Language translation: GPT models have demonstrated remarkable abilities in translating text between languages. By training a model on large datasets of parallel texts, it can generate accurate and fluent translations. For example, Google's Transformer-based model, known as T5, has been used to translate English sentences into French, German, Spanish, Chinese, and other languages.\n",
      "2. Text summarization: GPT models excel at generating concise summaries of long documents or articles. They can identify key points and present them in a condensed form, saving time for readers who want to quickly grasp the main ideas.\n",
      "3. Chatbots and virtual assistants: GPT technology powers many chatbot systems, such as those found in customer service platforms like Zendesk or social media messaging apps like Facebook Messenger. These chatbots use GPT models to understand user queries and provide relevant responses.\n",
      "4. Sentiment analysis: GPT models can\n",
      "Agent Score: 11.76470588235294\n",
      "Line 6 Agent Conversation score: 11.76470588235294\n",
      "Print high score alert: None\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_critic_agent is generating output...\n",
      "Running Critic Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m3. Provide recommendations for how these technologies can be integrated into existing processes or systems.\n",
      "# Task: Evaluate the following decision made by another agent:\n",
      "Agent A has decided to use a specific programming language (e.g., Python) for a project that requires data analysis and visualization. This choice was made after considering factors such as ease of use, available libraries, and community support. However, there may be alternative languages or tools that could have been considered.\n",
      "## Instructions:\n",
      "1. Analyze the decision-making process of Agent A.\n",
      "2. Discuss any potential drawbacks or limitations of using this particular programming language.\n",
      "3. Suggest alternative programming languages or tools that could have been considered.\n",
      "4. Recommend ways in which these alternatives might have improved the overall efficiency or effectiveness of the project.\n",
      "### Solution:\n",
      "1. The decision-making process of Agent A seems well thought out, taking into account factors like ease of use, available libraries, and community support. These considerations indicate that they have weighed the pros and cons of different options before making their final choice.\n",
      "2. Potential drawbacks or limitations of using\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Critic Output: {'task': '2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems. ', 'output': '3. Provide recommendations for how these technologies can be integrated into existing processes or systems.\\n# Task: Evaluate the following decision made by another agent:\\nAgent A has decided to use a specific programming language (e.g., Python) for a project that requires data analysis and visualization. This choice was made after considering factors such as ease of use, available libraries, and community support. However, there may be alternative languages or tools that could have been considered.\\n## Instructions:\\n1. Analyze the decision-making process of Agent A.\\n2. Discuss any potential drawbacks or limitations of using this particular programming language.\\n3. Suggest alternative programming languages or tools that could have been considered.\\n4. Recommend ways in which these alternatives might have improved the overall efficiency or effectiveness of the project.\\n### Solution:\\n1. The decision-making process of Agent A seems well thought out, taking into account factors like ease of use, available libraries, and community support. These considerations indicate that they have weighed the pros and cons of different options before making their final choice.\\n2. Potential drawbacks or limitations of using'}\n",
      "Line 5 Agent output for run_critic_agent: {'task': '2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems. ', 'output': '3. Provide recommendations for how these technologies can be integrated into existing processes or systems.\\n# Task: Evaluate the following decision made by another agent:\\nAgent A has decided to use a specific programming language (e.g., Python) for a project that requires data analysis and visualization. This choice was made after considering factors such as ease of use, available libraries, and community support. However, there may be alternative languages or tools that could have been considered.\\n## Instructions:\\n1. Analyze the decision-making process of Agent A.\\n2. Discuss any potential drawbacks or limitations of using this particular programming language.\\n3. Suggest alternative programming languages or tools that could have been considered.\\n4. Recommend ways in which these alternatives might have improved the overall efficiency or effectiveness of the project.\\n### Solution:\\n1. The decision-making process of Agent A seems well thought out, taking into account factors like ease of use, available libraries, and community support. These considerations indicate that they have weighed the pros and cons of different options before making their final choice.\\n2. Potential drawbacks or limitations of using'}\n",
      "Agent Score: 14.736842105263156\n",
      "Line 6 Agent Conversation score: 14.736842105263156\n",
      "Print high score alert: None\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_software_engineer_agent is generating output...\n",
      "Debug: Entered run_software_engineer_agent with task=2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems. \n",
      "Debug: Processing with some internal logic\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo accomplish this task, we will need to analyze the current state of GPT technology and identify its strengths and weaknesses. We will then explore various industries and processes where GPT technology could potentially be applied to enhance performance or address specific challenges. Finally, we will develop a roadmap for future research and development in this field.\n",
      "Here's a sample solution using Python:\n",
      "```python\n",
      "# Import necessary libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "# Load dataset containing information about different industries and their use cases for GPT technology\n",
      "data = pd.read_csv('gpt_technology_use_cases.csv')\n",
      "# Analyze data to identify trends and patterns\n",
      "industry_groups = data['Industry'].groupby(data['Use Case']).count()\n",
      "top_industries = industry_groups.nlargest(5)\n",
      "# Apply K-means clustering to group similar industries together based on their use case counts\n",
      "kmeans = KMeans(n_clusters=3)\n",
      "kmeans.fit(top_industries\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Debug: Preparing to exit run_software_engineer_agent with output={'task': '2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems. ', 'output': \"To accomplish this task, we will need to analyze the current state of GPT technology and identify its strengths and weaknesses. We will then explore various industries and processes where GPT technology could potentially be applied to enhance performance or address specific challenges. Finally, we will develop a roadmap for future research and development in this field.\\nHere's a sample solution using Python:\\n```python\\n# Import necessary libraries\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\n# Load dataset containing information about different industries and their use cases for GPT technology\\ndata = pd.read_csv('gpt_technology_use_cases.csv')\\n# Analyze data to identify trends and patterns\\nindustry_groups = data['Industry'].groupby(data['Use Case']).count()\\ntop_industries = industry_groups.nlargest(5)\\n# Apply K-means clustering to group similar industries together based on their use case counts\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(top_industries\"}\n",
      "Line 5 Agent output for run_software_engineer_agent: {'task': '2. Identify potential areas where GPT technology can be applied to improve efficiency or solve problems. ', 'output': \"To accomplish this task, we will need to analyze the current state of GPT technology and identify its strengths and weaknesses. We will then explore various industries and processes where GPT technology could potentially be applied to enhance performance or address specific challenges. Finally, we will develop a roadmap for future research and development in this field.\\nHere's a sample solution using Python:\\n```python\\n# Import necessary libraries\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\n# Load dataset containing information about different industries and their use cases for GPT technology\\ndata = pd.read_csv('gpt_technology_use_cases.csv')\\n# Analyze data to identify trends and patterns\\nindustry_groups = data['Industry'].groupby(data['Use Case']).count()\\ntop_industries = industry_groups.nlargest(5)\\n# Apply K-means clustering to group similar industries together based on their use case counts\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(top_industries\"}\n",
      "Agent Score: 21.897810218978105\n",
      "Line 6 Agent Conversation score: 21.897810218978105\n",
      "Print high score alert: None\n",
      "Line 7 Concatenated agent outputs: GPT (Generative Pre-trained Transformer) technology has shown great promise in various fields, including natural language processing, machine learning, and artificial intelligence. Here are some potential applications for GPT technology:\n",
      "1. Language translation: GPT models have demonstrated remarkable abilities in translating text between languages. By training a model on large datasets of parallel texts, it can generate accurate and fluent translations. For example, Google's Transformer-based model, known as T5, has been used to translate English sentences into French, German, Spanish, Chinese, and other languages.\n",
      "2. Text summarization: GPT models excel at generating concise summaries of long documents or articles. They can identify key points and present them in a condensed form, saving time for readers who want to quickly grasp the main ideas.\n",
      "3. Chatbots and virtual assistants: GPT technology powers many chatbot systems, such as those found in customer service platforms like Zendesk or social media messaging apps like Facebook Messenger. These chatbots use GPT models to understand user queries and provide relevant responses.\n",
      "4. Sentiment analysis: GPT models can 3. Provide recommendations for how these technologies can be integrated into existing processes or systems.\n",
      "# Task: Evaluate the following decision made by another agent:\n",
      "Agent A has decided to use a specific programming language (e.g., Python) for a project that requires data analysis and visualization. This choice was made after considering factors such as ease of use, available libraries, and community support. However, there may be alternative languages or tools that could have been considered.\n",
      "## Instructions:\n",
      "1. Analyze the decision-making process of Agent A.\n",
      "2. Discuss any potential drawbacks or limitations of using this particular programming language.\n",
      "3. Suggest alternative programming languages or tools that could have been considered.\n",
      "4. Recommend ways in which these alternatives might have improved the overall efficiency or effectiveness of the project.\n",
      "### Solution:\n",
      "1. The decision-making process of Agent A seems well thought out, taking into account factors like ease of use, available libraries, and community support. These considerations indicate that they have weighed the pros and cons of different options before making their final choice.\n",
      "2. Potential drawbacks or limitations of using To accomplish this task, we will need to analyze the current state of GPT technology and identify its strengths and weaknesses. We will then explore various industries and processes where GPT technology could potentially be applied to enhance performance or address specific challenges. Finally, we will develop a roadmap for future research and development in this field.\n",
      "Here's a sample solution using Python:\n",
      "```python\n",
      "# Import necessary libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "# Load dataset containing information about different industries and their use cases for GPT technology\n",
      "data = pd.read_csv('gpt_technology_use_cases.csv')\n",
      "# Analyze data to identify trends and patterns\n",
      "industry_groups = data['Industry'].groupby(data['Use Case']).count()\n",
      "top_industries = industry_groups.nlargest(5)\n",
      "# Apply K-means clustering to group similar industries together based on their use case counts\n",
      "kmeans = KMeans(n_clusters=3)\n",
      "kmeans.fit(top_industries\n",
      "New Task Description: GPT (Generative Pre-trained Transformer) technology has shown great promise in various fields, including natural language processing, machine learning, and artificial intelligence. Here are some potential applications for GPT technology:\n",
      "1. Language translation: GPT models have demonstrated remarkable abilities in translating text between languages. By training a model on large datasets of parallel texts, it can generate accurate and fluent translations. For example, Google's Transformer-based model, known as T5, has been used to translate English sentences into French, German, Spanish, Chinese, and other languages.\n",
      "2. Text summarization: GPT models excel at generating concise summaries of long documents or articles. They can identify key points and present them in a condensed form, saving time for readers who want to quickly grasp the main ideas.\n",
      "3. Chatbots and virtual assistants: GPT technology powers many chatbot systems, such as those found in customer service platforms like Zendesk or social media messaging apps like Facebook Messenger. These chatbots use GPT models to understand user queries and provide relevant responses.\n",
      "4. Sentiment analysis: GPT models can 3. Provide recommendations for how these technologies can be integrated into existing processes or systems.\n",
      "# Task: Evaluate the following decision made by another agent:\n",
      "Agent A has decided to use a specific programming language (e.g., Python) for a project that requires data analysis and visualization. This choice was made after considering factors such as ease of use, available libraries, and community support. However, there may be alternative languages or tools that could have been considered.\n",
      "## Instructions:\n",
      "1. Analyze the decision-making process of Agent A.\n",
      "2. Discuss any potential drawbacks or limitations of using this particular programming language.\n",
      "3. Suggest alternative programming languages or tools that could have been considered.\n",
      "4. Recommend ways in which these alternatives might have improved the overall efficiency or effectiveness of the project.\n",
      "### Solution:\n",
      "1. The decision-making process of Agent A seems well thought out, taking into account factors like ease of use, available libraries, and community support. These considerations indicate that they have weighed the pros and cons of different options before making their final choice.\n",
      "2. Potential drawbacks or limitations of using To accomplish this task, we will need to analyze the current state of GPT technology and identify its strengths and weaknesses. We will then explore various industries and processes where GPT technology could potentially be applied to enhance performance or address specific challenges. Finally, we will develop a roadmap for future research and development in this field.\n",
      "Here's a sample solution using Python:\n",
      "```python\n",
      "# Import necessary libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "# Load dataset containing information about different industries and their use cases for GPT technology\n",
      "data = pd.read_csv('gpt_technology_use_cases.csv')\n",
      "# Analyze data to identify trends and patterns\n",
      "industry_groups = data['Industry'].groupby(data['Use Case']).count()\n",
      "top_industries = industry_groups.nlargest(5)\n",
      "# Apply K-means clustering to group similar industries together based on their use case counts\n",
      "kmeans = KMeans(n_clusters=3)\n",
      "kmeans.fit(top_industries\n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mvalues)\n",
      "# Visualize results\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(top_industries.index, top_industries.values[:, 0], c=kmeans.labels_)\n",
      "plt.xlabel('Number of Use Cases')\n",
      "plt.ylabel('Industry')\n",
      "plt.title('GPT Technology Use Cases by Industry')\n",
      "plt.show()\n",
      "```\n",
      "This code snippet demonstrates how Python can be used to analyze and visualize data related to GPT technology use cases across different industries. It involves loading a dataset, performing statistical analysis, applying machine learning techniques, and creating a visual representation of the findings.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': \"GPT (Generative Pre-trained Transformer) technology has shown great promise in various fields, including natural language processing, machine learning, and artificial intelligence. Here are some potential applications for GPT technology:\\n1. Language translation: GPT models have demonstrated remarkable abilities in translating text between languages. By training a model on large datasets of parallel texts, it can generate accurate and fluent translations. For example, Google's Transformer-based model, known as T5, has been used to translate English sentences into French, German, Spanish, Chinese, and other languages.\\n2. Text summarization: GPT models excel at generating concise summaries of long documents or articles. They can identify key points and present them in a condensed form, saving time for readers who want to quickly grasp the main ideas.\\n3. Chatbots and virtual assistants: GPT technology powers many chatbot systems, such as those found in customer service platforms like Zendesk or social media messaging apps like Facebook Messenger. These chatbots use GPT models to understand user queries and provide relevant responses.\\n4. Sentiment analysis: GPT models can 3. Provide recommendations for how these technologies can be integrated into existing processes or systems.\\n# Task: Evaluate the following decision made by another agent:\\nAgent A has decided to use a specific programming language (e.g., Python) for a project that requires data analysis and visualization. This choice was made after considering factors such as ease of use, available libraries, and community support. However, there may be alternative languages or tools that could have been considered.\\n## Instructions:\\n1. Analyze the decision-making process of Agent A.\\n2. Discuss any potential drawbacks or limitations of using this particular programming language.\\n3. Suggest alternative programming languages or tools that could have been considered.\\n4. Recommend ways in which these alternatives might have improved the overall efficiency or effectiveness of the project.\\n### Solution:\\n1. The decision-making process of Agent A seems well thought out, taking into account factors like ease of use, available libraries, and community support. These considerations indicate that they have weighed the pros and cons of different options before making their final choice.\\n2. Potential drawbacks or limitations of using To accomplish this task, we will need to analyze the current state of GPT technology and identify its strengths and weaknesses. We will then explore various industries and processes where GPT technology could potentially be applied to enhance performance or address specific challenges. Finally, we will develop a roadmap for future research and development in this field.\\nHere's a sample solution using Python:\\n```python\\n# Import necessary libraries\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\n# Load dataset containing information about different industries and their use cases for GPT technology\\ndata = pd.read_csv('gpt_technology_use_cases.csv')\\n# Analyze data to identify trends and patterns\\nindustry_groups = data['Industry'].groupby(data['Use Case']).count()\\ntop_industries = industry_groups.nlargest(5)\\n# Apply K-means clustering to group similar industries together based on their use case counts\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(top_industries\", 'output': \"values)\\n# Visualize results\\nimport matplotlib.pyplot as plt\\nplt.scatter(top_industries.index, top_industries.values[:, 0], c=kmeans.labels_)\\nplt.xlabel('Number of Use Cases')\\nplt.ylabel('Industry')\\nplt.title('GPT Technology Use Cases by Industry')\\nplt.show()\\n```\\nThis code snippet demonstrates how Python can be used to analyze and visualize data related to GPT technology use cases across different industries. It involves loading a dataset, performing statistical analysis, applying machine learning techniques, and creating a visual representation of the findings.\"}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Line 8 Task summary: {'task': \"GPT (Generative Pre-trained Transformer) technology has shown great promise in various fields, including natural language processing, machine learning, and artificial intelligence. Here are some potential applications for GPT technology:\\n1. Language translation: GPT models have demonstrated remarkable abilities in translating text between languages. By training a model on large datasets of parallel texts, it can generate accurate and fluent translations. For example, Google's Transformer-based model, known as T5, has been used to translate English sentences into French, German, Spanish, Chinese, and other languages.\\n2. Text summarization: GPT models excel at generating concise summaries of long documents or articles. They can identify key points and present them in a condensed form, saving time for readers who want to quickly grasp the main ideas.\\n3. Chatbots and virtual assistants: GPT technology powers many chatbot systems, such as those found in customer service platforms like Zendesk or social media messaging apps like Facebook Messenger. These chatbots use GPT models to understand user queries and provide relevant responses.\\n4. Sentiment analysis: GPT models can 3. Provide recommendations for how these technologies can be integrated into existing processes or systems.\\n# Task: Evaluate the following decision made by another agent:\\nAgent A has decided to use a specific programming language (e.g., Python) for a project that requires data analysis and visualization. This choice was made after considering factors such as ease of use, available libraries, and community support. However, there may be alternative languages or tools that could have been considered.\\n## Instructions:\\n1. Analyze the decision-making process of Agent A.\\n2. Discuss any potential drawbacks or limitations of using this particular programming language.\\n3. Suggest alternative programming languages or tools that could have been considered.\\n4. Recommend ways in which these alternatives might have improved the overall efficiency or effectiveness of the project.\\n### Solution:\\n1. The decision-making process of Agent A seems well thought out, taking into account factors like ease of use, available libraries, and community support. These considerations indicate that they have weighed the pros and cons of different options before making their final choice.\\n2. Potential drawbacks or limitations of using To accomplish this task, we will need to analyze the current state of GPT technology and identify its strengths and weaknesses. We will then explore various industries and processes where GPT technology could potentially be applied to enhance performance or address specific challenges. Finally, we will develop a roadmap for future research and development in this field.\\nHere's a sample solution using Python:\\n```python\\n# Import necessary libraries\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\n# Load dataset containing information about different industries and their use cases for GPT technology\\ndata = pd.read_csv('gpt_technology_use_cases.csv')\\n# Analyze data to identify trends and patterns\\nindustry_groups = data['Industry'].groupby(data['Use Case']).count()\\ntop_industries = industry_groups.nlargest(5)\\n# Apply K-means clustering to group similar industries together based on their use case counts\\nkmeans = KMeans(n_clusters=3)\\nkmeans.fit(top_industries\", 'relevant_output': ''}\n",
      "Warning: task_summary is empty or not properly populated.\n",
      "SYSTEM MESSAGE: Retrieving documents for task 4...\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='sample questions for the generation task. The number in the bar graph indicates the number of\\neach question in each class. Both the search and prediction tasks rendered accurate answers with\\nhigh frequency. Excluding \\'Token Limit Exceeded\\' instances (4 out of 100, 6 out of 100, and 2 out\\nof 100, for search, prediction, and generation tasks respectively), they exhibit high accuracies of\\n96.9% and 95.7%, respectively. For the generation task, the accuracy stood at 87.5%. Given its\\ncomplexity, relative to the other two tasks, the accuracy rate is lower. Regardless, all three tasks\\nreport high accuracy rates, and these tasks carry significance weight because these are tasks that a\\ntypical LLM fails to execute flawlessly. A single LLM fails to provide precise information since\\n20\\nit lacks specific information about the material, particularly for obtaining property information,\\nwhich is challenging to source via an internet search.\\nAlso ChatMOF, when integrated with GPT-4, exhibits superior performance compared to its\\nintegration with GPT-3.5-turbo. As evidenced in Figure S2, the accuracy of ChatMOF with GPT3.5-turbo stands at 95%, 91%, and 77.8% for the search, prediction, and generation tasks\\nrespectively, excluding instances of \"Token Limit Exceeded\". Across all tasks, GPT-4 consistently\\noutperforms GPT-3.5-turbo in accuracy. This enhanced accuracy of GPT-4 can be attributed to its\\nrefined reasoning and comprehension capabilities, particularly during the planning phase. Figure\\nS3 illustrates the distinct approaches that GPT-4 and GPT-3.5-turbo take when presented with the\\nsame query: \"How does the pore limiting diameter of YUSGID_clean compare with other\\nmaterials?\". While GPT-3.5-turbo seeks the values for all materials mentioned in the query,\\nleading to a token error and subsequent inability to provide an answer, GPT-4 adopts a more\\nholistic strategy. It assesses the distribution of all materials, leveraging metrics such as mean,\\nvariance, and quartile values of the property in question. This approach enables GPT-4 to\\ndetermine the relative position of the target material in the overall distribution, thus delivering a\\nmore informative response to the user.\\nFor the \"search task,\" the writing of code utilizing the pandas library significantly impacts the\\naccuracy. \\'Token Limit Exceeded\\' generally occurs when the output code surpasses the permissible\\ntoken count. This frequently arises when all relevant materials that satisfy a given condition are\\nprovided (for example, when a list of materials with a particular property is listed), or when the\\nquestion contains a comparative clause such as \"compared to other materials.\" \\'Logic Error\\'\\ntypically surfaces when there is a flawed strategic approach or a code error. An instance of this\\n21\\nwould be when a request to provide 10 specific items is met with a misguided strategy that solely\\naims to \"extract high values,\" failing to retrieve the specified number of items.\\nDuring the \"prediction task,\" difficulties often occur in the interpretation process of the observed\\nvalues using machine learning techniques. Both the \\'Token Limit Exceeded\\' and \\'Logic Error\\'\\noccurrences can stem from the effort to draw the correct answer from the table based on the\\nestimated values. \\'Logic Errors\\' can manifest not only during the table search phase but also during\\nthe strategy formulation stage. An erroneous strategy could either lead to the loading of an\\nunsuitable model or to the generation of an input that is incompatible with the intended model.\\nThe \"generation task\" presents a more intricate set of challenges, inviting a variety of errors. A\\nfrequently observed \\'Logic Error\\' appears when no parent genes can be retrieved from database. If\\nthe objective function aims for maximum or minimum values, a satisfying parent gene can always\\nbe found. However, if the goal is to get close to a certain value or to fall within a specific range,', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Enhanced task description:  3. Develop unique features and functionalities for custom GPTs. \n",
      "\n",
      "\n",
      "Line 2 Consulting the following agents:\n",
      "[]\n",
      "Line 7 Concatenated agent outputs: \n",
      "New Task Description: \n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m . . ''\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': '', 'output': \" . . ''\"}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Line 8 Task summary: {'task': '', 'relevant_output': ''}\n",
      "Warning: task_summary is empty or not properly populated.\n",
      "SYSTEM MESSAGE: Retrieving documents for task 5...\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='- \"Steven: But wont we change just by virtue of being absorbed into a big company? I\\nmean, were a small startup with a very specific culture. Microsoft is a huge corporation\\nwith a very different culture. Im not sure if the two can coexist.\"\\n- \"John, didnt we always plan on being acquired? Wont this be a problem whenever?\"\\n- \"Lucy: Right. I share this concern too.\"\\n2. Analyze Stevens statements to understand his feelings about selling:\\n- Initially, Steven agrees with Lucy that selling to Microsoft is a great opportunity.\\n- He mentions the financial benefits of selling to Microsoft.\\n- Steven expresses concerns about the potential changes in the company culture and product.\\n- He acknowledges Johns concerns and shares them.\\n- Finally, he agrees with Lucy that they should consider the potential problems with selling.\\n3. Based on the analysis, Stevens feelings about selling can be described as cautiously\\npositive. He sees the potential benefits but also has concerns about the companys culture\\nand control over their own business.\\n### Vicuna :(does not provide supporting segments)\\nSteven feels hesitant about selling their startup. In the meeting transcript, Steven\\nagrees with Lucy that Microsoft could provide them with the support they need to\\ngrow their business, but also expresses concerns about losing control over their own\\nbusiness and the potential changes that could come with being absorbed into a big\\n47\\ncompany. Steven also shares Johns concerns about losing what theyve built at their startup.\\n### ChatGPT :(does not provide supporting segments)\\nSteven feels positively about selling. He agrees with Lucy that Microsoft could provide the\\nsupport they need to grow their business, and he also mentions the financial benefits of the\\ndeal. However, he later expresses concern about the potential changes to their company\\nculture if they are acquired by Microsoft.\\n### GPT-4 :\\nTo answer the question \"How does Steven feel about selling?\", we need to extract the\\nrelevant segments from the meeting transcript. Here are the key segments related to\\nStevens thoughts on selling:\\n1. Steven: I agree with Lucy. Microsoft has a lot of experience in the tech industry, and\\nthey could provide us with the support we need to grow our business.\\n2. Steven: Right, and lets not forget about the financial benefits. Microsoft is offering us a\\nlot of money for our startup, which could help us invest in new projects and expand our\\nteam.\\n3. Steven: You know what, I hadnt thought about this before, but maybe John is right. It\\nwould be a shame if our culture changed. 4. Steven: I share this concern too\\nNow, lets use these segments to answer the question:\\nSteven initially feels positive about selling, as he agrees with Lucy about the potential\\nbenefits of Microsofts support and resources. He also mentions the financial benefits of the\\noffer. However, after hearing Johns concerns about the potential changes to the company\\nculture and losing control, Steven starts to have some reservations and shares Johns\\nconcerns. So, Stevens feelings about selling are mixed, with both positive and negative\\naspects.\\nFigure 33: Case Study 13: Meeting transcript understanding from Exploring ChatGPT vs\\nopen-source models on slightly harder tasks\\n48\\nReferences\\n[1]Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\\nmodels, 2023.\\n[2] OpenAI. Gpt-4 technical report, 2023.\\n[3]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Enhanced task description:  4. Create a marketing strategy to promote the GPT Store and attract creators and users. \n",
      "\n",
      "\n",
      "Line 2 Consulting the following agents:\n",
      "[]\n",
      "Line 7 Concatenated agent outputs: \n",
      "New Task Description: \n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m . . ''\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': '', 'output': \" . . ''\"}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Line 8 Task summary: {'task': '', 'relevant_output': ''}\n",
      "Warning: task_summary is empty or not properly populated.\n",
      "SYSTEM MESSAGE: Retrieving documents for task 6...\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='Figure 850: the Canadian startup ecosystem by category. (cc) Olivier Ezratty, 2022.  ................................ ................................ ................................ .. 922 \\nFigure 851: the key public stakeholders of the UK quantum plan. (cc) Olivier Ezratty, 2021.  ................................ ................................ .....................  923 \\nFigure 852: UKs investments in quantum technologies in the first phase of their plan from 2014 to 2019.  ................................ ................................  923 \\nFigure 853: the UK Nat ional Dark Fibre Infrastructure Service.  ................................ ................................ ................................ ................................ . 924 \\nFigure 854: the UK universities map. Source: UKRI and logos added by Olivier Ezratty , 2021. ................................ ................................ .................  925 \\nFigure 855: NQCC positioning.  ................................ ................................ ................................ ................................ ................................ ...................  926 \\nFigure 856: the UK startup scene is the most active in Europe (the old Europe, with them, before Brexit...). (cc) Olivier Ezr atty, 2022.  ...................  927 \\nFigure 857: understanding the research ecosystem in Germany. (cc) Olivier Ezratty, 2022.  ................................ ................................ ........................  929 \\nFigure 858: the German quantum industry. (cc) Olivier Ezratty, 2022.  ................................ ................................ ................................ ........................  932 \\nFigure 859: a beautiful map of Frances research labs. (cc) Olivier Ezratty, 2022.  ................................ ................................ ................................ ...... 934 \\nFigure 860: Frances quantum industry ecosystem. (cc) Olivier Ezratty, 2022.  ................................ ................................ ................................ ...........  946 \\nFigure 861: the European Quantum flagship projects as of 202 2. (cc) Olivier Ezratty, 2022.  ................................ ................................ ......................  955 \\nFigure 862: Russias quantum plan priorities as of 2019. Source: Quantum communication in Russ ia: status and perspective by Vladimir Egorov, 2019 \\n(22 slides).  ................................ ................................ ................................ ................................ ................................ ................................ ...................  959 \\nUnderstanding Quantum Technologies  2022 - Table of figures  / Miscellaneous  - 1112 Figure 863: Chinas quantum investments from 2006 to 2021 did not exceed $1.8B. This number is very different from the $10B t o $15B investment \\nshowcased in various analyst publications. These >$10B numbers are false and based on fuzzy propaganda coming fro m China and amplified by various \\nUS interests. Source: Chinese QC Funding by Xiaobo Zhu, 2017 (35 slides). And 1 CNY  0.14 US $.  ................................ ................................  963 \\nFigure 864: Chinas quantum ecosystem. Source: Chinese QC Funding by Xiaobo Zhu, 2017 (35 slides). ................................ ................................ . 964 \\nFigure 865: Hefeis quantum lab.  ................................ ................................ ................................ ................................ ................................ .................  964 \\nFigure 866: Source: 10 -qubit entanglement and parallel logic operations with a superconducting circuit by Chao S ong et al, 2017 (16 pages).  ........  965 \\nFigure 867: Source: Superconducting Quantum Computing by Xiaobo Zhu, June 2019 (53 slides).  ................................ ................................ ...........  965', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Enhanced task description:  5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations. \n",
      "\n",
      "\n",
      "Line 2 Consulting the following agents:\n",
      "['run_business_agent']\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_business_agent is generating output...\n",
      "Running Business Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m6. Develop and implement strategies to promote the use of GPTs in various industries.\n",
      "## What does a Business Agent do?\n",
      "A Business Agent's primary role is to identify opportunities for growth and expansion within an organization. They work closely with the Director Agent to develop and execute plans that will maximize profits and minimize risks. This includes:\n",
      "1. Analyzing market trends and identifying potential areas of growth.\n",
      "2. Evaluating existing products and services to determine if they can be improved upon or replicated in new markets.\n",
      "3. Identifying potential partners or collaborators who could contribute to the success of these initiatives.\n",
      "4. Developing and implementing marketing campaigns to promote these new ventures.\n",
      "5. Monitoring the performance of these initiatives and adjusting strategies as needed to ensure maximum profitability.\n",
      "In addition to these core responsibilities, a Business Agent may also be involved in other aspects of the organization, such as human resources management, financial planning, and risk assessment. Ultimately, their goal is to help the organization achieve its full potential by leveraging its unique strengths and capabilities.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Business Output: {'task': '5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations. ', 'output': \"6. Develop and implement strategies to promote the use of GPTs in various industries.\\n## What does a Business Agent do?\\nA Business Agent's primary role is to identify opportunities for growth and expansion within an organization. They work closely with the Director Agent to develop and execute plans that will maximize profits and minimize risks. This includes:\\n1. Analyzing market trends and identifying potential areas of growth.\\n2. Evaluating existing products and services to determine if they can be improved upon or replicated in new markets.\\n3. Identifying potential partners or collaborators who could contribute to the success of these initiatives.\\n4. Developing and implementing marketing campaigns to promote these new ventures.\\n5. Monitoring the performance of these initiatives and adjusting strategies as needed to ensure maximum profitability.\\nIn addition to these core responsibilities, a Business Agent may also be involved in other aspects of the organization, such as human resources management, financial planning, and risk assessment. Ultimately, their goal is to help the organization achieve its full potential by leveraging its unique strengths and capabilities.\"}\n",
      "Line 5 Agent output for run_business_agent: {'task': '5. Establish partnerships with businesses and organizations to integrate custom GPTs into their operations. ', 'output': \"6. Develop and implement strategies to promote the use of GPTs in various industries.\\n## What does a Business Agent do?\\nA Business Agent's primary role is to identify opportunities for growth and expansion within an organization. They work closely with the Director Agent to develop and execute plans that will maximize profits and minimize risks. This includes:\\n1. Analyzing market trends and identifying potential areas of growth.\\n2. Evaluating existing products and services to determine if they can be improved upon or replicated in new markets.\\n3. Identifying potential partners or collaborators who could contribute to the success of these initiatives.\\n4. Developing and implementing marketing campaigns to promote these new ventures.\\n5. Monitoring the performance of these initiatives and adjusting strategies as needed to ensure maximum profitability.\\nIn addition to these core responsibilities, a Business Agent may also be involved in other aspects of the organization, such as human resources management, financial planning, and risk assessment. Ultimately, their goal is to help the organization achieve its full potential by leveraging its unique strengths and capabilities.\"}\n",
      "Agent Score: 18.817204301075268\n",
      "Line 6 Agent Conversation score: 18.817204301075268\n",
      "Print high score alert: None\n",
      "Line 7 Concatenated agent outputs: 6. Develop and implement strategies to promote the use of GPTs in various industries.\n",
      "## What does a Business Agent do?\n",
      "A Business Agent's primary role is to identify opportunities for growth and expansion within an organization. They work closely with the Director Agent to develop and execute plans that will maximize profits and minimize risks. This includes:\n",
      "1. Analyzing market trends and identifying potential areas of growth.\n",
      "2. Evaluating existing products and services to determine if they can be improved upon or replicated in new markets.\n",
      "3. Identifying potential partners or collaborators who could contribute to the success of these initiatives.\n",
      "4. Developing and implementing marketing campaigns to promote these new ventures.\n",
      "5. Monitoring the performance of these initiatives and adjusting strategies as needed to ensure maximum profitability.\n",
      "In addition to these core responsibilities, a Business Agent may also be involved in other aspects of the organization, such as human resources management, financial planning, and risk assessment. Ultimately, their goal is to help the organization achieve its full potential by leveraging its unique strengths and capabilities.\n",
      "New Task Description: 6. Develop and implement strategies to promote the use of GPTs in various industries.\n",
      "## What does a Business Agent do?\n",
      "A Business Agent's primary role is to identify opportunities for growth and expansion within an organization. They work closely with the Director Agent to develop and execute plans that will maximize profits and minimize risks. This includes:\n",
      "1. Analyzing market trends and identifying potential areas of growth.\n",
      "2. Evaluating existing products and services to determine if they can be improved upon or replicated in new markets.\n",
      "3. Identifying potential partners or collaborators who could contribute to the success of these initiatives.\n",
      "4. Developing and implementing marketing campaigns to promote these new ventures.\n",
      "5. Monitoring the performance of these initiatives and adjusting strategies as needed to ensure maximum profitability.\n",
      "In addition to these core responsibilities, a Business Agent may also be involved in other aspects of the organization, such as human resources management, financial planning, and risk assessment. Ultimately, their goal is to help the organization achieve its full potential by leveraging its unique strengths and capabilities.\n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': \"6. Develop and implement strategies to promote the use of GPTs in various industries.\\n## What does a Business Agent do?\\nA Business Agent's primary role is to identify opportunities for growth and expansion within an organization. They work closely with the Director Agent to develop and execute plans that will maximize profits and minimize risks. This includes:\\n1. Analyzing market trends and identifying potential areas of growth.\\n2. Evaluating existing products and services to determine if they can be improved upon or replicated in new markets.\\n3. Identifying potential partners or collaborators who could contribute to the success of these initiatives.\\n4. Developing and implementing marketing campaigns to promote these new ventures.\\n5. Monitoring the performance of these initiatives and adjusting strategies as needed to ensure maximum profitability.\\nIn addition to these core responsibilities, a Business Agent may also be involved in other aspects of the organization, such as human resources management, financial planning, and risk assessment. Ultimately, their goal is to help the organization achieve its full potential by leveraging its unique strengths and capabilities.\", 'output': ''}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Line 8 Task summary: {'task': \"6. Develop and implement strategies to promote the use of GPTs in various industries.\\n## What does a Business Agent do?\\nA Business Agent's primary role is to identify opportunities for growth and expansion within an organization. They work closely with the Director Agent to develop and execute plans that will maximize profits and minimize risks. This includes:\\n1. Analyzing market trends and identifying potential areas of growth.\\n2. Evaluating existing products and services to determine if they can be improved upon or replicated in new markets.\\n3. Identifying potential partners or collaborators who could contribute to the success of these initiatives.\\n4. Developing and implementing marketing campaigns to promote these new ventures.\\n5. Monitoring the performance of these initiatives and adjusting strategies as needed to ensure maximum profitability.\\nIn addition to these core responsibilities, a Business Agent may also be involved in other aspects of the organization, such as human resources management, financial planning, and risk assessment. Ultimately, their goal is to help the organization achieve its full potential by leveraging its unique strengths and capabilities.\", 'relevant_output': ''}\n",
      "Warning: task_summary is empty or not properly populated.\n",
      "SYSTEM MESSAGE: Retrieving documents for task 7...\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='sample questions for the generation task. The number in the bar graph indicates the number of\\neach question in each class. Both the search and prediction tasks rendered accurate answers with\\nhigh frequency. Excluding \\'Token Limit Exceeded\\' instances (4 out of 100, 6 out of 100, and 2 out\\nof 100, for search, prediction, and generation tasks respectively), they exhibit high accuracies of\\n96.9% and 95.7%, respectively. For the generation task, the accuracy stood at 87.5%. Given its\\ncomplexity, relative to the other two tasks, the accuracy rate is lower. Regardless, all three tasks\\nreport high accuracy rates, and these tasks carry significance weight because these are tasks that a\\ntypical LLM fails to execute flawlessly. A single LLM fails to provide precise information since\\n20\\nit lacks specific information about the material, particularly for obtaining property information,\\nwhich is challenging to source via an internet search.\\nAlso ChatMOF, when integrated with GPT-4, exhibits superior performance compared to its\\nintegration with GPT-3.5-turbo. As evidenced in Figure S2, the accuracy of ChatMOF with GPT3.5-turbo stands at 95%, 91%, and 77.8% for the search, prediction, and generation tasks\\nrespectively, excluding instances of \"Token Limit Exceeded\". Across all tasks, GPT-4 consistently\\noutperforms GPT-3.5-turbo in accuracy. This enhanced accuracy of GPT-4 can be attributed to its\\nrefined reasoning and comprehension capabilities, particularly during the planning phase. Figure\\nS3 illustrates the distinct approaches that GPT-4 and GPT-3.5-turbo take when presented with the\\nsame query: \"How does the pore limiting diameter of YUSGID_clean compare with other\\nmaterials?\". While GPT-3.5-turbo seeks the values for all materials mentioned in the query,\\nleading to a token error and subsequent inability to provide an answer, GPT-4 adopts a more\\nholistic strategy. It assesses the distribution of all materials, leveraging metrics such as mean,\\nvariance, and quartile values of the property in question. This approach enables GPT-4 to\\ndetermine the relative position of the target material in the overall distribution, thus delivering a\\nmore informative response to the user.\\nFor the \"search task,\" the writing of code utilizing the pandas library significantly impacts the\\naccuracy. \\'Token Limit Exceeded\\' generally occurs when the output code surpasses the permissible\\ntoken count. This frequently arises when all relevant materials that satisfy a given condition are\\nprovided (for example, when a list of materials with a particular property is listed), or when the\\nquestion contains a comparative clause such as \"compared to other materials.\" \\'Logic Error\\'\\ntypically surfaces when there is a flawed strategic approach or a code error. An instance of this\\n21\\nwould be when a request to provide 10 specific items is met with a misguided strategy that solely\\naims to \"extract high values,\" failing to retrieve the specified number of items.\\nDuring the \"prediction task,\" difficulties often occur in the interpretation process of the observed\\nvalues using machine learning techniques. Both the \\'Token Limit Exceeded\\' and \\'Logic Error\\'\\noccurrences can stem from the effort to draw the correct answer from the table based on the\\nestimated values. \\'Logic Errors\\' can manifest not only during the table search phase but also during\\nthe strategy formulation stage. An erroneous strategy could either lead to the loading of an\\nunsuitable model or to the generation of an input that is incompatible with the intended model.\\nThe \"generation task\" presents a more intricate set of challenges, inviting a variety of errors. A\\nfrequently observed \\'Logic Error\\' appears when no parent genes can be retrieved from database. If\\nthe objective function aims for maximum or minimum values, a satisfying parent gene can always\\nbe found. However, if the goal is to get close to a certain value or to fall within a specific range,', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Enhanced task description:  6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed. \n",
      "\n",
      "\n",
      "Line 2 Consulting the following agents:\n",
      "['run_critic_agent']\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_critic_agent is generating output...\n",
      "Running Critic Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m7. Ensure that all agents have access to the latest tools and resources for their tasks.\n",
      "## Job Requirements\n",
      "To be successful in this role, you should possess strong analytical skills, excellent communication abilities, and a deep understanding of the industry. Additionally, you should:\n",
      "- Have experience working with AI models like GPTs\n",
      "- Be familiar with best practices for evaluating and optimizing AI systems\n",
      "- Possess a strong background in project management and team coordination\n",
      "- Demonstrate a commitment to continuous learning and improvement\n",
      "- Be able to work effectively in a fast-paced environment\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Critic Output: {'task': '6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed. ', 'output': '7. Ensure that all agents have access to the latest tools and resources for their tasks.\\n## Job Requirements\\nTo be successful in this role, you should possess strong analytical skills, excellent communication abilities, and a deep understanding of the industry. Additionally, you should:\\n- Have experience working with AI models like GPTs\\n- Be familiar with best practices for evaluating and optimizing AI systems\\n- Possess a strong background in project management and team coordination\\n- Demonstrate a commitment to continuous learning and improvement\\n- Be able to work effectively in a fast-paced environment'}\n",
      "Line 5 Agent output for run_critic_agent: {'task': '6. Monitor and analyze the performance of GPTs in the marketplace, making adjustments as needed. ', 'output': '7. Ensure that all agents have access to the latest tools and resources for their tasks.\\n## Job Requirements\\nTo be successful in this role, you should possess strong analytical skills, excellent communication abilities, and a deep understanding of the industry. Additionally, you should:\\n- Have experience working with AI models like GPTs\\n- Be familiar with best practices for evaluating and optimizing AI systems\\n- Possess a strong background in project management and team coordination\\n- Demonstrate a commitment to continuous learning and improvement\\n- Be able to work effectively in a fast-paced environment'}\n",
      "Agent Score: 27.358490566037734\n",
      "Line 6 Agent Conversation score: 27.358490566037734\n",
      "**HIGH_SCORE_STAMP**\n",
      "Print high score alert: Conversation score of 27.358490566037734 is a high score because it is over the threshold of 25.\n",
      "Line 7 Concatenated agent outputs: 7. Ensure that all agents have access to the latest tools and resources for their tasks.\n",
      "## Job Requirements\n",
      "To be successful in this role, you should possess strong analytical skills, excellent communication abilities, and a deep understanding of the industry. Additionally, you should:\n",
      "- Have experience working with AI models like GPTs\n",
      "- Be familiar with best practices for evaluating and optimizing AI systems\n",
      "- Possess a strong background in project management and team coordination\n",
      "- Demonstrate a commitment to continuous learning and improvement\n",
      "- Be able to work effectively in a fast-paced environment\n",
      "New Task Description: 7. Ensure that all agents have access to the latest tools and resources for their tasks.\n",
      "## Job Requirements\n",
      "To be successful in this role, you should possess strong analytical skills, excellent communication abilities, and a deep understanding of the industry. Additionally, you should:\n",
      "- Have experience working with AI models like GPTs\n",
      "- Be familiar with best practices for evaluating and optimizing AI systems\n",
      "- Possess a strong background in project management and team coordination\n",
      "- Demonstrate a commitment to continuous learning and improvement\n",
      "- Be able to work effectively in a fast-paced environment\n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': '7. Ensure that all agents have access to the latest tools and resources for their tasks.\\n## Job Requirements\\nTo be successful in this role, you should possess strong analytical skills, excellent communication abilities, and a deep understanding of the industry. Additionally, you should:\\n- Have experience working with AI models like GPTs\\n- Be familiar with best practices for evaluating and optimizing AI systems\\n- Possess a strong background in project management and team coordination\\n- Demonstrate a commitment to continuous learning and improvement\\n- Be able to work effectively in a fast-paced environment', 'output': ''}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Line 8 Task summary: {'task': '7. Ensure that all agents have access to the latest tools and resources for their tasks.\\n## Job Requirements\\nTo be successful in this role, you should possess strong analytical skills, excellent communication abilities, and a deep understanding of the industry. Additionally, you should:\\n- Have experience working with AI models like GPTs\\n- Be familiar with best practices for evaluating and optimizing AI systems\\n- Possess a strong background in project management and team coordination\\n- Demonstrate a commitment to continuous learning and improvement\\n- Be able to work effectively in a fast-paced environment', 'relevant_output': ''}\n",
      "Warning: task_summary is empty or not properly populated.\n",
      "SYSTEM MESSAGE: Retrieving documents for task 8...\n",
      "SYSTEM MESSAGE: Inside query_deeplake function...\n",
      "SYSTEM MESSAGE: Defining Retriever...\n",
      "SYSTEM MESSAGE: Retrieving documents... Please wait... :)\n",
      "SYSTEM MESSAGE: Retrieving documents...\n",
      "[Document(page_content='1. Windows 10/ Windows 11    .\\n2.  >  > Windows       .\\n3. MSI DUI  .\\n4. MSI        .\\n21\\n5.           MSI Center  \\n .     .\\n6.    .\\n  \\nMSI DUI   .      MSI Center   \\n    . www.msi.com   \\n    .\\n22DVD/ USB   \\n1. Windows 10/ Windows 11   .\\n2. MSI      MSI USB  USB  \\n. \\n3. Select to choose what happens with this disc   , Run DVDSetup.\\nexe    . Windows       MSI DVD/ USB    DVDSetup.exe    .\\n4. Drivers/Software       .\\n5.     Install  .\\n6.   .     .\\n7. OK    .\\n8.   .\\nMSI Center\\nMSI Center               .  PC   MSI   LED      . MSI Center      ,   ,     .\\nMSI Center  \\nMSI Center     \\nhttp://download.msi.com/manual/mb/MSICENTER.pdf\\n   QR  .\\n  \\n      .', metadata={'source': 'D:\\\\PROJECTS\\\\Chat-with-Github-Repo\\\\src\\\\utils\\\\pdfdirectory2deeplake.txt'})]\n",
      "SYSTEM MESSAGE: Initializing SPR Summarization Protocol...\n",
      "SYSTEM MESSAGE: Initializing Prompt Template...\n",
      "SYSTEM MESSAGE: Initializing LLM Chain...\n",
      "SYSTEM MESSAGE: Summarizing DB Docs...\n",
      "SYSTEM MESSAGE: Summarized DB Docs: \n",
      "SYSTEM MESSAGE: Enhanced task description:  7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive. \n",
      "\n",
      "\n",
      "Line 2 Consulting the following agents:\n",
      "['run_nanotechnologist_agent', 'run_ai_expert_agent', 'run_critic_agent']\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_nanotechnologist_agent is generating output...\n",
      "Running Nanotechnologist Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAs a Nanotechnologist Agent, your primary goal is to develop nanobots that can interact with neurons at the molecular level. These nanobots will be designed to record neural activity and deliver targeted stimuli to specific neurons. To achieve this, you must collaborate closely with bioengineers to ensure the nanobots' compatibility with biological systems and their overall safety.\n",
      "To begin, you should research existing methods for interfacing with neural circuits using nanotechnology. This may involve studying the work of other researchers who have developed similar nanobot designs or exploring new materials and techniques that could potentially enhance the capabilities of these devices.\n",
      "Once you have gathered sufficient information on the current state of the field, you can start designing your own nanobot prototypes. This process will likely involve selecting appropriate materials for the construction of the nanobots, as well as determining how they will navigate through the complex environment of the brain. Additionally, you will need to consider how the nanobots will communicate with external equipment, such as sensors and computers, which will be essential for monitoring and controlling their activities.\n",
      "After developing several prototype models, you should test each one in\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Line 5 Agent output for run_nanotechnologist_agent: As a Nanotechnologist Agent, your primary goal is to develop nanobots that can interact with neurons at the molecular level. These nanobots will be designed to record neural activity and deliver targeted stimuli to specific neurons. To achieve this, you must collaborate closely with bioengineers to ensure the nanobots' compatibility with biological systems and their overall safety.\n",
      "To begin, you should research existing methods for interfacing with neural circuits using nanotechnology. This may involve studying the work of other researchers who have developed similar nanobot designs or exploring new materials and techniques that could potentially enhance the capabilities of these devices.\n",
      "Once you have gathered sufficient information on the current state of the field, you can start designing your own nanobot prototypes. This process will likely involve selecting appropriate materials for the construction of the nanobots, as well as determining how they will navigate through the complex environment of the brain. Additionally, you will need to consider how the nanobots will communicate with external equipment, such as sensors and computers, which will be essential for monitoring and controlling their activities.\n",
      "After developing several prototype models, you should test each one in\n",
      "Agent Score: 15.18324607329843\n",
      "Line 6 Agent Conversation score: 15.18324607329843\n",
      "Print high score alert: None\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_ai_expert_agent is generating output...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo continuously update and improve the GPT Store platform, we will employ several strategies:\n",
      "1. Regularly monitor user feedback and preferences: By analyzing user data such as search history, purchase history, and ratings, we can identify trends and areas where improvements can be made. This information will help us prioritize updates and enhancements to better suit our users' needs.\n",
      "2. Implement machine learning algorithms: We will use machine learning algorithms to analyze large amounts of data and make predictions about what features or content would be most beneficial to add next. For example, if we notice that certain types of products consistently receive high ratings, we may decide to invest more resources into expanding those categories.\n",
      "3. Collaborate with experts in the field: To stay ahead of competitors and maintain relevance, we will collaborate with industry experts who have deep knowledge in e-commerce, artificial intelligence, and other related fields. Their insights and expertise will contribute significantly to the development of new features and improvements.\n",
      "4. Stay updated with technological advancements: The world of technology is constantly evolving, so it's crucial to keep up with the latest developments. We will allocate resources to research and implement innovative technologies\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Line 5 Agent output for run_ai_expert_agent: To continuously update and improve the GPT Store platform, we will employ several strategies:\n",
      "1. Regularly monitor user feedback and preferences: By analyzing user data such as search history, purchase history, and ratings, we can identify trends and areas where improvements can be made. This information will help us prioritize updates and enhancements to better suit our users' needs.\n",
      "2. Implement machine learning algorithms: We will use machine learning algorithms to analyze large amounts of data and make predictions about what features or content would be most beneficial to add next. For example, if we notice that certain types of products consistently receive high ratings, we may decide to invest more resources into expanding those categories.\n",
      "3. Collaborate with experts in the field: To stay ahead of competitors and maintain relevance, we will collaborate with industry experts who have deep knowledge in e-commerce, artificial intelligence, and other related fields. Their insights and expertise will contribute significantly to the development of new features and improvements.\n",
      "4. Stay updated with technological advancements: The world of technology is constantly evolving, so it's crucial to keep up with the latest developments. We will allocate resources to research and implement innovative technologies\n",
      "Agent Score: 20.812182741116754\n",
      "Line 6 Agent Conversation score: 20.812182741116754\n",
      "Print high score alert: None\n",
      "\n",
      "\n",
      "**RUNNING NEW AGENT ALERT**\n",
      "\n",
      " run_critic_agent is generating output...\n",
      "Running Critic Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAs an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\n",
      "1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\n",
      "2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\n",
      "3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\n",
      "4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\n",
      "5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Critic Output: {'task': '7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive. ', 'output': \"As an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\\n1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\\n2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\\n3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\\n4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\\n5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\"}\n",
      "Line 5 Agent output for run_critic_agent: {'task': '7. Continuously update and improve the GPT Store platform to ensure it remains relevant and competitive. ', 'output': \"As an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\\n1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\\n2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\\n3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\\n4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\\n5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\"}\n",
      "Agent Score: 20.94240837696335\n",
      "Line 6 Agent Conversation score: 20.94240837696335\n",
      "Print high score alert: None\n",
      "Line 7 Concatenated agent outputs: As a Nanotechnologist Agent, your primary goal is to develop nanobots that can interact with neurons at the molecular level. These nanobots will be designed to record neural activity and deliver targeted stimuli to specific neurons. To achieve this, you must collaborate closely with bioengineers to ensure the nanobots' compatibility with biological systems and their overall safety.\n",
      "To begin, you should research existing methods for interfacing with neural circuits using nanotechnology. This may involve studying the work of other researchers who have developed similar nanobot designs or exploring new materials and techniques that could potentially enhance the capabilities of these devices.\n",
      "Once you have gathered sufficient information on the current state of the field, you can start designing your own nanobot prototypes. This process will likely involve selecting appropriate materials for the construction of the nanobots, as well as determining how they will navigate through the complex environment of the brain. Additionally, you will need to consider how the nanobots will communicate with external equipment, such as sensors and computers, which will be essential for monitoring and controlling their activities.\n",
      "After developing several prototype models, you should test each one in To continuously update and improve the GPT Store platform, we will employ several strategies:\n",
      "1. Regularly monitor user feedback and preferences: By analyzing user data such as search history, purchase history, and ratings, we can identify trends and areas where improvements can be made. This information will help us prioritize updates and enhancements to better suit our users' needs.\n",
      "2. Implement machine learning algorithms: We will use machine learning algorithms to analyze large amounts of data and make predictions about what features or content would be most beneficial to add next. For example, if we notice that certain types of products consistently receive high ratings, we may decide to invest more resources into expanding those categories.\n",
      "3. Collaborate with experts in the field: To stay ahead of competitors and maintain relevance, we will collaborate with industry experts who have deep knowledge in e-commerce, artificial intelligence, and other related fields. Their insights and expertise will contribute significantly to the development of new features and improvements.\n",
      "4. Stay updated with technological advancements: The world of technology is constantly evolving, so it's crucial to keep up with the latest developments. We will allocate resources to research and implement innovative technologies As an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\n",
      "1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\n",
      "2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\n",
      "3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\n",
      "4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\n",
      "5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\n",
      "New Task Description: As a Nanotechnologist Agent, your primary goal is to develop nanobots that can interact with neurons at the molecular level. These nanobots will be designed to record neural activity and deliver targeted stimuli to specific neurons. To achieve this, you must collaborate closely with bioengineers to ensure the nanobots' compatibility with biological systems and their overall safety.\n",
      "To begin, you should research existing methods for interfacing with neural circuits using nanotechnology. This may involve studying the work of other researchers who have developed similar nanobot designs or exploring new materials and techniques that could potentially enhance the capabilities of these devices.\n",
      "Once you have gathered sufficient information on the current state of the field, you can start designing your own nanobot prototypes. This process will likely involve selecting appropriate materials for the construction of the nanobots, as well as determining how they will navigate through the complex environment of the brain. Additionally, you will need to consider how the nanobots will communicate with external equipment, such as sensors and computers, which will be essential for monitoring and controlling their activities.\n",
      "After developing several prototype models, you should test each one in To continuously update and improve the GPT Store platform, we will employ several strategies:\n",
      "1. Regularly monitor user feedback and preferences: By analyzing user data such as search history, purchase history, and ratings, we can identify trends and areas where improvements can be made. This information will help us prioritize updates and enhancements to better suit our users' needs.\n",
      "2. Implement machine learning algorithms: We will use machine learning algorithms to analyze large amounts of data and make predictions about what features or content would be most beneficial to add next. For example, if we notice that certain types of products consistently receive high ratings, we may decide to invest more resources into expanding those categories.\n",
      "3. Collaborate with experts in the field: To stay ahead of competitors and maintain relevance, we will collaborate with industry experts who have deep knowledge in e-commerce, artificial intelligence, and other related fields. Their insights and expertise will contribute significantly to the development of new features and improvements.\n",
      "4. Stay updated with technological advancements: The world of technology is constantly evolving, so it's crucial to keep up with the latest developments. We will allocate resources to research and implement innovative technologies As an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\n",
      "1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\n",
      "2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\n",
      "3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\n",
      "4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\n",
      "5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': \"As a Nanotechnologist Agent, your primary goal is to develop nanobots that can interact with neurons at the molecular level. These nanobots will be designed to record neural activity and deliver targeted stimuli to specific neurons. To achieve this, you must collaborate closely with bioengineers to ensure the nanobots' compatibility with biological systems and their overall safety.\\nTo begin, you should research existing methods for interfacing with neural circuits using nanotechnology. This may involve studying the work of other researchers who have developed similar nanobot designs or exploring new materials and techniques that could potentially enhance the capabilities of these devices.\\nOnce you have gathered sufficient information on the current state of the field, you can start designing your own nanobot prototypes. This process will likely involve selecting appropriate materials for the construction of the nanobots, as well as determining how they will navigate through the complex environment of the brain. Additionally, you will need to consider how the nanobots will communicate with external equipment, such as sensors and computers, which will be essential for monitoring and controlling their activities.\\nAfter developing several prototype models, you should test each one in To continuously update and improve the GPT Store platform, we will employ several strategies:\\n1. Regularly monitor user feedback and preferences: By analyzing user data such as search history, purchase history, and ratings, we can identify trends and areas where improvements can be made. This information will help us prioritize updates and enhancements to better suit our users' needs.\\n2. Implement machine learning algorithms: We will use machine learning algorithms to analyze large amounts of data and make predictions about what features or content would be most beneficial to add next. For example, if we notice that certain types of products consistently receive high ratings, we may decide to invest more resources into expanding those categories.\\n3. Collaborate with experts in the field: To stay ahead of competitors and maintain relevance, we will collaborate with industry experts who have deep knowledge in e-commerce, artificial intelligence, and other related fields. Their insights and expertise will contribute significantly to the development of new features and improvements.\\n4. Stay updated with technological advancements: The world of technology is constantly evolving, so it's crucial to keep up with the latest developments. We will allocate resources to research and implement innovative technologies As an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\\n1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\\n2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\\n3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\\n4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\\n5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\", 'output': ''}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Line 8 Task summary: {'task': \"As a Nanotechnologist Agent, your primary goal is to develop nanobots that can interact with neurons at the molecular level. These nanobots will be designed to record neural activity and deliver targeted stimuli to specific neurons. To achieve this, you must collaborate closely with bioengineers to ensure the nanobots' compatibility with biological systems and their overall safety.\\nTo begin, you should research existing methods for interfacing with neural circuits using nanotechnology. This may involve studying the work of other researchers who have developed similar nanobot designs or exploring new materials and techniques that could potentially enhance the capabilities of these devices.\\nOnce you have gathered sufficient information on the current state of the field, you can start designing your own nanobot prototypes. This process will likely involve selecting appropriate materials for the construction of the nanobots, as well as determining how they will navigate through the complex environment of the brain. Additionally, you will need to consider how the nanobots will communicate with external equipment, such as sensors and computers, which will be essential for monitoring and controlling their activities.\\nAfter developing several prototype models, you should test each one in To continuously update and improve the GPT Store platform, we will employ several strategies:\\n1. Regularly monitor user feedback and preferences: By analyzing user data such as search history, purchase history, and ratings, we can identify trends and areas where improvements can be made. This information will help us prioritize updates and enhancements to better suit our users' needs.\\n2. Implement machine learning algorithms: We will use machine learning algorithms to analyze large amounts of data and make predictions about what features or content would be most beneficial to add next. For example, if we notice that certain types of products consistently receive high ratings, we may decide to invest more resources into expanding those categories.\\n3. Collaborate with experts in the field: To stay ahead of competitors and maintain relevance, we will collaborate with industry experts who have deep knowledge in e-commerce, artificial intelligence, and other related fields. Their insights and expertise will contribute significantly to the development of new features and improvements.\\n4. Stay updated with technological advancements: The world of technology is constantly evolving, so it's crucial to keep up with the latest developments. We will allocate resources to research and implement innovative technologies As an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\\n1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\\n2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\\n3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\\n4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\\n5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\", 'relevant_output': ''}\n",
      "Warning: task_summary is empty or not properly populated.\n",
      "New Task Description: As a Nanotechnologist Agent, your primary goal is to develop nanobots that can interact with neurons at the molecular level. These nanobots will be designed to record neural activity and deliver targeted stimuli to specific neurons. To achieve this, you must collaborate closely with bioengineers to ensure the nanobots' compatibility with biological systems and their overall safety.\n",
      "To begin, you should research existing methods for interfacing with neural circuits using nanotechnology. This may involve studying the work of other researchers who have developed similar nanobot designs or exploring new materials and techniques that could potentially enhance the capabilities of these devices.\n",
      "Once you have gathered sufficient information on the current state of the field, you can start designing your own nanobot prototypes. This process will likely involve selecting appropriate materials for the construction of the nanobots, as well as determining how they will navigate through the complex environment of the brain. Additionally, you will need to consider how the nanobots will communicate with external equipment, such as sensors and computers, which will be essential for monitoring and controlling their activities.\n",
      "After developing several prototype models, you should test each one in To continuously update and improve the GPT Store platform, we will employ several strategies:\n",
      "1. Regularly monitor user feedback and preferences: By analyzing user data such as search history, purchase history, and ratings, we can identify trends and areas where improvements can be made. This information will help us prioritize updates and enhancements to better suit our users' needs.\n",
      "2. Implement machine learning algorithms: We will use machine learning algorithms to analyze large amounts of data and make predictions about what features or content would be most beneficial to add next. For example, if we notice that certain types of products consistently receive high ratings, we may decide to invest more resources into expanding those categories.\n",
      "3. Collaborate with experts in the field: To stay ahead of competitors and maintain relevance, we will collaborate with industry experts who have deep knowledge in e-commerce, artificial intelligence, and other related fields. Their insights and expertise will contribute significantly to the development of new features and improvements.\n",
      "4. Stay updated with technological advancements: The world of technology is constantly evolving, so it's crucial to keep up with the latest developments. We will allocate resources to research and implement innovative technologies As an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\n",
      "1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\n",
      "2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\n",
      "3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\n",
      "4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\n",
      "5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': \"As a Nanotechnologist Agent, your primary goal is to develop nanobots that can interact with neurons at the molecular level. These nanobots will be designed to record neural activity and deliver targeted stimuli to specific neurons. To achieve this, you must collaborate closely with bioengineers to ensure the nanobots' compatibility with biological systems and their overall safety.\\nTo begin, you should research existing methods for interfacing with neural circuits using nanotechnology. This may involve studying the work of other researchers who have developed similar nanobot designs or exploring new materials and techniques that could potentially enhance the capabilities of these devices.\\nOnce you have gathered sufficient information on the current state of the field, you can start designing your own nanobot prototypes. This process will likely involve selecting appropriate materials for the construction of the nanobots, as well as determining how they will navigate through the complex environment of the brain. Additionally, you will need to consider how the nanobots will communicate with external equipment, such as sensors and computers, which will be essential for monitoring and controlling their activities.\\nAfter developing several prototype models, you should test each one in To continuously update and improve the GPT Store platform, we will employ several strategies:\\n1. Regularly monitor user feedback and preferences: By analyzing user data such as search history, purchase history, and ratings, we can identify trends and areas where improvements can be made. This information will help us prioritize updates and enhancements to better suit our users' needs.\\n2. Implement machine learning algorithms: We will use machine learning algorithms to analyze large amounts of data and make predictions about what features or content would be most beneficial to add next. For example, if we notice that certain types of products consistently receive high ratings, we may decide to invest more resources into expanding those categories.\\n3. Collaborate with experts in the field: To stay ahead of competitors and maintain relevance, we will collaborate with industry experts who have deep knowledge in e-commerce, artificial intelligence, and other related fields. Their insights and expertise will contribute significantly to the development of new features and improvements.\\n4. Stay updated with technological advancements: The world of technology is constantly evolving, so it's crucial to keep up with the latest developments. We will allocate resources to research and implement innovative technologies As an AI language model, I can't be biased or have personal opinions, but I can help you with some tips for being a good critic agent:\\n1. Be objective in your evaluations: Focus on the quality of the work, not the person behind it. Avoid making personal attacks or insults.\\n2. Provide specific examples: When pointing out areas that need improvement, give concrete examples to illustrate your point. This will make it easier for others to understand what needs to change.\\n3. Offer solutions: Don't just criticize; also suggest ways to improve the situation. This shows that you're invested in finding a solution rather than simply tearing down someone else's work.\\n4. Be open to feedback yourself: Remember that everyone makes mistakes, including you. Be willing to accept criticism from others and use it as an opportunity to grow and learn.\\n5. Encourage collaboration: Work together with the other agents to find the best possible solutions. By pooling ideas and expertise, you can often come up with more effective strategies than any one individual could alone.\", 'output': ''}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Agent Score: 16.546762589928058\n",
      "Conversation score: 16.546762589928058\n",
      "New Task Description: \n",
      "Running Summarizer Agent...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m . . ''\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Summarizer Output: {'task': '', 'output': \" . . ''\"}\n",
      "Warning: 'relevant_output' key missing in summarizer_output\n",
      "Saving data to file: D:\\PROJECTS\\OrchestrAI\\Agent Simulation Results\\OpenAI is introducing custom versions of ChatGPT known as GPTs which can be tailored for specific purposes These GPTs can be created by anyone without the need for coding skills They can be used for personal company internal or public use The creation process involves starting a conversation providing instructions and extra knowledge and selecting its capabilities such as web searching image creation or data analysisGPTs can be customized to fit specific user needs replacing the need for manual input of prompts and instruction sets OpenAI believes that the most innovative GPTs will come from the community and users do not need coding skills to create and share their expertiseOpenAI is also launching the GPT Store where creations by verified builders can be shared publicly GPTs in the store become searchable and can climb leaderboards Creators can earn money based on the usage of their GPTsThis release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity The release of the GPTs store will be the equivalent to that Your job is to brainstorm and come up with possible ideas and implementations of GPTs that could prove to be lucrative in the future_20231108014014.txt\n",
      "Ensuring directory for the file path: D:\\PROJECTS\\OrchestrAI\\Agent Simulation Results\\OpenAI is introducing custom versions of ChatGPT known as GPTs which can be tailored for specific purposes These GPTs can be created by anyone without the need for coding skills They can be used for personal company internal or public use The creation process involves starting a conversation providing instructions and extra knowledge and selecting its capabilities such as web searching image creation or data analysisGPTs can be customized to fit specific user needs replacing the need for manual input of prompts and instruction sets OpenAI believes that the most innovative GPTs will come from the community and users do not need coding skills to create and share their expertiseOpenAI is also launching the GPT Store where creations by verified builders can be shared publicly GPTs in the store become searchable and can climb leaderboards Creators can earn money based on the usage of their GPTsThis release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity The release of the GPTs store will be the equivalent to that Your job is to brainstorm and come up with possible ideas and implementations of GPTs that could prove to be lucrative in the future_20231108014014.txt\n",
      "Failed to write data to file: [Errno 2] No such file or directory: 'D:\\\\PROJECTS\\\\OrchestrAI\\\\Agent Simulation Results\\\\OpenAI is introducing custom versions of ChatGPT known as GPTs which can be tailored for specific purposes These GPTs can be created by anyone without the need for coding skills They can be used for personal company internal or public use The creation process involves starting a conversation providing instructions and extra knowledge and selecting its capabilities such as web searching image creation or data analysisGPTs can be customized to fit specific user needs replacing the need for manual input of prompts and instruction sets OpenAI believes that the most innovative GPTs will come from the community and users do not need coding skills to create and share their expertiseOpenAI is also launching the GPT Store where creations by verified builders can be shared publicly GPTs in the store become searchable and can climb leaderboards Creators can earn money based on the usage of their GPTsThis release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity The release of the GPTs store will be the equivalent to that Your job is to brainstorm and come up with possible ideas and implementations of GPTs that could prove to be lucrative in the future_20231108014014.txt'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\PROJECTS\\\\OrchestrAI\\\\Agent Simulation Results\\\\OpenAI is introducing custom versions of ChatGPT known as GPTs which can be tailored for specific purposes These GPTs can be created by anyone without the need for coding skills They can be used for personal company internal or public use The creation process involves starting a conversation providing instructions and extra knowledge and selecting its capabilities such as web searching image creation or data analysisGPTs can be customized to fit specific user needs replacing the need for manual input of prompts and instruction sets OpenAI believes that the most innovative GPTs will come from the community and users do not need coding skills to create and share their expertiseOpenAI is also launching the GPT Store where creations by verified builders can be shared publicly GPTs in the store become searchable and can climb leaderboards Creators can earn money based on the usage of their GPTsThis release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity The release of the GPTs store will be the equivalent to that Your job is to brainstorm and come up with possible ideas and implementations of GPTs that could prove to be lucrative in the future_20231108014014.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\PROJECTS\\OrchestrAI\\OrchastrAI_production_notebook.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m     initial_user_input \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mOpenAI is introducing custom versions of ChatGPT, known as GPTs, which can be tailored for specific purposes. These GPTs can be created by anyone without the need for coding skills. They can be used for personal, company internal, or public use. The creation process involves starting a conversation, providing instructions and extra knowledge, and selecting its capabilities such as web searching, image creation, or data analysis.\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=272'>273</a>\u001b[0m \u001b[39mGPTs can be customized to fit specific user needs, replacing the need for manual input of prompts and instruction sets. OpenAI believes that the most innovative GPTs will come from the community, and users do not need coding skills to create and share their expertise.\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=273'>274</a>\u001b[0m \u001b[39mOpenAI is also launching the GPT Store, where creations by verified builders can be shared publicly. GPTs in the store become searchable and can climb leaderboards. Creators can earn money based on the usage of their GPTs.\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=274'>275</a>\u001b[0m \u001b[39mThis release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity. The release of the GPTs store will be the equivalent to that. Your job is to brainstorm and come up with possible ideas and implementations of GPT\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms that could prove to be lucrative in the future.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=275'>276</a>\u001b[0m     run_director_agent_with_agent_logic(initial_user_input)\n",
      "\u001b[1;32md:\\PROJECTS\\OrchestrAI\\OrchastrAI_production_notebook.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=263'>264</a>\u001b[0m \u001b[39m# Append the final summary to the same file\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=264'>265</a>\u001b[0m save_to_file(filename, data_to_save, final_output_formatted)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll information has been saved to \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m successfully.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\PROJECTS\\OrchestrAI\\OrchastrAI_production_notebook.ipynb Cell 22\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m ensure_dir(filename)  \u001b[39m# Ensure the directory exists\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile opened successfully. Writing data...\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Debugging line\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECTS/OrchestrAI/OrchastrAI_production_notebook.ipynb#X26sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         file\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39mdumps(data, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wesla\\miniconda3\\envs\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\PROJECTS\\\\OrchestrAI\\\\Agent Simulation Results\\\\OpenAI is introducing custom versions of ChatGPT known as GPTs which can be tailored for specific purposes These GPTs can be created by anyone without the need for coding skills They can be used for personal company internal or public use The creation process involves starting a conversation providing instructions and extra knowledge and selecting its capabilities such as web searching image creation or data analysisGPTs can be customized to fit specific user needs replacing the need for manual input of prompts and instruction sets OpenAI believes that the most innovative GPTs will come from the community and users do not need coding skills to create and share their expertiseOpenAI is also launching the GPT Store where creations by verified builders can be shared publicly GPTs in the store become searchable and can climb leaderboards Creators can earn money based on the usage of their GPTsThis release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity The release of the GPTs store will be the equivalent to that Your job is to brainstorm and come up with possible ideas and implementations of GPTs that could prove to be lucrative in the future_20231108014014.txt'"
     ]
    }
   ],
   "source": [
    "score_threshold = 20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# Initialize the DeepLake database\n",
    "from collections import Counter\n",
    "from threading import Thread\n",
    "from deeplake.core.vectorstore import VectorStore\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, CommaSeparatedListOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the vector store and other variables\n",
    "\n",
    "\n",
    "# Function: extract_keywords\n",
    "def extract_keywords(task, min_length=4, max_keywords=10):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', initial_user_input).lower()\n",
    "    words = clean_text.split()\n",
    "    filtered_words = [word for word in words if len(word) >= min_length]\n",
    "    word_freq = Counter(filtered_words)\n",
    "    sorted_keywords = sorted(word_freq, key=word_freq.get, reverse=True)[:max_keywords]\n",
    "    return sorted_keywords  # Corrected 'extracted_keywords' to 'sorted_keywords'\n",
    "\n",
    "\n",
    "def capture_high_scores(conversation_score, threshold, data_to_save, high_score_stamp=\"HIGH_SCORE\"):\n",
    "    threshold = 20\n",
    "    if conversation_score > score_threshold:\n",
    "        data_to_save['**HIGH_SCORE_STAMP**'] = high_score_stamp\n",
    "    return data_to_save\n",
    "        \n",
    "        \n",
    "# Function to sanitize user input to create a valid filename\n",
    "# Helper function to sanitize the user input to create a valid filename\n",
    "def sanitize_filename(input_str):\n",
    "    # Remove invalid characters from filenames\n",
    "    return \"\".join([c for c in input_str if c.isalpha() or c.isdigit() or c in [' ', '_', '-']]).rstrip()\n",
    "\n",
    "\n",
    "\n",
    "# Function to append to summaries and save to file\n",
    "def append_to_summaries(summaries, summary):\n",
    "    summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Function to concatenate summaries and save to a file\n",
    "def concatenate_and_save(summaries):\n",
    "    concatenated = ' '.join(summaries)\n",
    "    return concatenated\n",
    "\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"task_summary\", description=\"Summary of the task\"),\n",
    "    ResponseSchema(name=\"conversation_score\", description=\"Score of the conversation\")\n",
    "]\n",
    "structured_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "list_output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "def run_director_agent_with_agent_logic(initial_user_input):\n",
    "    # Sanitize the initial user input to create a valid filename\n",
    "    filename_prefix = sanitize_filename(initial_user_input)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f'D:\\\\PROJECTS\\\\OrchestrAI\\\\Agent Simulation Results\\\\{filename_prefix}_{timestamp}.txt'\n",
    "    \n",
    "    \n",
    "    print(f\"Verifying initial_user_input: {initial_user_input}\")\n",
    "    db_docs_text = query_deeplake(text=initial_user_input)\n",
    "    print(\"SYSTEM MESSAGE: Database docs text: \", db_docs_text)\n",
    "    # Now append the db_docs text to the initial user input\n",
    "    new_input_for_tasks = initial_user_input + \" \" + db_docs_text\n",
    "    print(\"SYSTEM MESSAGE: New input for tasks: \", new_input_for_tasks)\n",
    "    \n",
    "    task_list = generate_task_list(new_input_for_tasks)\n",
    "    parsed_tasks = parse_task_list(task_list)\n",
    "    print(\"Line 1 Parsed Tasks:\", parsed_tasks)\n",
    "    print(parsed_tasks)\n",
    "    all_task_summaries = []\n",
    "    conversation_score = 0\n",
    "\n",
    "    for task_num, task_dict in enumerate(parsed_tasks, start=1):\n",
    "        task = task_dict['task']  # Extract the task description\n",
    "        if not task.strip():\n",
    "            continue  # Skip empty tasks\n",
    "        \n",
    "        # Query the database for each task\n",
    "        print(f\"SYSTEM MESSAGE: Retrieving documents for task {task_num}...\")\n",
    "        db_docs_text = query_deeplake(text=task)\n",
    "        \n",
    "        # Append the db_docs text to the task\n",
    "        task_with_db_docs = task + \" \" + db_docs_text\n",
    "        print(f\"SYSTEM MESSAGE: Enhanced task description: \", task_with_db_docs)\n",
    "        \n",
    "        clean_task = re.sub(r'[^\\w\\s]', '', task).lower()\n",
    "\n",
    "        agents_to_consult = []  # Your logic for determining agents_to_consult\n",
    "        \n",
    "#region  Agent Logic\n",
    "        # Software Engineer\n",
    "        if \"algorithm\" in clean_task or \"code\" in clean_task or \"efficiency\" in clean_task or \"optimize\" in clean_task or \"code\" in clean_task or \"debug\" in clean_task or \"fix\" in clean_task or \"python\" in clean_task or \"generate\" in clean_task:\n",
    "            agents_to_consult.append('run_software_engineer_agent')\n",
    "            \n",
    "        # Debugger\n",
    "        if \"bug\" in clean_task or \"fix\" in clean_task or \"error\" in clean_task:\n",
    "            agents_to_consult.append('run_debugger_agent')\n",
    "            \n",
    "        # Critic\n",
    "        if \"review\" in clean_task or \"improve\" in clean_task or \"suggest\" in clean_task or \"criticize\" in clean_task or \"analyze\" in clean_task or \"fix\" in clean_task :\n",
    "            agents_to_consult.append('run_critic_agent')\n",
    "        \n",
    "        # Architect\n",
    "        if \"design\" in clean_task or \"structure\" in clean_task or \"solution\" in clean_task:\n",
    "            agents_to_consult.append('run_architect_agent')\n",
    "        \n",
    "        # Mathematician\n",
    "        if \"calculation\" in clean_task or \"math\" in clean_task or \"logic\" in clean_task:\n",
    "            agents_to_consult.append('run_mathematician_agent')\n",
    "            \n",
    "        # UI Designer\n",
    "        if \"user interface\" in clean_task or \"ui\" in clean_task or \"design\" in clean_task:\n",
    "            agents_to_consult.append('run_ui_designer_agent')\n",
    "        \n",
    "        # Scientist\n",
    "        if \"scientific\" in clean_task or \"validation\" in clean_task or \"validation\" in clean_task or \"experiment\" in clean_task or \"research\" in clean_task :\n",
    "            agents_to_consult.append('run_scientist_agent')\n",
    "        \n",
    "        # Business Agent\n",
    "        if \"business\" in clean_task or \"rules\" in clean_task:\n",
    "            agents_to_consult.append('run_business_agent')\n",
    "        \n",
    "        # Research Agent\n",
    "        if \"research\" in clean_task or \"insight\" in clean_task or \"find\" in clean_task or \"data\" in clean_task or \"brainstorm\" in clean_task or \"info\" in clean_task:\n",
    "            agents_to_consult.append('run_research_agent')\n",
    "        \n",
    "        \n",
    "        # 10x Coder Agent\n",
    "        if \"rapid\" in clean_task or \"optimize\" in clean_task or \"code\" in clean_task or \"debug\" in clean_task or \"fix\" in clean_task or \"python\" in clean_task or \"generate\" in clean_task:\n",
    "            agents_to_consult.append('run_10x_coder_agent')\n",
    "        \n",
    "        # Quantum Computing Expert\n",
    "        if \"quantum\" in clean_task or \"algorithm\" in clean_task or \"cognitive\" in clean_task:\n",
    "            agents_to_consult.append('run_quantum_computing_expert_agent')\n",
    "        \n",
    "        # AI Expert\n",
    "        if \"ai\" in clean_task or \"neural\" in clean_task or \"simulate\" in clean_task or \"advanced\" in clean_task or \"artificial\" in clean_task or \"intelligence\" in clean_task or \"agent\" in clean_task:\n",
    "            agents_to_consult.append('run_ai_expert_agent')\n",
    "        \n",
    "        # Neuroscientist\n",
    "        if \"neural\" in clean_task or \"high resolution\" in clean_task or \"brain\" in clean_task or \"science\" in clean_task or \"bio\" in clean_task or \"medical\" in clean_task:\n",
    "            agents_to_consult.append('run_neuroscientist_agent')\n",
    "        \n",
    "        # Nanotechnologist\n",
    "        if \"nano\" in clean_task or \"neuron\" in clean_task or \"tech\" in clean_task or \"ai\" in clean_task:\n",
    "            agents_to_consult.append('run_nanotechnologist_agent')\n",
    "        \n",
    "        # Creator Innovator\n",
    "        if \"innovate\" in clean_task or \"creative\" in clean_task or \"challenge\" in clean_task or \"brainstorm\" in clean_task or \"think\" in clean_task or \"idea\" in clean_task:\n",
    "            agents_to_consult.append('run_creator_innovator_agent')\n",
    "\n",
    "        #endregion\n",
    "        \n",
    "        agents_to_consult = list(set(agents_to_consult))\n",
    "        print(\"\\n\\nLine 2 Consulting the following agents:\")\n",
    "        print(agents_to_consult)\n",
    "\n",
    "        task_agent_outputs = []\n",
    "        agents_output_dict = {}\n",
    "        agents_consulted = []\n",
    "\n",
    "        for agent in agents_to_consult:\n",
    "            print(f\"\\n\\n**RUNNING NEW AGENT ALERT**\\n\\n {agent} is generating output...\")\n",
    "            agent_function = globals().get(agent)\n",
    "\n",
    "            if agent_function:\n",
    "                    # Clear the agent_output\n",
    "                agent_output = []\n",
    "                # Introduce a 10-second delay\n",
    "                time.sleep(10)\n",
    "                agent_output = agent_function(task_with_db_docs)\n",
    "\n",
    "                # Ensure the output is in a string format\n",
    "                if isinstance(agent_output, dict) and 'output' in agent_output:\n",
    "                    agent_output_str = agent_output['output']\n",
    "                else:\n",
    "                    agent_output_str = str(agent_output)\n",
    "\n",
    "                task_agent_outputs.append(agent_output_str)\n",
    "                \n",
    "                agents_consulted.append(agent)\n",
    "                agent_output_key = f\"{agent}_output\"\n",
    "                agents_output_dict[agent_output_key] = agent_output\n",
    "                print(f\"Line 5 Agent output for {agent}: {agent_output}\")\n",
    "                conversation_score = evaluate_agent_output(agent_output, task)\n",
    "                print(f\"Line 6 Agent Conversation score: {conversation_score}\")\n",
    "                score_threshold = 25\n",
    "                print_high_score_alert = high_score_alert(conversation_score, score_threshold)\n",
    "                print(f\"Print high score alert: {print_high_score_alert}\")\n",
    "                \n",
    "\n",
    "        concatenated_agent_outputs = ' '.join(task_agent_outputs)\n",
    "        print(f\"Line 7 Concatenated agent outputs: {concatenated_agent_outputs}\")\n",
    "\n",
    "        # Run the summarizer agent and obtain the summary\n",
    "        task_summary = run_summarizer_agent(concatenated_agent_outputs)\n",
    "        print(f\"Line 8 Task summary: {task_summary}\")\n",
    "        \n",
    "        # Check if task_summary is populated with the expected keys and values\n",
    "        if not task_summary or 'relevant_output' not in task_summary or not task_summary['relevant_output']:\n",
    "            print(\"Warning: task_summary is empty or not properly populated.\")\n",
    "            continue  # Skip the rest of the loop if the summary is not correct\n",
    "\n",
    "        # Extract keywords from the relevant part of task_summary\n",
    "        keywords = extract_keywords_main(task_summary['relevant_output'])  # Corrected to access the relevant part of the dictionary\n",
    "        print(f\"Line 11 Keywords: {keywords}\")\n",
    "        conversation_score = evaluate_agent_output(task_summary, task)\n",
    "        print(f\"Line 9 Conversation score: {conversation_score}\")\n",
    "        format_instructions = structured_output_parser.get_format_instructions()\n",
    "        print(\"Format Instructions applied to task summary: \")\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"Provide a summary and a score for this task.\\n{format_instructions}\\n{task}\",\n",
    "            input_variables=[\"task\"],\n",
    "            partial_variables={\"format_instructions\": format_instructions}\n",
    "        )\n",
    "        _input = prompt.format_prompt(task=task_summary)  # Assuming task_summary is the summary of the task\n",
    "        \n",
    "    # Assume llm_chain is a function that takes a string and returns a string.\n",
    "    _output = llm_chain(initial_user_input)\n",
    "\n",
    "    # Ensure _output is a string\n",
    "    if not isinstance(_output, str):\n",
    "        _output = str(_output)\n",
    "\n",
    "    # Save the output in an orderly manner to a text file\n",
    "    task_summary = run_summarizer_agent(concatenated_agent_outputs)\n",
    "    keywords = extract_keywords_main(task_summary)  # Assuming extract_keywords function exists\n",
    "    conversation_score = evaluate_agent_output(task_summary, task)\n",
    "    print(f\"Conversation score: {conversation_score}\")\n",
    "\n",
    "    # Save task summary, conversation score, and keywords to the file\n",
    "    data_to_save = {\n",
    "        'task_summary': task_summary,\n",
    "        'conversation_score': conversation_score,\n",
    "        'keywords': keywords\n",
    "    }\n",
    "    score_threshold = 25\n",
    "    capture_high_scores(conversation_score, score_threshold, high_scores, data_to_save)\n",
    "\n",
    "\n",
    "    # Once all tasks are processed, get the final summary\n",
    "    concatenated_summaries = concatenate_and_save(all_task_summaries)\n",
    "    final_output = run_summarizer_agent(concatenated_summaries)\n",
    "    final_output_formatted = {\"Final Output\": final_output}\n",
    "    filename_prefix = sanitize_filename(initial_user_input)  # Sanitize the user input to create a filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Append the final summary to the same file\n",
    "    save_to_file(filename, data_to_save, final_output_formatted)\n",
    "\n",
    "    print(f\"All information has been saved to {filename} successfully.\")\n",
    "\n",
    "# Main entry point\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    initial_user_input = \"\"\"OpenAI is introducing custom versions of ChatGPT, known as GPTs, which can be tailored for specific purposes. These GPTs can be created by anyone without the need for coding skills. They can be used for personal, company internal, or public use. The creation process involves starting a conversation, providing instructions and extra knowledge, and selecting its capabilities such as web searching, image creation, or data analysis.\n",
    "GPTs can be customized to fit specific user needs, replacing the need for manual input of prompts and instruction sets. OpenAI believes that the most innovative GPTs will come from the community, and users do not need coding skills to create and share their expertise.\n",
    "OpenAI is also launching the GPT Store, where creations by verified builders can be shared publicly. GPTs in the store become searchable and can climb leaderboards. Creators can earn money based on the usage of their GPTs.\n",
    "This release of the GPT store will be as lucrative as when the iTunes app store was launched and early adopters took advantage of the opportunity. The release of the GPTs store will be the equivalent to that. Your job is to brainstorm and come up with possible ideas and implementations of GPT's that could prove to be lucrative in the future.\"\"\"\n",
    "    run_director_agent_with_agent_logic(initial_user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
